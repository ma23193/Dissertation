{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0409b79-93c6-4176-ad28-f155bf7758c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bw SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.6913185119628906] [G loss: 0.7053751349449158] [SNR: 0.9412021189928055] [RMSE: 0.6012424826622009]\n",
      "[bw SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.6911393404006958] [G loss: 0.705209493637085] [SNR: 0.9413871914148331] [RMSE: 0.601229727268219]\n",
      "[bw SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.6909600496292114] [G loss: 0.7050558924674988] [SNR: 0.941583514213562] [RMSE: 0.6012163758277893]\n",
      "[bw SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.690780758857727] [G loss: 0.7049150466918945] [SNR: 0.9417656809091568] [RMSE: 0.6012035012245178]\n",
      "[bw SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.690601110458374] [G loss: 0.7047874927520752] [SNR: 0.9419690817594528] [RMSE: 0.6011896133422852]\n",
      "[bw SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.6904205083847046] [G loss: 0.7046741247177124] [SNR: 0.9421636909246445] [RMSE: 0.6011759638786316]\n",
      "[em SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.6903332471847534] [G loss: 0.7046830654144287] [SNR: 0.9423695504665375] [RMSE: 0.6011618971824646]\n",
      "[em SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.6901447772979736] [G loss: 0.7045344114303589] [SNR: 0.9425808489322662] [RMSE: 0.6011471748352051]\n",
      "[em SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.6899573802947998] [G loss: 0.7043871879577637] [SNR: 0.9427937865257263] [RMSE: 0.6011323928833008]\n",
      "[em SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.6897706985473633] [G loss: 0.7042522430419922] [SNR: 0.9430112689733505] [RMSE: 0.6011173129081726]\n",
      "[em SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.6895837783813477] [G loss: 0.7041344046592712] [SNR: 0.9432350099086761] [RMSE: 0.6011018753051758]\n",
      "[em SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.6893960237503052] [G loss: 0.7040354013442993] [SNR: 0.9434708207845688] [RMSE: 0.6010856032371521]\n",
      "[ma SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.689333438873291] [G loss: 0.7040660977363586] [SNR: 0.9436974674463272] [RMSE: 0.6010696291923523]\n",
      "[ma SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.6891366243362427] [G loss: 0.7039464116096497] [SNR: 0.943944975733757] [RMSE: 0.6010528802871704]\n",
      "[ma SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.6889399290084839] [G loss: 0.7038330435752869] [SNR: 0.9441807121038437] [RMSE: 0.6010364890098572]\n",
      "[ma SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.688742458820343] [G loss: 0.7037338614463806] [SNR: 0.9444194287061691] [RMSE: 0.6010197997093201]\n",
      "[ma SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.6885434985160828] [G loss: 0.7036526799201965] [SNR: 0.9446709603071213] [RMSE: 0.6010023951530457]\n",
      "[ma SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.6883424520492554] [G loss: 0.703589677810669] [SNR: 0.9449291974306107] [RMSE: 0.6009846329689026]\n",
      "[em+bw SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.6882166862487793] [G loss: 0.7035794854164124] [SNR: 0.9451957792043686] [RMSE: 0.6009661555290222]\n",
      "[em+bw SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.6880061030387878] [G loss: 0.7035205960273743] [SNR: 0.9454572945833206] [RMSE: 0.6009481549263]\n",
      "[em+bw SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.6877950429916382] [G loss: 0.703467845916748] [SNR: 0.945727527141571] [RMSE: 0.600929319858551]\n",
      "[em+bw SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.6875829696655273] [G loss: 0.7034260630607605] [SNR: 0.9460044652223587] [RMSE: 0.6009101867675781]\n",
      "[em+bw SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.6873694062232971] [G loss: 0.7033972144126892] [SNR: 0.9462838619947433] [RMSE: 0.60089111328125]\n",
      "[em+bw SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.6871541738510132] [G loss: 0.703380286693573] [SNR: 0.9465660899877548] [RMSE: 0.6008715033531189]\n",
      "[ma+bw SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.6871237754821777] [G loss: 0.7034308314323425] [SNR: 0.9468488395214081] [RMSE: 0.6008517742156982]\n",
      "[ma+bw SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.6868900060653687] [G loss: 0.7033873200416565] [SNR: 0.9471410512924194] [RMSE: 0.6008315086364746]\n",
      "[ma+bw SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.6866574287414551] [G loss: 0.7033411860466003] [SNR: 0.9474261850118637] [RMSE: 0.600811779499054]\n",
      "[ma+bw SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.686424970626831] [G loss: 0.7033026218414307] [SNR: 0.9477271139621735] [RMSE: 0.6007912158966064]\n",
      "[ma+bw SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.686191976070404] [G loss: 0.703276515007019] [SNR: 0.9480172395706177] [RMSE: 0.6007707715034485]\n",
      "[ma+bw SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.6859577894210815] [G loss: 0.7032645344734192] [SNR: 0.9483293443918228] [RMSE: 0.6007494926452637]\n",
      "[ma+em SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.6859878301620483] [G loss: 0.7033074498176575] [SNR: 0.9486281126737595] [RMSE: 0.6007286310195923]\n",
      "[ma+em SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.6857271194458008] [G loss: 0.7032700181007385] [SNR: 0.9489373117685318] [RMSE: 0.6007074117660522]\n",
      "[ma+em SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.6854690313339233] [G loss: 0.7032305598258972] [SNR: 0.9492539614439011] [RMSE: 0.6006855964660645]\n",
      "[ma+em SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.6852127313613892] [G loss: 0.703200101852417] [SNR: 0.9495676308870316] [RMSE: 0.6006636619567871]\n",
      "[ma+em SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.6849571466445923] [G loss: 0.7031837701797485] [SNR: 0.9498867392539978] [RMSE: 0.6006414294242859]\n",
      "[ma+em SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.6847014427185059] [G loss: 0.7031823992729187] [SNR: 0.9502162039279938] [RMSE: 0.6006188988685608]\n",
      "[ma+em+bw SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.6846339702606201] [G loss: 0.7032403945922852] [SNR: 0.9505531191825867] [RMSE: 0.6005957722663879]\n",
      "[ma+em+bw SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.6843574047088623] [G loss: 0.7032313346862793] [SNR: 0.9508825838565826] [RMSE: 0.6005727648735046]\n",
      "[ma+em+bw SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.6840834617614746] [G loss: 0.7032247185707092] [SNR: 0.9512156993150711] [RMSE: 0.6005497574806213]\n",
      "[ma+em+bw SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.6838107109069824] [G loss: 0.7032282948493958] [SNR: 0.9515579789876938] [RMSE: 0.6005262136459351]\n",
      "[ma+em+bw SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.6835387945175171] [G loss: 0.703244686126709] [SNR: 0.951903909444809] [RMSE: 0.6005021333694458]\n",
      "[ma+em+bw SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.6832669973373413] [G loss: 0.7032740712165833] [SNR: 0.9522515535354614] [RMSE: 0.6004783511161804]\n",
      "[bw SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.6790487170219421] [G loss: 0.7127644419670105] [SNR: -2.4957163631916046] [RMSE: 0.8930909633636475]\n",
      "[bw SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.6785167455673218] [G loss: 0.7129626274108887] [SNR: -2.4957485496997833] [RMSE: 0.8930940628051758]\n",
      "[bw SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.6779956221580505] [G loss: 0.7131686210632324] [SNR: -2.495764195919037] [RMSE: 0.8930957317352295]\n",
      "[bw SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.677484393119812] [G loss: 0.7133817672729492] [SNR: -2.4957624077796936] [RMSE: 0.8930957317352295]\n",
      "[bw SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.6769825220108032] [G loss: 0.7136022448539734] [SNR: -2.4957476556301117] [RMSE: 0.8930942416191101]\n",
      "[bw SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.6764885783195496] [G loss: 0.7138301730155945] [SNR: -2.495724707841873] [RMSE: 0.89309161901474]\n",
      "[em SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.6768852472305298] [G loss: 0.713771402835846] [SNR: -2.4956902861595154] [RMSE: 0.8930882215499878]\n",
      "[em SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.6763123273849487] [G loss: 0.7139892578125] [SNR: -2.4956294894218445] [RMSE: 0.8930819034576416]\n",
      "[em SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.6757500171661377] [G loss: 0.7142103314399719] [SNR: -2.495560497045517] [RMSE: 0.8930746912956238]\n",
      "[em SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.6751976609230042] [G loss: 0.7144421935081482] [SNR: -2.495487481355667] [RMSE: 0.8930672407150269]\n",
      "[em SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.6746541261672974] [G loss: 0.7146883010864258] [SNR: -2.4953991174697876] [RMSE: 0.8930580615997314]\n",
      "[em SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.6741179823875427] [G loss: 0.7149491310119629] [SNR: -2.4953071773052216] [RMSE: 0.8930487632751465]\n",
      "[ma SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.6742856502532959] [G loss: 0.715047299861908] [SNR: -2.495209276676178] [RMSE: 0.8930386900901794]\n",
      "[ma SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.6737016439437866] [G loss: 0.7153085470199585] [SNR: -2.495095580816269] [RMSE: 0.893027126789093]\n",
      "[ma SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.6731258034706116] [G loss: 0.7155752778053284] [SNR: -2.494956851005554] [RMSE: 0.8930130004882812]\n",
      "[ma SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.6725570559501648] [G loss: 0.7158544063568115] [SNR: -2.4948184192180634] [RMSE: 0.8929985165596008]\n",
      "[ma SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.6719945669174194] [G loss: 0.7161483764648438] [SNR: -2.4946731328964233] [RMSE: 0.8929834961891174]\n",
      "[ma SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.6714376211166382] [G loss: 0.716457188129425] [SNR: -2.49451220035553] [RMSE: 0.8929668664932251]\n",
      "[em+bw SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.6712785959243774] [G loss: 0.7165715098381042] [SNR: -2.494351714849472] [RMSE: 0.892950713634491]\n",
      "[em+bw SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.6706851720809937] [G loss: 0.7169146537780762] [SNR: -2.4941834807395935] [RMSE: 0.8929333090782166]\n",
      "[em+bw SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.6701004505157471] [G loss: 0.71726393699646] [SNR: -2.4940042197704315] [RMSE: 0.8929146528244019]\n",
      "[em+bw SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.6695231795310974] [G loss: 0.7176218032836914] [SNR: -2.4938221275806427] [RMSE: 0.8928961753845215]\n",
      "[em+bw SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.6689521670341492] [G loss: 0.7179886698722839] [SNR: -2.493630051612854] [RMSE: 0.8928760290145874]\n",
      "[em+bw SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.6683864593505859] [G loss: 0.7183637022972107] [SNR: -2.4934254586696625] [RMSE: 0.8928552269935608]\n",
      "[ma+bw SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.6684858798980713] [G loss: 0.7184681296348572] [SNR: -2.4932140111923218] [RMSE: 0.8928337693214417]\n",
      "[ma+bw SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.6678481101989746] [G loss: 0.7188633680343628] [SNR: -2.4929939210414886] [RMSE: 0.8928109407424927]\n",
      "[ma+bw SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.6672213077545166] [G loss: 0.7192567586898804] [SNR: -2.4927502870559692] [RMSE: 0.8927861452102661]\n",
      "[ma+bw SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.6666041612625122] [G loss: 0.7196547985076904] [SNR: -2.492513209581375] [RMSE: 0.8927614688873291]\n",
      "[ma+bw SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.6659952402114868] [G loss: 0.7200603485107422] [SNR: -2.4922549724578857] [RMSE: 0.8927351832389832]\n",
      "[ma+bw SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.6653928160667419] [G loss: 0.7204751372337341] [SNR: -2.491993010044098] [RMSE: 0.892707884311676]\n",
      "[ma+em SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.66557776927948] [G loss: 0.7205011248588562] [SNR: -2.491721510887146] [RMSE: 0.8926804065704346]\n",
      "[ma+em SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.664885401725769] [G loss: 0.7209503650665283] [SNR: -2.4914444983005524] [RMSE: 0.8926517367362976]\n",
      "[ma+em SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.6642059683799744] [G loss: 0.7213972806930542] [SNR: -2.4911536276340485] [RMSE: 0.8926218748092651]\n",
      "[ma+em SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.6635379791259766] [G loss: 0.7218491435050964] [SNR: -2.4908624589443207] [RMSE: 0.8925914764404297]\n",
      "[ma+em SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.6628793478012085] [G loss: 0.7223102450370789] [SNR: -2.4905459582805634] [RMSE: 0.8925591111183167]\n",
      "[ma+em SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.6622285842895508] [G loss: 0.7227811813354492] [SNR: -2.490231841802597] [RMSE: 0.892527163028717]\n",
      "[ma+em+bw SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.6621042490005493] [G loss: 0.7230085730552673] [SNR: -2.4899157881736755] [RMSE: 0.8924944996833801]\n",
      "[ma+em+bw SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.6613950729370117] [G loss: 0.7235106825828552] [SNR: -2.489580512046814] [RMSE: 0.8924602270126343]\n",
      "[ma+em+bw SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.660698652267456] [G loss: 0.7240118384361267] [SNR: -2.4892480671405792] [RMSE: 0.8924261331558228]\n",
      "[ma+em+bw SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.6600127220153809] [G loss: 0.724518358707428] [SNR: -2.4889005720615387] [RMSE: 0.8923907279968262]\n",
      "[ma+em+bw SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.6593348979949951] [G loss: 0.7250329852104187] [SNR: -2.488563507795334] [RMSE: 0.8923552632331848]\n",
      "[ma+em+bw SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.658663272857666] [G loss: 0.7255569100379944] [SNR: -2.488206773996353] [RMSE: 0.8923189043998718]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import wfdb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the ECGDataset class\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, raw_signals, noisy_signals):\n",
    "        self.raw_signals = raw_signals\n",
    "        self.noisy_signals = noisy_signals\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_signals)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        raw_signal = self.raw_signals[idx]\n",
    "        noisy_signal = self.noisy_signals[idx]\n",
    "        return torch.tensor(raw_signal, dtype=torch.float32), torch.tensor(noisy_signal, dtype=torch.float32)\n",
    "\n",
    "# Define the add_noise function\n",
    "def add_noise(signal, noise, snr):\n",
    "    signal_power = np.mean(signal ** 2)\n",
    "    noise_power = np.mean(noise ** 2)\n",
    "    factor = (signal_power / noise_power) / (10 ** (snr / 10))\n",
    "    noisy_signal = signal + noise * np.sqrt(factor)\n",
    "    return noisy_signal\n",
    "\n",
    "# Define the load_mit_bih_data function\n",
    "def load_mit_bih_data(records, noise_type, snr_levels, target_length=650000):\n",
    "    raw_signals = []\n",
    "    noisy_signals_dict = {snr: [] for snr in snr_levels}\n",
    "    \n",
    "    for record in records:\n",
    "        raw_record = wfdb.rdrecord(f'C:\\\\Users\\\\malik\\\\Desktop\\\\Disertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0/{record}')\n",
    "        raw_signal = raw_record.p_signal[:, 0]  # Use the first channel for simplicity\n",
    "        \n",
    "        # Load noise and add it to the raw signal\n",
    "        noise_record = wfdb.rdrecord(f'C:\\\\Users\\\\malik\\\\Desktop\\\\Disertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0/{noise_type}')\n",
    "        noise_signal = noise_record.p_signal[:, 0]\n",
    "        \n",
    "        # Ensure the signals are of the same length\n",
    "        min_length = min(len(raw_signal), len(noise_signal), target_length)\n",
    "        raw_signal = raw_signal[:min_length]\n",
    "        noise_signal = noise_signal[:min_length]\n",
    "        \n",
    "        # Pad signals to target length\n",
    "        if min_length < target_length:\n",
    "            raw_signal = np.pad(raw_signal, (0, target_length - min_length), 'constant')\n",
    "            noise_signal = np.pad(noise_signal, (0, target_length - min_length), 'constant')\n",
    "        \n",
    "        raw_signals.append(raw_signal)\n",
    "        \n",
    "        for snr in snr_levels:\n",
    "            noisy_signal = add_noise(raw_signal, noise_signal, snr)\n",
    "            noisy_signals_dict[snr].append(noisy_signal)\n",
    "    \n",
    "    return np.array(raw_signals), {snr: np.array(noisy_signals_dict[snr]) for snr in snr_levels}\n",
    "\n",
    "# Select records and noise types for the experiment\n",
    "records = ['103', '105', '111', '116', '122', '205', '213', '219', '223', '230']\n",
    "noise_types = ['bw', 'em', 'ma']\n",
    "combined_noise_types = ['em+bw', 'ma+bw', 'ma+em', 'ma+em+bw']\n",
    "snr_levels = [0, 1, 2, 3, 4, 5]\n",
    "target_length = 649984\n",
    "\n",
    "raw_signals, noisy_signals_dict = {}, {}\n",
    "for noise_type in noise_types:\n",
    "    raw_signals[noise_type], noisy_signals_dict[noise_type] = load_mit_bih_data(records, noise_type, snr_levels, target_length)\n",
    "\n",
    "# For combined noise types, combine the corresponding noises\n",
    "for combined_noise in combined_noise_types:\n",
    "    components = combined_noise.split('+')\n",
    "    combined_raw_signals, combined_noisy_signals = [], {snr: [] for snr in snr_levels}\n",
    "    for i in range(len(records)):\n",
    "        combined_signal = np.zeros(target_length)\n",
    "        for component in components:\n",
    "            raw_signal = raw_signals[component][i]\n",
    "            combined_signal += raw_signal / len(components)  # Average the signals\n",
    "        combined_raw_signals.append(combined_signal)\n",
    "        for snr in snr_levels:\n",
    "            combined_noise_signal = np.zeros(target_length)\n",
    "            for component in components:\n",
    "                noise_signal = noisy_signals_dict[component][snr][i]\n",
    "                combined_noise_signal += noise_signal / len(components)  # Average the noises\n",
    "            combined_noisy_signals[snr].append(combined_noise_signal)\n",
    "    raw_signals[combined_noise] = np.array(combined_raw_signals)\n",
    "    noisy_signals_dict[combined_noise] = {snr: np.array(combined_noisy_signals[snr]) for snr in snr_levels}\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "datasets = {}\n",
    "for noise_type in noise_types + combined_noise_types:\n",
    "    for snr in snr_levels:\n",
    "        datasets[(noise_type, snr)] = ECGDataset(raw_signals[noise_type], noisy_signals_dict[noise_type][snr])\n",
    "\n",
    "dataloaders = {key: DataLoader(dataset, batch_size=16, shuffle=True) for key, dataset in datasets.items()}\n",
    "\n",
    "# Define the Generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, use_z=True):\n",
    "        super(Generator, self).__init__()\n",
    "        self.use_z = use_z\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, 8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(8, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(16, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(512, 1024, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.decoder_with_z = nn.Sequential(\n",
    "            nn.ConvTranspose1d(2048, 1024, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(1024, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(32, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(16, 8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(8, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.decoder_without_z = nn.Sequential(\n",
    "            nn.ConvTranspose1d(1024, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(32, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(16, 8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(8, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        if self.use_z:\n",
    "            z = torch.randn_like(encoded)\n",
    "            encoded = torch.cat((encoded, z), 1)\n",
    "            decoded = self.decoder_with_z(encoded)\n",
    "        else:\n",
    "            decoded = self.decoder_without_z(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Define the Discriminator class\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv1d(2, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(512, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x).view(x.size(0), -1)\n",
    "\n",
    "# Define the calculate_snr function\n",
    "def calculate_snr(original, denoised):\n",
    "    noise = original - denoised\n",
    "    snr = 10 * np.log10(np.sum(original ** 2) / np.sum(noise ** 2))\n",
    "    return snr\n",
    "\n",
    "def calculate_rmse(original, denoised):\n",
    "    mse = np.mean((original - denoised) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "def train(generator, discriminator, dataloaders, num_epochs=100, lr=0.000005):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    \n",
    "    results = {'Noise_Type': [], 'SNR_Level': [], 'Epoch': [], 'Batch': [], 'D_loss': [], 'G_loss': [], 'SNR': [], 'RMSE': []}\n",
    "    \n",
    "    for (noise_type, snr), dataloader in dataloaders.items():\n",
    "        for epoch in range(num_epochs):\n",
    "            for i, (raw_signals, noisy_signals) in enumerate(dataloader):\n",
    "                batch_size = raw_signals.size(0)\n",
    "                \n",
    "                # Ensure the signals have the same length\n",
    "                min_length = min(raw_signals.shape[-1], noisy_signals.shape[-1])\n",
    "                raw_signals = raw_signals[:, :min_length]\n",
    "                noisy_signals = noisy_signals[:, :min_length]\n",
    "\n",
    "                # Denoise the noisy signals\n",
    "                noisy_signals = noisy_signals.unsqueeze(1)  # Add channel dimension\n",
    "                raw_signals = raw_signals.unsqueeze(1)\n",
    "\n",
    "                # Train Generator\n",
    "                optimizer_G.zero_grad()\n",
    "                gen_signals = generator(noisy_signals)\n",
    "                \n",
    "                # Ensure the generated signals have the same length as the input signals\n",
    "                if gen_signals.shape[-1] != noisy_signals.shape[-1]:\n",
    "                    gen_signals = torch.nn.functional.pad(gen_signals, (0, noisy_signals.shape[-1] - gen_signals.shape[-1]))\n",
    "\n",
    "                # Update valid and fake labels to match the discriminator output size\n",
    "                disc_output_size = discriminator(torch.cat((gen_signals, noisy_signals), 1)).size()\n",
    "                valid = torch.ones(disc_output_size).to(gen_signals.device)\n",
    "                fake = torch.zeros(disc_output_size).to(gen_signals.device)\n",
    "                \n",
    "                g_loss = criterion(discriminator(torch.cat((gen_signals, noisy_signals), 1)), valid)\n",
    "                g_loss.backward()\n",
    "                optimizer_G.step()\n",
    "\n",
    "                # Train Discriminator\n",
    "                optimizer_D.zero_grad()\n",
    "                real_loss = criterion(discriminator(torch.cat((raw_signals, noisy_signals), 1)), valid)\n",
    "                fake_loss = criterion(discriminator(torch.cat((gen_signals.detach(), noisy_signals), 1)), fake)\n",
    "                d_loss = (real_loss + fake_loss) / 2\n",
    "                d_loss.backward()\n",
    "                optimizer_D.step()\n",
    "                \n",
    "                # Calculate SNR and RMSE\n",
    "                snr_value = calculate_snr(raw_signals.squeeze().cpu().numpy(), gen_signals.squeeze().cpu().detach().numpy())\n",
    "                rmse_value = calculate_rmse(raw_signals.squeeze().cpu().numpy(), gen_signals.squeeze().cpu().detach().numpy())\n",
    "\n",
    "                # Store results\n",
    "                results['Noise_Type'].append(noise_type)\n",
    "                results['SNR_Level'].append(snr)\n",
    "                results['Epoch'].append(epoch + 1)\n",
    "                results['Batch'].append(i + 1)\n",
    "                results['D_loss'].append(d_loss.item())\n",
    "                results['G_loss'].append(g_loss.item())\n",
    "                results['SNR'].append(snr_value)\n",
    "                results['RMSE'].append(rmse_value)\n",
    "                \n",
    "                print(f\"[{noise_type} SNR {snr}] [Epoch {epoch + 1}/{num_epochs}] [Batch {i + 1}/{len(dataloader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}] [SNR: {snr_value}] [RMSE: {rmse_value}]\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Initialize models with and without z\n",
    "generator_with_z = Generator(use_z=True)\n",
    "generator_without_z = Generator(use_z=False)\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Train the models and collect results for each SNR level\n",
    "results_with_z = train(generator_with_z, discriminator, dataloaders, num_epochs=1)  # Adjusted for quick testing\n",
    "results_without_z = train(generator_without_z, discriminator, dataloaders, num_epochs=1)  # Adjusted for quick testing\n",
    "\n",
    "# Create DataFrames to display results\n",
    "df_results_with_z = pd.DataFrame(results_with_z)\n",
    "df_results_without_z = pd.DataFrame(results_without_z)\n",
    "\n",
    "# Average SNR and RMSE for each SNR level\n",
    "avg_snr_with_z = df_results_with_z.groupby('SNR_Level')['SNR'].mean()\n",
    "avg_rmse_with_z = df_results_with_z.groupby('SNR_Level')['RMSE'].mean()\n",
    "\n",
    "avg_snr_without_z = df_results_without_z.groupby('SNR_Level')['SNR'].mean()\n",
    "avg_rmse_without_z = df_results_without_z.groupby('SNR_Level')['RMSE'].mean()\n",
    "\n",
    "# Plot SNR vs Average SNR\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(avg_snr_with_z.index, avg_snr_with_z.values, label='With z')\n",
    "plt.plot(avg_snr_without_z.index, avg_snr_without_z.values, label='Without z')\n",
    "plt.xlabel('SNR (dB)')\n",
    "plt.ylabel('Average SNR (dB)')\n",
    "plt.title('SNR vs Average SNR')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot SNR vs RMSE\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(avg_rmse_with_z.index, avg_rmse_with_z.values, label='With z')\n",
    "plt.plot(avg_rmse_without_z.index, avg_rmse_without_z.values, label='Without z')\n",
    "plt.xlabel('SNR (dB)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('SNR vs RMSE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Display the DataFrames as a table\n",
    "import ace_tools as tools; tools.display_dataframe_to_user(name=\"Denoising Results with z\", dataframe=df_results_with_z)\n",
    "import ace_tools as tools; tools.display_dataframe_to_user(name=\"Denoising Results without z\", dataframe=df_results_without_z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5e24ab-a212-4960-9fe1-ba69c62b12db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bw SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.6926300525665283] [G loss: 0.686773955821991] [SNR: 1.2358668446540833] [RMSE: 0.5811878442764282]\n",
      "[bw SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.6789685487747192] [G loss: 0.68644779920578] [SNR: 1.2510575354099274] [RMSE: 0.5801721215248108]\n",
      "[bw SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.6581389307975769] [G loss: 0.6894474029541016] [SNR: 1.2809893488883972] [RMSE: 0.5781766176223755]\n",
      "[bw SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.6229909658432007] [G loss: 0.7185783982276917] [SNR: 1.3263104856014252] [RMSE: 0.5751673579216003]\n",
      "[bw SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.5710328817367554] [G loss: 0.787251889705658] [SNR: 1.4014048874378204] [RMSE: 0.570216178894043]\n",
      "[bw SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.5183911323547363] [G loss: 0.8633074164390564] [SNR: 1.5446023643016815] [RMSE: 0.5608925223350525]\n",
      "[em SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.604749858379364] [G loss: 1.0012465715408325] [SNR: 1.8322236835956573] [RMSE: 0.5426235198974609]\n",
      "[em SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 1.4462296962738037] [G loss: 0.1251147985458374] [SNR: -0.2842377871274948] [RMSE: 0.6923436522483826]\n",
      "[em SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.9106392860412598] [G loss: 0.8324608206748962] [SNR: -1.328614056110382] [RMSE: 0.7808014750480652]\n",
      "[em SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.7390133738517761] [G loss: 1.5089517831802368] [SNR: -1.330605000257492] [RMSE: 0.780980110168457]\n",
      "[em SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.6212196946144104] [G loss: 1.2840828895568848] [SNR: -1.3306261599063873] [RMSE: 0.7809820771217346]\n",
      "[em SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.5581730604171753] [G loss: 1.077001690864563] [SNR: -1.3306355476379395] [RMSE: 0.7809829115867615]\n",
      "[ma SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.5110910534858704] [G loss: 0.9969670176506042] [SNR: -1.3306355476379395] [RMSE: 0.7809829711914062]\n",
      "[ma SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.45281052589416504] [G loss: 1.062780737876892] [SNR: -1.3306321203708649] [RMSE: 0.7809828519821167]\n",
      "[ma SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.3872062861919403] [G loss: 1.2716480493545532] [SNR: -1.3306327164173126] [RMSE: 0.7809832096099854]\n",
      "[ma SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.3296318054199219] [G loss: 1.5202528238296509] [SNR: -1.3306362926959991] [RMSE: 0.780983030796051]\n",
      "[ma SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.2855147421360016] [G loss: 1.7463949918746948] [SNR: -1.3306370377540588] [RMSE: 0.7809831500053406]\n",
      "[ma SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.25490787625312805] [G loss: 1.9890302419662476] [SNR: -1.330639123916626] [RMSE: 0.7809831500053406]\n",
      "[em+bw SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.2506084144115448] [G loss: 2.144163131713867] [SNR: -1.3306359946727753] [RMSE: 0.7809832096099854]\n",
      "[em+bw SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.23874041438102722] [G loss: 2.496493339538574] [SNR: -1.3306380808353424] [RMSE: 0.7809830904006958]\n",
      "[em+bw SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.25417137145996094] [G loss: 1.583716869354248] [SNR: -1.3306373357772827] [RMSE: 0.7809830904006958]\n",
      "[em+bw SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.29357054829597473] [G loss: 3.296377658843994] [SNR: -1.3306359946727753] [RMSE: 0.7809831500053406]\n",
      "[em+bw SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.22203414142131805] [G loss: 2.5925240516662598] [SNR: -1.3306362926959991] [RMSE: 0.7809830904006958]\n",
      "[em+bw SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.22585195302963257] [G loss: 1.7072213888168335] [SNR: -1.3306458294391632] [RMSE: 0.7809835076332092]\n",
      "[ma+bw SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.22050824761390686] [G loss: 2.3239967823028564] [SNR: -1.330636590719223] [RMSE: 0.7809832096099854]\n",
      "[ma+bw SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.2198956310749054] [G loss: 2.623260259628296] [SNR: -1.3306380808353424] [RMSE: 0.7809830904006958]\n",
      "[ma+bw SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.20164623856544495] [G loss: 2.377376079559326] [SNR: -1.3306345045566559] [RMSE: 0.780983030796051]\n",
      "[ma+bw SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.19557307660579681] [G loss: 2.122230052947998] [SNR: -1.3306359946727753] [RMSE: 0.7809832096099854]\n",
      "[ma+bw SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.18382245302200317] [G loss: 2.4222075939178467] [SNR: -1.3306394219398499] [RMSE: 0.7809832096099854]\n",
      "[ma+bw SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.17458347976207733] [G loss: 2.4557385444641113] [SNR: -1.3306373357772827] [RMSE: 0.780983030796051]\n",
      "[ma+em SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.1933899223804474] [G loss: 2.521545648574829] [SNR: -1.3306298851966858] [RMSE: 0.7809828519821167]\n",
      "[ma+em SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.19268211722373962] [G loss: 2.8167474269866943] [SNR: -1.3306331634521484] [RMSE: 0.780983030796051]\n",
      "[ma+em SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.1788485199213028] [G loss: 2.333314895629883] [SNR: -1.3306362926959991] [RMSE: 0.780983030796051]\n",
      "[ma+em SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.1721165031194687] [G loss: 2.628840923309326] [SNR: -1.330634206533432] [RMSE: 0.780983030796051]\n",
      "[ma+em SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.16430258750915527] [G loss: 2.492621898651123] [SNR: -1.3306352496147156] [RMSE: 0.7809830904006958]\n",
      "[ma+em SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.16511736810207367] [G loss: 2.983365058898926] [SNR: -1.3306313753128052] [RMSE: 0.7809827923774719]\n",
      "[ma+em+bw SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.17352107167243958] [G loss: 2.6703457832336426] [SNR: -1.3306321203708649] [RMSE: 0.7809827327728271]\n",
      "[ma+em+bw SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.17084529995918274] [G loss: 2.9790639877319336] [SNR: -1.3306362926959991] [RMSE: 0.7809830904006958]\n",
      "[ma+em+bw SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.15707671642303467] [G loss: 2.6261422634124756] [SNR: -1.3306359946727753] [RMSE: 0.7809829711914062]\n",
      "[ma+em+bw SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.14986202120780945] [G loss: 2.7095909118652344] [SNR: -1.3306383788585663] [RMSE: 0.7809830904006958]\n",
      "[ma+em+bw SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.14854805171489716] [G loss: 3.108083963394165] [SNR: -1.330636590719223] [RMSE: 0.780983030796051]\n",
      "[ma+em+bw SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.15056556463241577] [G loss: 2.700054168701172] [SNR: -1.3306362926959991] [RMSE: 0.780983030796051]\n",
      "[bw SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 44.1235466003418] [G loss: 5.620379397441866e-07] [SNR: -2.0550021529197693] [RMSE: 0.8489062786102295]\n",
      "[bw SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 28.696163177490234] [G loss: 0.00014036535867489874] [SNR: -2.075152099132538] [RMSE: 0.8508781790733337]\n",
      "[bw SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 10.135220527648926] [G loss: 0.0018463263986632228] [SNR: -2.0990951359272003] [RMSE: 0.8532268404960632]\n",
      "[bw SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 5.456506252288818] [G loss: 0.008031086064875126] [SNR: -2.1265049278736115] [RMSE: 0.8559234738349915]\n",
      "[bw SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 3.6044869422912598] [G loss: 0.023774944245815277] [SNR: -2.1575115621089935] [RMSE: 0.8589847087860107]\n",
      "[bw SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 2.4925856590270996] [G loss: 0.056050803512334824] [SNR: -2.192254066467285] [RMSE: 0.8624274134635925]\n",
      "[em SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 1.8307147026062012] [G loss: 0.14016969501972198] [SNR: -2.231397330760956] [RMSE: 0.8663225173950195]\n",
      "[em SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 1.295806646347046] [G loss: 0.2294035255908966] [SNR: -2.286643832921982] [RMSE: 0.8718506693840027]\n",
      "[em SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.9129621982574463] [G loss: 0.3698708117008209] [SNR: -2.4827711284160614] [RMSE: 0.8917611241340637]\n",
      "[em SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.6394780874252319] [G loss: 0.5949869751930237] [SNR: -3.556903600692749] [RMSE: 1.0091480016708374]\n",
      "[em SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.3478405177593231] [G loss: 1.2276345491409302] [SNR: -6.056375503540039] [RMSE: 1.3456387519836426]\n",
      "[em SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.21414251625537872] [G loss: 2.1517856121063232] [SNR: -5.147700905799866] [RMSE: 1.2119783163070679]\n",
      "[ma SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.2156638205051422] [G loss: 2.317405939102173] [SNR: -4.057338237762451] [RMSE: 1.0689977407455444]\n",
      "[ma SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.2781848609447479] [G loss: 1.3941748142242432] [SNR: -4.421708583831787] [RMSE: 1.1147949695587158]\n",
      "[ma SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 1.4553558826446533] [G loss: 0.09353720396757126] [SNR: -3.201013505458832] [RMSE: 0.9686356782913208]\n",
      "[ma SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 1.1435563564300537] [G loss: 0.20035666227340698] [SNR: -3.0873966217041016] [RMSE: 0.9560473561286926]\n",
      "[ma SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.7337061166763306] [G loss: 0.8823068737983704] [SNR: -2.9725760221481323] [RMSE: 0.9434928297996521]\n",
      "[ma SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.6794354915618896] [G loss: 1.6977050304412842] [SNR: -2.880672514438629] [RMSE: 0.9335626363754272]\n",
      "[em+bw SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.6105817556381226] [G loss: 1.6489198207855225] [SNR: -2.7338311076164246] [RMSE: 0.9179127216339111]\n",
      "[em+bw SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.5076531767845154] [G loss: 1.2784862518310547] [SNR: -2.614385187625885] [RMSE: 0.9053757190704346]\n",
      "[em+bw SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.43837839365005493] [G loss: 1.1816775798797607] [SNR: -2.6065033674240112] [RMSE: 0.9045549035072327]\n",
      "[em+bw SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.41586440801620483] [G loss: 1.100341558456421] [SNR: -2.4581775069236755] [RMSE: 0.8892395496368408]\n",
      "[em+bw SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.3356994390487671] [G loss: 1.477242112159729] [SNR: -2.4731627106666565] [RMSE: 0.8907748460769653]\n",
      "[em+bw SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.26248446106910706] [G loss: 2.064713954925537] [SNR: -2.525719404220581] [RMSE: 0.8961811661720276]\n",
      "[ma+bw SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.32812249660491943] [G loss: 1.6625560522079468] [SNR: -2.253303825855255] [RMSE: 0.868510365486145]\n",
      "[ma+bw SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.3369222581386566] [G loss: 1.5705053806304932] [SNR: -2.0188970863819122] [RMSE: 0.8453852534294128]\n",
      "[ma+bw SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.3126261532306671] [G loss: 2.4936602115631104] [SNR: -1.8347860872745514] [RMSE: 0.8276546001434326]\n",
      "[ma+bw SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.3586166203022003] [G loss: 1.5706530809402466] [SNR: -1.5135711431503296] [RMSE: 0.7976057529449463]\n",
      "[ma+bw SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.374332070350647] [G loss: 3.4209611415863037] [SNR: -1.5135715901851654] [RMSE: 0.7976062893867493]\n",
      "[ma+bw SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.2927733063697815] [G loss: 1.6559264659881592] [SNR: -1.5135733783245087] [RMSE: 0.7976060509681702]\n",
      "[ma+em SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.30967065691947937] [G loss: 2.969404697418213] [SNR: -1.5135681629180908] [RMSE: 0.7976057529449463]\n",
      "[ma+em SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.2634475529193878] [G loss: 2.3752083778381348] [SNR: -1.5135565400123596] [RMSE: 0.797604501247406]\n",
      "[ma+em SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.23891346156597137] [G loss: 2.296185255050659] [SNR: -1.5135186910629272] [RMSE: 0.7976009249687195]\n",
      "[ma+em SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.22097891569137573] [G loss: 2.6724438667297363] [SNR: -1.5128380060195923] [RMSE: 0.797538697719574]\n",
      "[ma+em SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.2586250305175781] [G loss: 1.9210189580917358] [SNR: -1.3306401669979095] [RMSE: 0.7809830904006958]\n",
      "[ma+em SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.2774776518344879] [G loss: 3.634243965148926] [SNR: -1.3306623697280884] [RMSE: 0.780985414981842]\n",
      "[ma+em+bw SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.2772822976112366] [G loss: 2.160659074783325] [SNR: -1.330663412809372] [RMSE: 0.7809853553771973]\n",
      "[ma+em+bw SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.24600961804389954] [G loss: 3.0740420818328857] [SNR: -1.3306580483913422] [RMSE: 0.7809853553771973]\n",
      "[ma+em+bw SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.22584912180900574] [G loss: 2.6056814193725586] [SNR: -1.330663412809372] [RMSE: 0.7809855341911316]\n",
      "[ma+em+bw SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.21398206055164337] [G loss: 2.5886034965515137] [SNR: -1.330660879611969] [RMSE: 0.7809853553771973]\n",
      "[ma+em+bw SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.20415626466274261] [G loss: 2.8906285762786865] [SNR: -1.3306601345539093] [RMSE: 0.7809852361679077]\n",
      "[ma+em+bw SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.19444221258163452] [G loss: 2.6517186164855957] [SNR: -1.3306580483913422] [RMSE: 0.7809853553771973]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import wfdb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the ECGDataset class\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, raw_signals, noisy_signals):\n",
    "        self.raw_signals = raw_signals\n",
    "        self.noisy_signals = noisy_signals\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_signals)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        raw_signal = self.raw_signals[idx]\n",
    "        noisy_signal = self.noisy_signals[idx]\n",
    "        return torch.tensor(raw_signal, dtype=torch.float32), torch.tensor(noisy_signal, dtype=torch.float32)\n",
    "\n",
    "# Define the add_noise function\n",
    "def add_noise(signal, noise, snr):\n",
    "    signal_power = np.mean(signal ** 2)\n",
    "    noise_power = np.mean(noise ** 2)\n",
    "    factor = (signal_power / noise_power) / (10 ** (snr / 10))\n",
    "    noisy_signal = signal + noise * np.sqrt(factor)\n",
    "    return noisy_signal\n",
    "\n",
    "# Define the load_mit_bih_data function\n",
    "def load_mit_bih_data(records, noise_type, snr_levels, target_length=650000):\n",
    "    raw_signals = []\n",
    "    noisy_signals_dict = {snr: [] for snr in snr_levels}\n",
    "    \n",
    "    for record in records:\n",
    "        raw_record = wfdb.rdrecord(f'C:\\\\Users\\\\malik\\\\Desktop\\\\Disertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0/{record}')\n",
    "        raw_signal = raw_record.p_signal[:, 0]  # Use the first channel for simplicity\n",
    "        \n",
    "        # Load noise and add it to the raw signal\n",
    "        noise_record = wfdb.rdrecord(f'C:\\\\Users\\\\malik\\\\Desktop\\\\Disertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0/{noise_type}')\n",
    "        noise_signal = noise_record.p_signal[:, 0]\n",
    "        \n",
    "        # Ensure the signals are of the same length\n",
    "        min_length = min(len(raw_signal), len(noise_signal), target_length)\n",
    "        raw_signal = raw_signal[:min_length]\n",
    "        noise_signal = noise_signal[:min_length]\n",
    "        \n",
    "        # Pad signals to target length\n",
    "        if min_length < target_length:\n",
    "            raw_signal = np.pad(raw_signal, (0, target_length - min_length), 'constant')\n",
    "            noise_signal = np.pad(noise_signal, (0, target_length - min_length), 'constant')\n",
    "        \n",
    "        raw_signals.append(raw_signal)\n",
    "        \n",
    "        for snr in snr_levels:\n",
    "            noisy_signal = add_noise(raw_signal, noise_signal, snr)\n",
    "            noisy_signals_dict[snr].append(noisy_signal)\n",
    "    \n",
    "    return np.array(raw_signals), {snr: np.array(noisy_signals_dict[snr]) for snr in snr_levels}\n",
    "\n",
    "# Select records and noise types for the experiment\n",
    "records = ['103', '105', '111', '116', '122', '205', '213', '219', '223', '230']\n",
    "noise_types = ['bw', 'em', 'ma']\n",
    "combined_noise_types = ['em+bw', 'ma+bw', 'ma+em', 'ma+em+bw']\n",
    "snr_levels = [0, 1, 2, 3, 4, 5]\n",
    "target_length = 649984\n",
    "\n",
    "raw_signals, noisy_signals_dict = {}, {}\n",
    "for noise_type in noise_types:\n",
    "    raw_signals[noise_type], noisy_signals_dict[noise_type] = load_mit_bih_data(records, noise_type, snr_levels, target_length)\n",
    "\n",
    "# For combined noise types, combine the corresponding noises\n",
    "for combined_noise in combined_noise_types:\n",
    "    components = combined_noise.split('+')\n",
    "    combined_raw_signals, combined_noisy_signals = [], {snr: [] for snr in snr_levels}\n",
    "    for i in range(len(records)):\n",
    "        combined_signal = np.zeros(target_length)\n",
    "        for component in components:\n",
    "            raw_signal = raw_signals[component][i]\n",
    "            combined_signal += raw_signal / len(components)  # Average the signals\n",
    "        combined_raw_signals.append(combined_signal)\n",
    "        for snr in snr_levels:\n",
    "            combined_noise_signal = np.zeros(target_length)\n",
    "            for component in components:\n",
    "                noise_signal = noisy_signals_dict[component][snr][i]\n",
    "                combined_noise_signal += noise_signal / len(components)  # Average the noises\n",
    "            combined_noisy_signals[snr].append(combined_noise_signal)\n",
    "    raw_signals[combined_noise] = np.array(combined_raw_signals)\n",
    "    noisy_signals_dict[combined_noise] = {snr: np.array(combined_noisy_signals[snr]) for snr in snr_levels}\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "datasets = {}\n",
    "for noise_type in noise_types + combined_noise_types:\n",
    "    for snr in snr_levels:\n",
    "        datasets[(noise_type, snr)] = ECGDataset(raw_signals[noise_type], noisy_signals_dict[noise_type][snr])\n",
    "\n",
    "dataloaders = {key: DataLoader(dataset, batch_size=16, shuffle=True) for key, dataset in datasets.items()}\n",
    "\n",
    "# Define the Generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, use_z=True):\n",
    "        super(Generator, self).__init__()\n",
    "        self.use_z = use_z\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, 8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(8, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(16, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(512, 1024, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.decoder_with_z = nn.Sequential(\n",
    "            nn.ConvTranspose1d(2048, 1024, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(1024, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(32, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(16, 8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(8, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.decoder_without_z = nn.Sequential(\n",
    "            nn.ConvTranspose1d(1024, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(32, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(16, 8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(8, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        if self.use_z:\n",
    "            z = torch.randn_like(encoded)\n",
    "            encoded = torch.cat((encoded, z), 1)\n",
    "            decoded = self.decoder_with_z(encoded)\n",
    "        else:\n",
    "            decoded = self.decoder_without_z(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Define the Discriminator class\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv1d(2, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(512, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x).view(x.size(0), -1)\n",
    "\n",
    "# Define the calculate_snr function\n",
    "def calculate_snr(original, denoised):\n",
    "    noise = original - denoised\n",
    "    snr = 10 * np.log10(np.sum(original ** 2) / np.sum(noise ** 2))\n",
    "    return snr\n",
    "\n",
    "def calculate_rmse(original, denoised):\n",
    "    mse = np.mean((original - denoised) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "def train(generator, discriminator, dataloaders, num_epochs=100, lr=0.0005):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    \n",
    "    results = {'Noise_Type': [], 'SNR_Level': [], 'Epoch': [], 'Batch': [], 'D_loss': [], 'G_loss': [], 'SNR': [], 'RMSE': []}\n",
    "    \n",
    "    for (noise_type, snr), dataloader in dataloaders.items():\n",
    "        for epoch in range(num_epochs):\n",
    "            for i, (raw_signals, noisy_signals) in enumerate(dataloader):\n",
    "                batch_size = raw_signals.size(0)\n",
    "                \n",
    "                # Ensure the signals have the same length\n",
    "                min_length = min(raw_signals.shape[-1], noisy_signals.shape[-1])\n",
    "                raw_signals = raw_signals[:, :min_length]\n",
    "                noisy_signals = noisy_signals[:, :min_length]\n",
    "\n",
    "                # Denoise the noisy signals\n",
    "                noisy_signals = noisy_signals.unsqueeze(1)  # Add channel dimension\n",
    "                raw_signals = raw_signals.unsqueeze(1)\n",
    "\n",
    "                # Train Generator\n",
    "                optimizer_G.zero_grad()\n",
    "                gen_signals = generator(noisy_signals)\n",
    "                \n",
    "                # Ensure the generated signals have the same length as the input signals\n",
    "                if gen_signals.shape[-1] != noisy_signals.shape[-1]:\n",
    "                    gen_signals = torch.nn.functional.interpolate(gen_signals, size=noisy_signals.shape[-1])\n",
    "\n",
    "                # Update valid and fake labels to match the discriminator output size\n",
    "                disc_output_size = discriminator(torch.cat((gen_signals, noisy_signals), 1)).size()\n",
    "                valid = torch.ones(disc_output_size).to(gen_signals.device)\n",
    "                fake = torch.zeros(disc_output_size).to(gen_signals.device)\n",
    "                \n",
    "                g_loss = criterion(discriminator(torch.cat((gen_signals, noisy_signals), 1)), valid)\n",
    "                g_loss.backward()\n",
    "                optimizer_G.step()\n",
    "\n",
    "                # Train Discriminator\n",
    "                optimizer_D.zero_grad()\n",
    "                real_loss = criterion(discriminator(torch.cat((raw_signals, noisy_signals), 1)), valid)\n",
    "                fake_loss = criterion(discriminator(torch.cat((gen_signals.detach(), noisy_signals), 1)), fake)\n",
    "                d_loss = (real_loss + fake_loss) / 2\n",
    "                d_loss.backward()\n",
    "                optimizer_D.step()\n",
    "                \n",
    "                # Calculate SNR and RMSE\n",
    "                snr_value = calculate_snr(raw_signals.squeeze().cpu().numpy(), gen_signals.squeeze().cpu().detach().numpy())\n",
    "                rmse_value = calculate_rmse(raw_signals.squeeze().cpu().numpy(), gen_signals.squeeze().cpu().detach().numpy())\n",
    "\n",
    "                # Store results\n",
    "                results['Noise_Type'].append(noise_type)\n",
    "                results['SNR_Level'].append(snr)\n",
    "                results['Epoch'].append(epoch + 1)\n",
    "                results['Batch'].append(i + 1)\n",
    "                results['D_loss'].append(d_loss.item())\n",
    "                results['G_loss'].append(g_loss.item())\n",
    "                results['SNR'].append(snr_value)\n",
    "                results['RMSE'].append(rmse_value)\n",
    "                \n",
    "                print(f\"[{noise_type} SNR {snr}] [Epoch {epoch + 1}/{num_epochs}] [Batch {i + 1}/{len(dataloader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}] [SNR: {snr_value}] [RMSE: {rmse_value}]\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Initialize models with and without z\n",
    "generator_with_z = Generator(use_z=True)\n",
    "generator_without_z = Generator(use_z=False)\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Train the models and collect results for each SNR level\n",
    "results_with_z = train(generator_with_z, discriminator, dataloaders, num_epochs=1)  # Adjusted for quick testing\n",
    "results_without_z = train(generator_without_z, discriminator, dataloaders, num_epochs=1)  # Adjusted for quick testing\n",
    "\n",
    "# Create DataFrames to display results\n",
    "df_results_with_z = pd.DataFrame(results_with_z)\n",
    "df_results_without_z = pd.DataFrame(results_without_z)\n",
    "\n",
    "# Average SNR and RMSE for each SNR level\n",
    "avg_snr_with_z = df_results_with_z.groupby('SNR_Level')['SNR'].mean()\n",
    "avg_rmse_with_z = df_results_with_z.groupby('SNR_Level')['RMSE'].mean()\n",
    "\n",
    "avg_snr_without_z = df_results_without_z.groupby('SNR_Level')['SNR'].mean()\n",
    "avg_rmse_without_z = df_results_without_z.groupby('SNR_Level')['RMSE'].mean()\n",
    "\n",
    "# Plot SNR vs Average SNR\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(avg_snr_with_z.index, avg_snr_with_z.values, label='With z')\n",
    "plt.plot(avg_snr_without_z.index, avg_snr_without_z.values, label='Without z')\n",
    "plt.xlabel('SNR (dB)')\n",
    "plt.ylabel('Average SNR (dB)')\n",
    "plt.title('SNR vs Average SNR')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot SNR vs RMSE\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(avg_rmse_with_z.index, avg_rmse_with_z.values, label='With z')\n",
    "plt.plot(avg_rmse_without_z.index, avg_rmse_without_z.values, label='Without z')\n",
    "plt.xlabel('SNR (dB)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('SNR vs RMSE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Display the DataFrames as a table\n",
    "import ace_tools as tools; tools.display_dataframe_to_user(name=\"Denoising Results with z\", dataframe=df_results_with_z)\n",
    "import ace_tools as tools; tools.display_dataframe_to_user(name=\"Denoising Results without z\", dataframe=df_results_without_z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cce1e9-5246-46de-8a5b-1544ba5c76fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[With z] [bw SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.7072569131851196] [G loss: 0.6577890515327454] [SNR: 2.1607254445552826] [RMSE: 0.5224846005439758]\n",
      "[With z] [bw SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 2.292759656906128] [G loss: 2.442997932434082] [SNR: 2.1606281399726868] [RMSE: 0.5224904417991638]\n",
      "[With z] [bw SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.8449093103408813] [G loss: 1.1702847480773926] [SNR: 2.1599072217941284] [RMSE: 0.5225340723991394]\n",
      "[With z] [bw SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.7401205897331238] [G loss: 0.6336720585823059] [SNR: 2.1560870110988617] [RMSE: 0.522763729095459]\n",
      "[With z] [bw SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.7254235148429871] [G loss: 0.5658223032951355] [SNR: 2.116299569606781] [RMSE: 0.5251639485359192]\n",
      "[With z] [bw SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.697318434715271] [G loss: 0.7003597021102905] [SNR: 0.529041700065136] [RMSE: 0.6304603815078735]\n",
      "[With z] [em SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.7075744867324829] [G loss: 0.7886320352554321] [SNR: -4.091973006725311] [RMSE: 1.0732691287994385]\n",
      "[With z] [em SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.7288933396339417] [G loss: 0.578389585018158] [SNR: -4.221408367156982] [RMSE: 1.0893818140029907]\n",
      "[With z] [em SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.7072250843048096] [G loss: 0.8182833194732666] [SNR: -4.221673309803009] [RMSE: 1.089415431022644]\n",
      "[With z] [em SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.6915018558502197] [G loss: 0.7122951745986938] [SNR: -4.2216408252716064] [RMSE: 1.0894113779067993]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import wfdb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the ECGDataset class\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, raw_signals, noisy_signals):\n",
    "        self.raw_signals = raw_signals\n",
    "        self.noisy_signals = noisy_signals\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_signals)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        raw_signal = self.raw_signals[idx]\n",
    "        noisy_signal = self.noisy_signals[idx]\n",
    "        return torch.tensor(raw_signal, dtype=torch.float32), torch.tensor(noisy_signal, dtype=torch.float32)\n",
    "\n",
    "# Define the add_noise function\n",
    "def add_noise(signal, noise, snr):\n",
    "    signal_power = np.mean(signal ** 2)\n",
    "    noise_power = np.mean(noise ** 2)\n",
    "    factor = (signal_power / noise_power) / (10 ** (snr / 10))\n",
    "    noisy_signal = signal + noise * 100 * np.sqrt(factor)\n",
    "    return noisy_signal\n",
    "\n",
    "# Define the load_mit_bih_data function\n",
    "def load_mit_bih_data(records, noise_type, snr_levels, target_length=650000):\n",
    "    raw_signals = []\n",
    "    noisy_signals_dict = {snr: [] for snr in snr_levels}\n",
    "    \n",
    "    for record in records:\n",
    "        raw_record = wfdb.rdrecord(f'C:\\\\Users\\\\malik\\\\Desktop\\\\Disertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0/{record}')\n",
    "        raw_signal = raw_record.p_signal[:, 0]  # Use the first channel for simplicity\n",
    "        \n",
    "        # Load noise and add it to the raw signal\n",
    "        noise_record = wfdb.rdrecord(f'C:\\\\Users\\\\malik\\\\Desktop\\\\Disertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0/{noise_type}')\n",
    "        noise_signal = noise_record.p_signal[:, 0]\n",
    "        \n",
    "        # Ensure the signals are of the same length\n",
    "        min_length = min(len(raw_signal), len(noise_signal), target_length)\n",
    "        raw_signal = raw_signal[:min_length]\n",
    "        noise_signal = noise_signal[:min_length]\n",
    "        \n",
    "        # Pad signals to target length\n",
    "        if min_length < target_length:\n",
    "            raw_signal = np.pad(raw_signal, (0, target_length - min_length), 'constant')\n",
    "            noise_signal = np.pad(noise_signal, (0, target_length - min_length), 'constant')\n",
    "        \n",
    "        raw_signals.append(raw_signal)\n",
    "        \n",
    "        for snr in snr_levels:\n",
    "            noisy_signal = add_noise(raw_signal, noise_signal, snr)\n",
    "            noisy_signals_dict[snr].append(noisy_signal)\n",
    "    \n",
    "    return np.array(raw_signals), {snr: np.array(noisy_signals_dict[snr]) for snr in snr_levels}\n",
    "\n",
    "# Select records and noise types for the experiment\n",
    "records = ['103', '105', '111', '116', '122', '205', '213', '219', '223', '230']\n",
    "noise_types = ['bw', 'em', 'ma']\n",
    "combined_noise_types = ['em+bw', 'ma+bw', 'ma+em', 'ma+em+bw']\n",
    "snr_levels = [0, 1, 2, 3, 4, 5]\n",
    "target_length = 649984\n",
    "\n",
    "raw_signals, noisy_signals_dict = {}, {}\n",
    "for noise_type in noise_types:\n",
    "    raw_signals[noise_type], noisy_signals_dict[noise_type] = load_mit_bih_data(records, noise_type, snr_levels, target_length)\n",
    "\n",
    "# For combined noise types, combine the corresponding noises\n",
    "for combined_noise in combined_noise_types:\n",
    "    components = combined_noise.split('+')\n",
    "    combined_raw_signals, combined_noisy_signals = [], {snr: [] for snr in snr_levels}\n",
    "    for i in range(len(records)):\n",
    "        combined_signal = np.zeros(target_length)\n",
    "        for component in components:\n",
    "            raw_signal = raw_signals[component][i]\n",
    "            combined_signal += raw_signal / len(components)  # Average the signals\n",
    "        combined_raw_signals.append(combined_signal)\n",
    "        for snr in snr_levels:\n",
    "            combined_noise_signal = np.zeros(target_length)\n",
    "            for component in components:\n",
    "                noise_signal = noisy_signals_dict[component][snr][i]\n",
    "                combined_noise_signal += noise_signal / len(components)  # Average the noises\n",
    "            combined_noisy_signals[snr].append(combined_noise_signal)\n",
    "    raw_signals[combined_noise] = np.array(combined_raw_signals)\n",
    "    noisy_signals_dict[combined_noise] = {snr: np.array(combined_noisy_signals[snr]) for snr in snr_levels}\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "datasets = {}\n",
    "for noise_type in noise_types + combined_noise_types:\n",
    "    for snr in snr_levels:\n",
    "        datasets[(noise_type, snr)] = ECGDataset(raw_signals[noise_type], noisy_signals_dict[noise_type][snr])\n",
    "\n",
    "dataloaders = {key: DataLoader(dataset, batch_size=16, shuffle=True) for key, dataset in datasets.items()}\n",
    "\n",
    "# Define the Generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, use_z=True):\n",
    "        super(Generator, self).__init__()\n",
    "        self.use_z = use_z\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, 8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(8, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(16, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(512, 1024, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.decoder_with_z = nn.Sequential(\n",
    "            nn.ConvTranspose1d(2048, 1024, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(1024, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(32, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(16, 8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(8, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.decoder_without_z = nn.Sequential(\n",
    "            nn.ConvTranspose1d(1024, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(32, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(16, 8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(8, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        if self.use_z:\n",
    "            z = torch.randn_like(encoded)\n",
    "            encoded = torch.cat((encoded, z), 1)\n",
    "            decoded = self.decoder_with_z(encoded)\n",
    "        else:\n",
    "            decoded = self.decoder_without_z(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Define the Discriminator class\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv1d(2, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(512, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x).view(x.size(0), -1)\n",
    "\n",
    "# Define the calculate_snr function\n",
    "def calculate_snr(original, denoised):\n",
    "    noise = original - denoised\n",
    "    snr = 10 * np.log10(np.sum(original ** 2) / np.sum(noise ** 2))\n",
    "    return snr\n",
    "\n",
    "def calculate_rmse(original, denoised):\n",
    "    mse = np.mean((original - denoised) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "def train(generator, discriminator, dataloaders, num_epochs=1, lr=0.0005):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    \n",
    "    results = {'Noise_Type': [], 'SNR_Level': [], 'Epoch': [], 'Batch': [], 'D_loss': [], 'G_loss': [], 'SNR': [], 'RMSE': []}\n",
    "    \n",
    "    for (noise_type, snr), dataloader in dataloaders.items():\n",
    "        for epoch in range(num_epochs):\n",
    "            for i, (raw_signals, noisy_signals) in enumerate(dataloader):\n",
    "                batch_size = raw_signals.size(0)\n",
    "                \n",
    "                # Ensure the signals have the same length\n",
    "                min_length = min(raw_signals.shape[-1], noisy_signals.shape[-1])\n",
    "                raw_signals = raw_signals[:, :min_length]\n",
    "                noisy_signals = noisy_signals[:, :min_length]\n",
    "\n",
    "                # Denoise the noisy signals\n",
    "                noisy_signals = noisy_signals.unsqueeze(1)  # Add channel dimension\n",
    "                raw_signals = raw_signals.unsqueeze(1)\n",
    "\n",
    "                # Train Generator\n",
    "                optimizer_G.zero_grad()\n",
    "                gen_signals = generator(noisy_signals)\n",
    "                \n",
    "                # Ensure the generated signals have the same length as the input signals\n",
    "                if gen_signals.shape[-1] != noisy_signals.shape[-1]:\n",
    "                    gen_signals = torch.nn.functional.interpolate(gen_signals, size=noisy_signals.shape[-1])\n",
    "\n",
    "                # Update valid and fake labels to match the discriminator output size\n",
    "                disc_output_size = discriminator(torch.cat((gen_signals, noisy_signals), 1)).size()\n",
    "                valid = torch.ones(disc_output_size).to(gen_signals.device)\n",
    "                fake = torch.zeros(disc_output_size).to(gen_signals.device)\n",
    "                \n",
    "                g_loss = criterion(discriminator(torch.cat((gen_signals, noisy_signals), 1)), valid)\n",
    "                g_loss.backward()\n",
    "                optimizer_G.step()\n",
    "\n",
    "                # Train Discriminator\n",
    "                optimizer_D.zero_grad()\n",
    "                real_loss = criterion(discriminator(torch.cat((raw_signals, noisy_signals), 1)), valid)\n",
    "                fake_loss = criterion(discriminator(torch.cat((gen_signals.detach(), noisy_signals), 1)), fake)\n",
    "                d_loss = (real_loss + fake_loss) / 2\n",
    "                d_loss.backward()\n",
    "                optimizer_D.step()\n",
    "                \n",
    "                # Calculate SNR and RMSE\n",
    "                snr_value = calculate_snr(raw_signals.squeeze().cpu().numpy(), gen_signals.squeeze().cpu().detach().numpy())\n",
    "                rmse_value = calculate_rmse(raw_signals.squeeze().cpu().numpy(), gen_signals.squeeze().cpu().detach().numpy())\n",
    "\n",
    "                # Store results\n",
    "                results['Noise_Type'].append(noise_type)\n",
    "                results['SNR_Level'].append(snr)\n",
    "                results['Epoch'].append(epoch + 1)\n",
    "                results['Batch'].append(i + 1)\n",
    "                results['D_loss'].append(d_loss.item())\n",
    "                results['G_loss'].append(g_loss.item())\n",
    "                results['SNR'].append(snr_value)\n",
    "                results['RMSE'].append(rmse_value)\n",
    "                \n",
    "                print(f\"[{'With z' if generator.use_z else 'Without z'}] [{noise_type} SNR {snr}] [Epoch {epoch + 1}/{num_epochs}] [Batch {i + 1}/{len(dataloader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}] [SNR: {snr_value}] [RMSE: {rmse_value}]\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Initialize models with and without z\n",
    "generator_with_z = Generator(use_z=True)\n",
    "generator_without_z = Generator(use_z=False)\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Train the models and collect results for each SNR level\n",
    "results_with_z = train(generator_with_z, discriminator, dataloaders, num_epochs=1)  # Adjusted for quick testing\n",
    "results_without_z = train(generator_without_z, discriminator, dataloaders, num_epochs=1)  # Adjusted for quick testing\n",
    "\n",
    "# Create DataFrames to display results\n",
    "df_results_with_z = pd.DataFrame(results_with_z)\n",
    "df_results_without_z = pd.DataFrame(results_without_z)\n",
    "\n",
    "# Average SNR and RMSE for each SNR level\n",
    "avg_snr_with_z = df_results_with_z.groupby('SNR_Level')['SNR'].mean()\n",
    "avg_rmse_with_z = df_results_with_z.groupby('SNR_Level')['RMSE'].mean()\n",
    "\n",
    "avg_snr_without_z = df_results_without_z.groupby('SNR_Level')['SNR'].mean()\n",
    "avg_rmse_without_z = df_results_without_z.groupby('SNR_Level')['RMSE'].mean()\n",
    "\n",
    "# Plot SNR vs Average SNR\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(avg_snr_with_z.index, avg_snr_with_z.values, label='With z')\n",
    "plt.plot(avg_snr_without_z.index, avg_snr_without_z.values, label='Without z')\n",
    "plt.xlabel('SNR (dB)')\n",
    "plt.ylabel('Average SNR (dB)')\n",
    "plt.title('SNR vs Average SNR')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot SNR vs RMSE\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(avg_rmse_with_z.index, avg_rmse_with_z.values, label='With z')\n",
    "plt.plot(avg_rmse_without_z.index, avg_rmse_without_z.values, label='Without z')\n",
    "plt.xlabel('SNR (dB)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('SNR vs RMSE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Display the DataFrames as a table\n",
    "import ace_tools as tools\n",
    "tools.display_dataframe_to_user(name=\"Denoising Results with z\", dataframe=df_results_with_z)\n",
    "tools.display_dataframe_to_user(name=\"Denoising Results without z\", dataframe=df_results_without_z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb0bb97-452b-4b5e-95e9-be7d5ada3442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bw SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.6938540935516357] [G loss: 0.70538330078125] [SNR: 1.9466766715049744] [RMSE: 0.49582982063293457]\n",
      "[bw SNR 0] [Epoch 1/1] [Batch 1/1] [SNR: 1.7225344479084015] [RMSE: 0.6887550950050354]\n",
      "[bw SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.6827574968338013] [G loss: 0.6731495261192322] [SNR: 1.922444999217987] [RMSE: 0.574996292591095]\n",
      "[bw SNR 1] [Epoch 1/1] [Batch 1/1] [SNR: 1.3121423125267029] [RMSE: 0.37077462673187256]\n",
      "[bw SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.6667925119400024] [G loss: 0.6643915772438049] [SNR: 1.8695633113384247] [RMSE: 0.5844566226005554]\n",
      "[bw SNR 2] [Epoch 1/1] [Batch 1/1] [SNR: 2.02610120177269] [RMSE: 0.29989880323410034]\n",
      "[bw SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.6478341817855835] [G loss: 0.6929236650466919] [SNR: 1.9218753278255463] [RMSE: 0.5441152453422546]\n",
      "[bw SNR 3] [Epoch 1/1] [Batch 1/1] [SNR: 1.7928458750247955] [RMSE: 0.5154151916503906]\n",
      "[bw SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.6219359636306763] [G loss: 0.7352486848831177] [SNR: 1.626967340707779] [RMSE: 0.5493919253349304]\n",
      "[bw SNR 4] [Epoch 1/1] [Batch 1/1] [SNR: 3.2659056782722473] [RMSE: 0.4800695776939392]\n",
      "[bw SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.5442359447479248] [G loss: 0.8554836511611938] [SNR: 1.8990865349769592] [RMSE: 0.5822984576225281]\n",
      "[bw SNR 5] [Epoch 1/1] [Batch 1/1] [SNR: 2.4551528692245483] [RMSE: 0.2866981029510498]\n",
      "[em SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.5362846255302429] [G loss: 1.0487934350967407] [SNR: 1.9417202472686768] [RMSE: 0.5633006691932678]\n",
      "[em SNR 0] [Epoch 1/1] [Batch 1/1] [SNR: 1.1611447483301163] [RMSE: 0.44616612792015076]\n",
      "[em SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.6290953159332275] [G loss: 0.5306485891342163] [SNR: 1.704995334148407] [RMSE: 0.5938643217086792]\n",
      "[em SNR 1] [Epoch 1/1] [Batch 1/1] [SNR: -5.709754824638367] [RMSE: 0.7618797421455383]\n",
      "[em SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.6143589019775391] [G loss: 1.8207817077636719] [SNR: -1.1857014894485474] [RMSE: 0.7874714732170105]\n",
      "[em SNR 2] [Epoch 1/1] [Batch 1/1] [SNR: -0.44115442782640457] [RMSE: 0.6286666989326477]\n",
      "[em SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.5089714527130127] [G loss: 1.2467018365859985] [SNR: 2.267978936433792] [RMSE: 0.5608566999435425]\n",
      "[em SNR 3] [Epoch 1/1] [Batch 1/1] [SNR: -5.5097973346710205] [RMSE: 0.6634054780006409]\n",
      "[em SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 1.0481853485107422] [G loss: 0.25209739804267883] [SNR: 1.1119360476732254] [RMSE: 0.6377254724502563]\n",
      "[em SNR 4] [Epoch 1/1] [Batch 1/1] [SNR: -6.288090348243713] [RMSE: 0.7810599207878113]\n",
      "[em SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.7553535103797913] [G loss: 0.8388110399246216] [SNR: -0.17849784344434738] [RMSE: 0.7068184018135071]\n",
      "[em SNR 5] [Epoch 1/1] [Batch 1/1] [SNR: -4.496005475521088] [RMSE: 0.9595107436180115]\n",
      "[ma SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.6735686659812927] [G loss: 1.9286062717437744] [SNR: -2.2665415704250336] [RMSE: 0.7503365278244019]\n",
      "[ma SNR 0] [Epoch 1/1] [Batch 1/1] [SNR: -0.32573506236076355] [RMSE: 0.9895864129066467]\n",
      "[ma SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.5750038027763367] [G loss: 1.2673033475875854] [SNR: -2.619401514530182] [RMSE: 0.895778477191925]\n",
      "[ma SNR 1] [Epoch 1/1] [Batch 1/1] [SNR: -2.6877814531326294] [RMSE: 0.9527710676193237]\n",
      "[ma SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.7499356865882874] [G loss: 0.5903429985046387] [SNR: -3.032861351966858] [RMSE: 1.0172559022903442]\n",
      "[ma SNR 2] [Epoch 1/1] [Batch 1/1] [SNR: -7.563157081604004] [RMSE: 1.0300897359848022]\n",
      "[ma SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.8723142743110657] [G loss: 0.4048815071582794] [SNR: -4.458010494709015] [RMSE: 1.1338000297546387]\n",
      "[ma SNR 3] [Epoch 1/1] [Batch 1/1] [SNR: -5.39194643497467] [RMSE: 1.1805338859558105]\n",
      "[ma SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.7075339555740356] [G loss: 0.6487728953361511] [SNR: -4.9851298332214355] [RMSE: 1.2051504850387573]\n",
      "[ma SNR 4] [Epoch 1/1] [Batch 1/1] [SNR: -5.697044134140015] [RMSE: 1.2208203077316284]\n",
      "[ma SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.6447411775588989] [G loss: 0.7906374335289001] [SNR: -4.956305027008057] [RMSE: 1.2786518335342407]\n",
      "[ma SNR 5] [Epoch 1/1] [Batch 1/1] [SNR: -8.885021805763245] [RMSE: 1.098120093345642]\n",
      "[em+bw SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.5758610963821411] [G loss: 0.928075909614563] [SNR: -5.101367235183716] [RMSE: 1.267344355583191]\n",
      "[em+bw SNR 0] [Epoch 1/1] [Batch 1/1] [SNR: -7.133989930152893] [RMSE: 1.1594535112380981]\n",
      "[em+bw SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.5157179236412048] [G loss: 1.067543625831604] [SNR: -5.101376175880432] [RMSE: 1.267345666885376]\n",
      "[em+bw SNR 1] [Epoch 1/1] [Batch 1/1] [SNR: -7.133991718292236] [RMSE: 1.1594537496566772]\n",
      "[em+bw SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.44394412636756897] [G loss: 1.253116250038147] [SNR: -5.915536284446716] [RMSE: 1.2130106687545776]\n",
      "[em+bw SNR 2] [Epoch 1/1] [Batch 1/1] [SNR: -4.072205424308777] [RMSE: 1.3723759651184082]\n",
      "[em+bw SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.36238202452659607] [G loss: 1.614676833152771] [SNR: -5.148078203201294] [RMSE: 1.2642335891723633]\n",
      "[em+bw SNR 3] [Epoch 1/1] [Batch 1/1] [SNR: -6.747652888298035] [RMSE: 1.1729661226272583]\n",
      "[em+bw SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.2695823609828949] [G loss: 2.1482748985290527] [SNR: -4.928877949714661] [RMSE: 1.2840014696121216]\n",
      "[em+bw SNR 4] [Epoch 1/1] [Batch 1/1] [SNR: -9.728146195411682] [RMSE: 1.0836765766143799]\n",
      "[em+bw SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.17481887340545654] [G loss: 2.7792086601257324] [SNR: -5.148078799247742] [RMSE: 1.2642335891723633]\n",
      "[em+bw SNR 5] [Epoch 1/1] [Batch 1/1] [SNR: -6.747653484344482] [RMSE: 1.1729662418365479]\n",
      "[ma+bw SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.10914473235607147] [G loss: 3.4509096145629883] [SNR: -5.26820182800293] [RMSE: 1.2554514408111572]\n",
      "[ma+bw SNR 0] [Epoch 1/1] [Batch 1/1] [SNR: -5.967922806739807] [RMSE: 1.210110068321228]\n",
      "[ma+bw SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.050033725798130035] [G loss: 4.055269718170166] [SNR: -5.225592255592346] [RMSE: 1.2633380889892578]\n",
      "[ma+bw SNR 1] [Epoch 1/1] [Batch 1/1] [SNR: -6.2524789571762085] [RMSE: 1.1768200397491455]\n",
      "[ma+bw SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.02221241220831871] [G loss: 4.51318359375] [SNR: -5.144651532173157] [RMSE: 1.2633272409439087]\n",
      "[ma+bw SNR 2] [Epoch 1/1] [Batch 1/1] [SNR: -6.757685542106628] [RMSE: 1.176866054534912]\n",
      "[ma+bw SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.011224567890167236] [G loss: 5.045111179351807] [SNR: -4.969832599163055] [RMSE: 1.281270146369934]\n",
      "[ma+bw SNR 3] [Epoch 1/1] [Batch 1/1] [SNR: -8.929643630981445] [RMSE: 1.0965323448181152]\n",
      "[ma+bw SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.0077506788074970245] [G loss: 5.3679914474487305] [SNR: -5.911562442779541] [RMSE: 1.2086471319198608]\n",
      "[ma+bw SNR 4] [Epoch 1/1] [Batch 1/1] [SNR: -4.113265872001648] [RMSE: 1.3876899480819702]\n",
      "[ma+bw SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.004494853317737579] [G loss: 5.833284854888916] [SNR: -4.905524849891663] [RMSE: 1.2843174934387207]\n",
      "[ma+bw SNR 5] [Epoch 1/1] [Batch 1/1] [SNR: -10.17282485961914] [RMSE: 1.0821796655654907]\n",
      "[ma+em SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.004388491623103619] [G loss: 6.078895092010498] [SNR: -5.364168882369995] [RMSE: 1.248591423034668]\n",
      "[ma+em SNR 0] [Epoch 1/1] [Batch 1/1] [SNR: -5.505784749984741] [RMSE: 1.2381739616394043]\n",
      "[ma+em SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.0033839994575828314] [G loss: 6.3077545166015625] [SNR: -5.119187831878662] [RMSE: 1.2636477947235107]\n",
      "[ma+em SNR 1] [Epoch 1/1] [Batch 1/1] [SNR: -6.937057375907898] [RMSE: 1.175487756729126]\n",
      "[ma+em SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.002657814184203744] [G loss: 6.547904968261719] [SNR: -5.013515949249268] [RMSE: 1.2781920433044434]\n",
      "[ma+em SNR 2] [Epoch 1/1] [Batch 1/1] [SNR: -8.248029947280884] [RMSE: 1.1108105182647705]\n",
      "[ma+em SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.0023350724950432777] [G loss: 6.665159225463867] [SNR: -5.101375579833984] [RMSE: 1.267345666885376]\n",
      "[ma+em SNR 3] [Epoch 1/1] [Batch 1/1] [SNR: -7.133989930152893] [RMSE: 1.1594539880752563]\n",
      "[ma+em SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.0019096601754426956] [G loss: 6.812212944030762] [SNR: -4.944586455821991] [RMSE: 1.2803517580032349]\n",
      "[ma+em SNR 4] [Epoch 1/1] [Batch 1/1] [SNR: -9.230636954307556] [RMSE: 1.100814938545227]\n",
      "[ma+em SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.002073379699140787] [G loss: 6.762729644775391] [SNR: -6.719262599945068] [RMSE: 1.168860673904419]\n",
      "[ma+em SNR 5] [Epoch 1/1] [Batch 1/1] [SNR: -3.2840606570243835] [RMSE: 1.5179102420806885]\n",
      "[ma+em+bw SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.0019084750674664974] [G loss: 6.9567341804504395] [SNR: -5.515226125717163] [RMSE: 1.2354447841644287]\n",
      "[ma+em+bw SNR 0] [Epoch 1/1] [Batch 1/1] [SNR: -4.966537058353424] [RMSE: 1.2898449897766113]\n",
      "[ma+em+bw SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.0016865123761817813] [G loss: 7.049417495727539] [SNR: -5.515223741531372] [RMSE: 1.2354447841644287]\n",
      "[ma+em+bw SNR 1] [Epoch 1/1] [Batch 1/1] [SNR: -4.966541528701782] [RMSE: 1.2898452281951904]\n",
      "[ma+em+bw SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.0013238447718322277] [G loss: 7.22667932510376] [SNR: -4.971765577793121] [RMSE: 1.2809300422668457]\n",
      "[ma+em+bw SNR 2] [Epoch 1/1] [Batch 1/1] [SNR: -8.885035514831543] [RMSE: 1.0981217622756958]\n",
      "[ma+em+bw SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.0012998126912862062] [G loss: 7.20935583114624] [SNR: -5.853877067565918] [RMSE: 1.217195749282837]\n",
      "[ma+em+bw SNR 3] [Epoch 1/1] [Batch 1/1] [SNR: -4.170798659324646] [RMSE: 1.3574738502502441]\n",
      "[ma+em+bw SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.0010992124443873763] [G loss: 7.374945163726807] [SNR: -5.010079741477966] [RMSE: 1.2772949934005737]\n",
      "[ma+em+bw SNR 4] [Epoch 1/1] [Batch 1/1] [SNR: -8.25059175491333] [RMSE: 1.1149277687072754]\n",
      "[ma+em+bw SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.001142640016041696] [G loss: 7.366588115692139] [SNR: -5.466063022613525] [RMSE: 1.2392021417617798]\n",
      "[ma+em+bw SNR 5] [Epoch 1/1] [Batch 1/1] [SNR: -5.12205183506012] [RMSE: 1.2753463983535767]\n",
      "[bw SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 2.0004801750183105] [G loss: 0.02122803032398224] [SNR: 0.9292184561491013] [RMSE: 0.5574496388435364]\n",
      "[bw SNR 0] [Epoch 1/1] [Batch 1/1] [SNR: 0.8147383481264114] [RMSE: 0.7646358013153076]\n",
      "[bw SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.8932006359100342] [G loss: 0.4563845992088318] [SNR: 0.9155438840389252] [RMSE: 0.6456691026687622]\n",
      "[bw SNR 1] [Epoch 1/1] [Batch 1/1] [SNR: 0.8559311181306839] [RMSE: 0.39076942205429077]\n",
      "[bw SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.37553155422210693] [G loss: 1.534708023071289] [SNR: 0.9048781543970108] [RMSE: 0.6531105041503906]\n",
      "[bw SNR 2] [Epoch 1/1] [Batch 1/1] [SNR: 1.3329973816871643] [RMSE: 0.3248104155063629]\n",
      "[bw SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.25631722807884216] [G loss: 2.5222339630126953] [SNR: 0.9596507996320724] [RMSE: 0.6078580021858215]\n",
      "[bw SNR 3] [Epoch 1/1] [Batch 1/1] [SNR: 0.981987938284874] [RMSE: 0.5658484101295471]\n",
      "[bw SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.2839603126049042] [G loss: 2.7857584953308105] [SNR: 0.896659791469574] [RMSE: 0.5975820422172546]\n",
      "[bw SNR 4] [Epoch 1/1] [Batch 1/1] [SNR: 1.337072104215622] [RMSE: 0.5994402170181274]\n",
      "[bw SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.4063814878463745] [G loss: 1.7514152526855469] [SNR: 0.9031173586845398] [RMSE: 0.6530464887619019]\n",
      "[bw SNR 5] [Epoch 1/1] [Batch 1/1] [SNR: -2.9157713055610657] [RMSE: 0.5320727229118347]\n",
      "[em SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 1.8679170608520508] [G loss: 0.2854837477207184] [SNR: -1.1900031566619873] [RMSE: 0.8078424334526062]\n",
      "[em SNR 0] [Epoch 1/1] [Batch 1/1] [SNR: -5.778073072433472] [RMSE: 0.9918762445449829]\n",
      "[em SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 3.0557363033294678] [G loss: 0.021129122003912926] [SNR: -3.8525819778442383] [RMSE: 1.1260720491409302]\n",
      "[em SNR 1] [Epoch 1/1] [Batch 1/1] [SNR: -8.424112796783447] [RMSE: 1.0413682460784912]\n",
      "[em SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 1.9071286916732788] [G loss: 0.11331147700548172] [SNR: -4.768135845661163] [RMSE: 1.1894782781600952]\n",
      "[em SNR 2] [Epoch 1/1] [Batch 1/1] [SNR: -5.7572245597839355] [RMSE: 1.1593750715255737]\n",
      "[em SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.6880302429199219] [G loss: 0.9167141318321228] [SNR: -4.571963846683502] [RMSE: 1.232676386833191]\n",
      "[em SNR 3] [Epoch 1/1] [Batch 1/1] [SNR: -9.553264379501343] [RMSE: 1.05670166015625]\n",
      "[em SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.42712152004241943] [G loss: 2.6604132652282715] [SNR: -4.6137624979019165] [RMSE: 1.232873558998108]\n",
      "[em SNR 4] [Epoch 1/1] [Batch 1/1] [SNR: -8.899568915367126] [RMSE: 1.0550141334533691]\n",
      "[em SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.28561708331108093] [G loss: 3.1151323318481445] [SNR: -4.806841313838959] [RMSE: 1.204272747039795]\n",
      "[em SNR 5] [Epoch 1/1] [Batch 1/1] [SNR: -5.767890214920044] [RMSE: 1.110821008682251]\n",
      "[ma SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.36833131313323975] [G loss: 2.1752138137817383] [SNR: -5.518402457237244] [RMSE: 1.0910613536834717]\n",
      "[ma SNR 0] [Epoch 1/1] [Batch 1/1] [SNR: -2.644263803958893] [RMSE: 1.2923500537872314]\n",
      "[ma SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.35355088114738464] [G loss: 1.722747802734375] [SNR: -4.610901176929474] [RMSE: 1.1266149282455444]\n",
      "[ma SNR 1] [Epoch 1/1] [Batch 1/1] [SNR: -3.6918920278549194] [RMSE: 1.0695327520370483]\n",
      "[ma SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.3175056576728821] [G loss: 1.3434038162231445] [SNR: -3.760867118835449] [RMSE: 1.1061917543411255]\n",
      "[ma SNR 2] [Epoch 1/1] [Batch 1/1] [SNR: -7.438248991966248] [RMSE: 1.0153825283050537]\n",
      "[ma SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.283932089805603] [G loss: 1.586909294128418] [SNR: -4.109773933887482] [RMSE: 1.089242935180664]\n",
      "[ma SNR 3] [Epoch 1/1] [Batch 1/1] [SNR: -4.300728738307953] [RMSE: 1.041160225868225]\n",
      "[ma SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.22479155659675598] [G loss: 2.105067253112793] [SNR: -3.8834235072135925] [RMSE: 1.0615878105163574]\n",
      "[ma SNR 4] [Epoch 1/1] [Batch 1/1] [SNR: -4.407700896263123] [RMSE: 1.052409052848816]\n",
      "[ma SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.16457103192806244] [G loss: 3.1795778274536133] [SNR: -3.4521707892417908] [RMSE: 1.075339436531067]\n",
      "[ma SNR 5] [Epoch 1/1] [Batch 1/1] [SNR: -7.952967882156372] [RMSE: 0.9863865375518799]\n",
      "[em+bw SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.16814839839935303] [G loss: 3.182264566421509] [SNR: -3.482540249824524] [RMSE: 1.0518487691879272]\n",
      "[em+bw SNR 0] [Epoch 1/1] [Batch 1/1] [SNR: -5.689806938171387] [RMSE: 0.9818478226661682]\n",
      "[em+bw SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.12143871188163757] [G loss: 3.308954954147339] [SNR: -3.499574363231659] [RMSE: 1.0539137125015259]\n",
      "[em+bw SNR 1] [Epoch 1/1] [Batch 1/1] [SNR: -5.674190521240234] [RMSE: 0.9800841212272644]\n",
      "[em+bw SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.09971550852060318] [G loss: 3.3448915481567383] [SNR: -4.416810572147369] [RMSE: 1.02077054977417]\n",
      "[em+bw SNR 2] [Epoch 1/1] [Batch 1/1] [SNR: -2.1933704614639282] [RMSE: 1.1054303646087646]\n",
      "[em+bw SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.06694521009922028] [G loss: 3.579232692718506] [SNR: -3.521462380886078] [RMSE: 1.048326849937439]\n",
      "[em+bw SNR 3] [Epoch 1/1] [Batch 1/1] [SNR: -5.193608403205872] [RMSE: 0.9808063507080078]\n",
      "[em+bw SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.04517601802945137] [G loss: 3.6013147830963135] [SNR: -3.083885610103607] [RMSE: 1.0382832288742065]\n",
      "[em+bw SNR 4] [Epoch 1/1] [Batch 1/1] [SNR: -8.671682476997375] [RMSE: 0.9595696926116943]\n",
      "[em+bw SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.04623336344957352] [G loss: 3.5108048915863037] [SNR: -3.3027520775794983] [RMSE: 1.022258996963501]\n",
      "[em+bw SNR 5] [Epoch 1/1] [Batch 1/1] [SNR: -5.081522464752197] [RMSE: 0.9682310223579407]\n",
      "[ma+bw SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.03939872235059738] [G loss: 4.266857147216797] [SNR: -3.4107249975204468] [RMSE: 1.013738989830017]\n",
      "[ma+bw SNR 0] [Epoch 1/1] [Batch 1/1] [SNR: -4.194968640804291] [RMSE: 0.9866819381713867]\n",
      "[ma+bw SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.040780216455459595] [G loss: 3.664559841156006] [SNR: -3.2003766298294067] [RMSE: 1.0005961656570435]\n",
      "[ma+bw SNR 1] [Epoch 1/1] [Batch 1/1] [SNR: -5.190791487693787] [RMSE: 1.0414193868637085]\n",
      "[ma+bw SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.08970256894826889] [G loss: 2.271834373474121] [SNR: -3.5038381814956665] [RMSE: 1.0458641052246094]\n",
      "[ma+bw SNR 2] [Epoch 1/1] [Batch 1/1] [SNR: -4.488120377063751] [RMSE: 0.9062515497207642]\n",
      "[ma+bw SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 1.0197042226791382] [G loss: 0.2744983732700348] [SNR: -2.7593207359313965] [RMSE: 0.9933789968490601]\n",
      "[ma+bw SNR 3] [Epoch 1/1] [Batch 1/1] [SNR: -6.684079170227051] [RMSE: 0.846726655960083]\n",
      "[ma+bw SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.5001406073570251] [G loss: 3.9129531383514404] [SNR: -3.557005524635315] [RMSE: 0.9216620326042175]\n",
      "[ma+bw SNR 4] [Epoch 1/1] [Batch 1/1] [SNR: 0.4008603096008301] [RMSE: 0.8252512216567993]\n",
      "[ma+bw SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.5113400220870972] [G loss: 1.9328043460845947] [SNR: -0.6078862771391869] [RMSE: 0.7830517292022705]\n",
      "[ma+bw SNR 5] [Epoch 1/1] [Batch 1/1] [SNR: -2.4681314826011658] [RMSE: 0.44572263956069946]\n",
      "[ma+em SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.6410361528396606] [G loss: 1.8175854682922363] [SNR: 0.055971844121813774] [RMSE: 0.6689801216125488]\n",
      "[ma+em SNR 0] [Epoch 1/1] [Batch 1/1] [SNR: -0.36132656037807465] [RMSE: 0.6847921013832092]\n",
      "[ma+em SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.43977344036102295] [G loss: 1.759415626525879] [SNR: -0.24436073377728462] [RMSE: 0.7209154367446899]\n",
      "[ma+em SNR 1] [Epoch 1/1] [Batch 1/1] [SNR: -0.18090009689331055] [RMSE: 0.5400204658508301]\n",
      "[ma+em SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.2955264449119568] [G loss: 2.126777410507202] [SNR: 0.20737247541546822] [RMSE: 0.7007314562797546]\n",
      "[ma+em SNR 2] [Epoch 1/1] [Batch 1/1] [SNR: -0.44949375092983246] [RMSE: 0.45259857177734375]\n",
      "[ma+em SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.30418580770492554] [G loss: 2.1460843086242676] [SNR: 1.0451000928878784] [RMSE: 0.6245558857917786]\n",
      "[ma+em SNR 3] [Epoch 1/1] [Batch 1/1] [SNR: 1.4354895055294037] [RMSE: 0.4322943389415741]\n",
      "[ma+em SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.26370134949684143] [G loss: 2.4402005672454834] [SNR: 1.2934775650501251] [RMSE: 0.6243469715118408]\n",
      "[ma+em SNR 4] [Epoch 1/1] [Batch 1/1] [SNR: 0.48895902931690216] [RMSE: 0.3595294654369354]\n",
      "[ma+em SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.3461293876171112] [G loss: 1.6269196271896362] [SNR: 0.5823221802711487] [RMSE: 0.5042935609817505]\n",
      "[ma+em SNR 5] [Epoch 1/1] [Batch 1/1] [SNR: 2.231103628873825] [RMSE: 0.804429292678833]\n",
      "[ma+em+bw SNR 0] [Epoch 1/1] [Batch 1/1] [D loss: 0.4177521765232086] [G loss: 2.073017120361328] [SNR: 0.8713149279356003] [RMSE: 0.5922381281852722]\n",
      "[ma+em+bw SNR 0] [Epoch 1/1] [Batch 1/1] [SNR: 0.8688933402299881] [RMSE: 0.6588188409805298]\n",
      "[ma+em+bw SNR 1] [Epoch 1/1] [Batch 1/1] [D loss: 0.5326719284057617] [G loss: 1.3341128826141357] [SNR: -0.38830898702144623] [RMSE: 0.684664249420166]\n",
      "[ma+em+bw SNR 1] [Epoch 1/1] [Batch 1/1] [SNR: 0.36157194525003433] [RMSE: 0.6984444856643677]\n",
      "[ma+em+bw SNR 2] [Epoch 1/1] [Batch 1/1] [D loss: 0.497967928647995] [G loss: 4.100700855255127] [SNR: -0.25555090978741646] [RMSE: 0.7442428469657898]\n",
      "[ma+em+bw SNR 2] [Epoch 1/1] [Batch 1/1] [SNR: -2.281760424375534] [RMSE: 0.5134365558624268]\n",
      "[ma+em+bw SNR 3] [Epoch 1/1] [Batch 1/1] [D loss: 0.6274127960205078] [G loss: 0.7248884439468384] [SNR: 1.117490828037262] [RMSE: 0.5454964637756348]\n",
      "[ma+em+bw SNR 3] [Epoch 1/1] [Batch 1/1] [SNR: 2.3233239352703094] [RMSE: 0.6427252292633057]\n",
      "[ma+em+bw SNR 4] [Epoch 1/1] [Batch 1/1] [D loss: 0.3650232255458832] [G loss: 2.3042588233947754] [SNR: 1.9371800124645233] [RMSE: 0.5740217566490173]\n",
      "[ma+em+bw SNR 4] [Epoch 1/1] [Batch 1/1] [SNR: -3.4426766633987427] [RMSE: 0.6409895420074463]\n",
      "[ma+em+bw SNR 5] [Epoch 1/1] [Batch 1/1] [D loss: 0.4274038076400757] [G loss: 1.7155972719192505] [SNR: 0.1868107169866562] [RMSE: 0.6463955044746399]\n",
      "[ma+em+bw SNR 5] [Epoch 1/1] [Batch 1/1] [SNR: 1.0531685501337051] [RMSE: 0.6264221668243408]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import wfdb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the ECGDataset class\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, raw_signals, noisy_signals):\n",
    "        self.raw_signals = raw_signals\n",
    "        self.noisy_signals = noisy_signals\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_signals)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        raw_signal = self.raw_signals[idx]\n",
    "        noisy_signal = self.noisy_signals[idx]\n",
    "        return torch.tensor(raw_signal, dtype=torch.float32), torch.tensor(noisy_signal, dtype=torch.float32)\n",
    "\n",
    "# Define the add_noise function\n",
    "def add_noise(signal, noise, snr):\n",
    "    signal_power = np.mean(signal ** 2)\n",
    "    noise_power = np.mean(noise ** 2)\n",
    "    factor = (signal_power / noise_power) / (10 ** (snr / 10))\n",
    "    noisy_signal = signal + noise * np.sqrt(factor)\n",
    "    return noisy_signal\n",
    "\n",
    "# Define the load_mit_bih_data function\n",
    "def load_mit_bih_data(records, noise_type, snr_levels, target_length=650000):\n",
    "    raw_signals = []\n",
    "    noisy_signals_dict = {snr: [] for snr in snr_levels}\n",
    "    \n",
    "    for record in records:\n",
    "        raw_record = wfdb.rdrecord(f'C:\\\\Users\\\\malik\\\\Desktop\\\\Disertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0/{record}')\n",
    "        raw_signal = raw_record.p_signal[:, 0]  # Use the first channel for simplicity\n",
    "        \n",
    "        # Load noise and add it to the raw signal\n",
    "        noise_record = wfdb.rdrecord(f'C:\\\\Users\\\\malik\\\\Desktop\\\\Disertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0/{noise_type}')\n",
    "        noise_signal = noise_record.p_signal[:, 0]\n",
    "        \n",
    "        # Ensure the signals are of the same length\n",
    "        min_length = min(len(raw_signal), len(noise_signal), target_length)\n",
    "        raw_signal = raw_signal[:min_length]\n",
    "        noise_signal = noise_signal[:min_length]\n",
    "        \n",
    "        # Pad signals to target length\n",
    "        if min_length < target_length:\n",
    "            raw_signal = np.pad(raw_signal, (0, target_length - min_length), 'constant')\n",
    "            noise_signal = np.pad(noise_signal, (0, target_length - min_length), 'constant')\n",
    "        \n",
    "        raw_signals.append(raw_signal)\n",
    "        \n",
    "        for snr in snr_levels:\n",
    "            noisy_signal = add_noise(raw_signal, noise_signal, snr)\n",
    "            noisy_signals_dict[snr].append(noisy_signal)\n",
    "    \n",
    "    return np.array(raw_signals), {snr: np.array(noisy_signals_dict[snr]) for snr in snr_levels}\n",
    "\n",
    "# Select records and noise types for the experiment\n",
    "records = ['103', '105', '111', '116', '122', '205', '213', '219', '223', '230']\n",
    "noise_types = ['bw', 'em', 'ma']\n",
    "combined_noise_types = ['em+bw', 'ma+bw', 'ma+em', 'ma+em+bw']\n",
    "snr_levels = [0, 1, 2, 3, 4, 5]\n",
    "target_length = 649984\n",
    "\n",
    "raw_signals, noisy_signals_dict = {}, {}\n",
    "for noise_type in noise_types:\n",
    "    raw_signals[noise_type], noisy_signals_dict[noise_type] = load_mit_bih_data(records, noise_type, snr_levels, target_length)\n",
    "\n",
    "# For combined noise types, combine the corresponding noises\n",
    "for combined_noise in combined_noise_types:\n",
    "    components = combined_noise.split('+')\n",
    "    combined_raw_signals, combined_noisy_signals = [], {snr: [] for snr in snr_levels}\n",
    "    for i in range(len(records)):\n",
    "        combined_signal = np.zeros(target_length)\n",
    "        for component in components:\n",
    "            raw_signal = raw_signals[component][i]\n",
    "            combined_signal += raw_signal / len(components)  # Average the signals\n",
    "        combined_raw_signals.append(combined_signal)\n",
    "        for snr in snr_levels:\n",
    "            combined_noise_signal = np.zeros(target_length)\n",
    "            for component in components:\n",
    "                noise_signal = noisy_signals_dict[component][snr][i]\n",
    "                combined_noise_signal += noise_signal / len(components)  # Average the noises\n",
    "            combined_noisy_signals[snr].append(combined_noise_signal)\n",
    "    raw_signals[combined_noise] = np.array(combined_raw_signals)\n",
    "    noisy_signals_dict[combined_noise] = {snr: np.array(combined_noisy_signals[snr]) for snr in snr_levels}\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "datasets = {}\n",
    "for noise_type in noise_types + combined_noise_types:\n",
    "    for snr in snr_levels:\n",
    "        datasets[(noise_type, snr)] = ECGDataset(raw_signals[noise_type], noisy_signals_dict[noise_type][snr])\n",
    "\n",
    "dataloaders = {}\n",
    "for key, dataset in datasets.items():\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    dataloaders[key] = {'train': DataLoader(train_dataset, batch_size=16, shuffle=True), 'test': DataLoader(test_dataset, batch_size=16, shuffle=False)}\n",
    "\n",
    "# Define the Generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, use_z=True):\n",
    "        super(Generator, self).__init__()\n",
    "        self.use_z = use_z\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, 8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(8, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(16, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(512, 1024, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.decoder_with_z = nn.Sequential(\n",
    "            nn.ConvTranspose1d(2048, 1024, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(1024, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(32, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(16, 8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(8, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.decoder_without_z = nn.Sequential(\n",
    "            nn.ConvTranspose1d(1024, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(32, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(16, 8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(8, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        if self.use_z:\n",
    "            z = torch.randn_like(encoded)\n",
    "            encoded = torch.cat((encoded, z), 1)\n",
    "            decoded = self.decoder_with_z(encoded)\n",
    "        else:\n",
    "            decoded = self.decoder_without_z(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Define the Discriminator class\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv1d(2, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(512, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x).view(x.size(0), -1)\n",
    "\n",
    "# Define the calculate_snr function\n",
    "def calculate_snr(original, denoised):\n",
    "    noise = original - denoised\n",
    "    snr = 10 * np.log10(np.sum(original ** 2) / np.sum(noise ** 2))\n",
    "    return snr\n",
    "\n",
    "def calculate_rmse(original, denoised):\n",
    "    mse = np.mean((original - denoised) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "def train(generator, discriminator, dataloaders, num_epochs=100, lr=0.0005, use_z=True):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    \n",
    "    results = {'Noise_Type': [], 'SNR_Level': [], 'Epoch': [], 'Batch': [], 'D_loss': [], 'G_loss': [], 'SNR': [], 'RMSE': [], 'Type': []}\n",
    "    \n",
    "    for (noise_type, snr), split_dataloaders in dataloaders.items():\n",
    "        train_loader = split_dataloaders['train']\n",
    "        test_loader = split_dataloaders['test']\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Training phase\n",
    "            generator.train()\n",
    "            discriminator.train()\n",
    "            for i, (raw_signals, noisy_signals) in enumerate(train_loader):\n",
    "                batch_size = raw_signals.size(0)\n",
    "                \n",
    "                # Ensure the signals have the same length\n",
    "                min_length = min(raw_signals.shape[-1], noisy_signals.shape[-1])\n",
    "                raw_signals = raw_signals[:, :min_length]\n",
    "                noisy_signals = noisy_signals[:, :min_length]\n",
    "\n",
    "                # Denoise the noisy signals\n",
    "                noisy_signals = noisy_signals.unsqueeze(1)  # Add channel dimension\n",
    "                raw_signals = raw_signals.unsqueeze(1)\n",
    "\n",
    "                # Train Generator\n",
    "                optimizer_G.zero_grad()\n",
    "                gen_signals = generator(noisy_signals)\n",
    "                \n",
    "                # Ensure the generated signals have the same length as the input signals\n",
    "                if gen_signals.shape[-1] != noisy_signals.shape[-1]:\n",
    "                    gen_signals = torch.nn.functional.interpolate(gen_signals, size=noisy_signals.shape[-1])\n",
    "\n",
    "                # Update valid and fake labels to match the discriminator output size\n",
    "                disc_output_size = discriminator(torch.cat((gen_signals, noisy_signals), 1)).size()\n",
    "                valid = torch.ones(disc_output_size).to(gen_signals.device)\n",
    "                fake = torch.zeros(disc_output_size).to(gen_signals.device)\n",
    "                \n",
    "                g_loss = criterion(discriminator(torch.cat((gen_signals, noisy_signals), 1)), valid)\n",
    "                g_loss.backward()\n",
    "                optimizer_G.step()\n",
    "\n",
    "                # Train Discriminator\n",
    "                optimizer_D.zero_grad()\n",
    "                real_loss = criterion(discriminator(torch.cat((raw_signals, noisy_signals), 1)), valid)\n",
    "                fake_loss = criterion(discriminator(torch.cat((gen_signals.detach(), noisy_signals), 1)), fake)\n",
    "                d_loss = (real_loss + fake_loss) / 2\n",
    "                d_loss.backward()\n",
    "                optimizer_D.step()\n",
    "\n",
    "                # Calculate SNR and RMSE\n",
    "                snr_value = calculate_snr(raw_signals.squeeze().cpu().numpy(), gen_signals.squeeze().cpu().detach().numpy())\n",
    "                rmse_value = calculate_rmse(raw_signals.squeeze().cpu().numpy(), gen_signals.squeeze().cpu().detach().numpy())\n",
    "\n",
    "                # Store results\n",
    "                results['Noise_Type'].append(noise_type)\n",
    "                results['SNR_Level'].append(snr)\n",
    "                results['Epoch'].append(epoch + 1)\n",
    "                results['Batch'].append(i + 1)\n",
    "                results['D_loss'].append(d_loss.item())\n",
    "                results['G_loss'].append(g_loss.item())\n",
    "                results['SNR'].append(snr_value)\n",
    "                results['RMSE'].append(rmse_value)\n",
    "                results['Type'].append('Train' if use_z else 'Test')\n",
    "\n",
    "                print(f\"[{noise_type} SNR {snr}] [Epoch {epoch + 1}/{num_epochs}] [Batch {i + 1}/{len(train_loader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}] [SNR: {snr_value}] [RMSE: {rmse_value}]\")\n",
    "        \n",
    "        # Testing phase\n",
    "        generator.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (raw_signals, noisy_signals) in enumerate(test_loader):\n",
    "                batch_size = raw_signals.size(0)\n",
    "                \n",
    "                # Ensure the signals have the same length\n",
    "                min_length = min(raw_signals.shape[-1], noisy_signals.shape[-1])\n",
    "                raw_signals = raw_signals[:, :min_length]\n",
    "                noisy_signals = noisy_signals[:, :min_length]\n",
    "\n",
    "                # Denoise the noisy signals\n",
    "                noisy_signals = noisy_signals.unsqueeze(1)  # Add channel dimension\n",
    "                raw_signals = raw_signals.unsqueeze(1)\n",
    "\n",
    "                # Generate signals\n",
    "                gen_signals = generator(noisy_signals)\n",
    "                \n",
    "                # Ensure the generated signals have the same length as the input signals\n",
    "                if gen_signals.shape[-1] != noisy_signals.shape[-1]:\n",
    "                    gen_signals = torch.nn.functional.interpolate(gen_signals, size=noisy_signals.shape[-1])\n",
    "\n",
    "                # Calculate SNR and RMSE\n",
    "                snr_value = calculate_snr(raw_signals.squeeze().cpu().numpy(), gen_signals.squeeze().cpu().detach().numpy())\n",
    "                rmse_value = calculate_rmse(raw_signals.squeeze().cpu().numpy(), gen_signals.squeeze().cpu().detach().numpy())\n",
    "\n",
    "                # Store results\n",
    "                results['Noise_Type'].append(noise_type)\n",
    "                results['SNR_Level'].append(snr)\n",
    "                results['Epoch'].append(epoch + 1)\n",
    "                results['Batch'].append(i + 1)\n",
    "                results['D_loss'].append(0)\n",
    "                results['G_loss'].append(0)\n",
    "                results['SNR'].append(snr_value)\n",
    "                results['RMSE'].append(rmse_value)\n",
    "                results['Type'].append('Test')\n",
    "\n",
    "                print(f\"[{noise_type} SNR {snr}] [Epoch {epoch + 1}/{num_epochs}] [Batch {i + 1}/{len(test_loader)}] [SNR: {snr_value}] [RMSE: {rmse_value}]\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Initialize models with and without z\n",
    "generator_with_z = Generator(use_z=True)\n",
    "generator_without_z = Generator(use_z=False)\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Train the models and collect results for each SNR level\n",
    "results_with_z = train(generator_with_z, discriminator, dataloaders, num_epochs=1, use_z=True)  # Adjusted for quick testing\n",
    "results_without_z = train(generator_without_z, discriminator, dataloaders, num_epochs=1, use_z=False)  # Adjusted for quick testing\n",
    "\n",
    "# Create DataFrames to display results\n",
    "df_results_with_z = pd.DataFrame(results_with_z)\n",
    "df_results_without_z = pd.DataFrame(results_without_z)\n",
    "\n",
    "# Average SNR and RMSE for each SNR level\n",
    "avg_snr_with_z = df_results_with_z.groupby('SNR_Level')['SNR'].mean()\n",
    "avg_rmse_with_z = df_results_with_z.groupby('SNR_Level')['RMSE'].mean()\n",
    "\n",
    "avg_snr_without_z = df_results_without_z.groupby('SNR_Level')['SNR'].mean()\n",
    "avg_rmse_without_z = df_results_without_z.groupby('SNR_Level')['RMSE'].mean()\n",
    "\n",
    "# Plot SNR vs Average SNR\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(avg_snr_with_z.index, avg_snr_with_z.values, label='With z')\n",
    "plt.plot(avg_snr_without_z.index, avg_snr_without_z.values, label='Without z')\n",
    "plt.xlabel('SNR (dB)')\n",
    "plt.ylabel('Average SNR (dB)')\n",
    "plt.title('SNR vs Average SNR')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot SNR vs RMSE\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(avg_rmse_with_z.index, avg_rmse_with_z.values, label='With z')\n",
    "plt.plot(avg_rmse_without_z.index, avg_rmse_without_z.values, label='Without z')\n",
    "plt.xlabel('SNR (dB)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('SNR vs RMSE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Display the DataFrames as a table\n",
    "import ace_tools as tools; tools.display_dataframe_to_user(name=\"Denoising Results with z\", dataframe=df_results_with_z)\n",
    "import ace_tools as tools; tools.display_dataframe_to_user(name=\"Denoising Results without z\", dataframe=df_results_without_z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5823d5-c4f8-40ea-a901-892e88df8277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
