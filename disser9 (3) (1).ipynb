{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nolwsarGudKD",
    "outputId": "43f38bec-5c47-4c2f-f9b5-f7ccd9fd9bde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal length: 650000\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'C:\\\\Users\\\\malik\\\\Desktop\\\\Disertation\\\\mit-bih-arrhythmia-database-1.0.0'\n",
    "noise_dir = 'C:\\\\Users\\\\malik\\\\Desktop\\\\Disertation\\\\mit-bih-noise-stress-test-database-1.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the extraction paths\n",
    "arrhythmia_extract_path = 'C:\\\\Users\\\\malik\\\\Desktop\\\\Disertation\\\\mit-bih-arrhythmia-database-1.0.0'\n",
    "noise_extract_path = 'C:\\\\Users\\\\malik\\\\Desktop\\\\Disertation\\\\mit-bih-noise-stress-test-database-1.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets extracted successfully!\n",
      "ECG and noise signals loaded successfully!\n",
      "Noisy ECG signals prepared successfully!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 2662400000 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 205\u001b[0m\n\u001b[0;32m    202\u001b[0m generator \u001b[38;5;241m=\u001b[39m Generator()\n\u001b[0;32m    203\u001b[0m discriminator \u001b[38;5;241m=\u001b[39m Discriminator()\n\u001b[1;32m--> 205\u001b[0m train(generator, discriminator, dataloader)\n",
      "Cell \u001b[1;32mIn[12], line 180\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(generator, discriminator, dataloader, num_epochs, lr)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# Train Discriminator\u001b[39;00m\n\u001b[0;32m    179\u001b[0m optimizer_D\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 180\u001b[0m real_output \u001b[38;5;241m=\u001b[39m discriminator(torch\u001b[38;5;241m.\u001b[39mcat((clean_signal, noisy_signal), \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    181\u001b[0m fake_signal \u001b[38;5;241m=\u001b[39m generator(noisy_signal)\n\u001b[0;32m    182\u001b[0m fake_output \u001b[38;5;241m=\u001b[39m discriminator(torch\u001b[38;5;241m.\u001b[39mcat((fake_signal\u001b[38;5;241m.\u001b[39mdetach(), noisy_signal), \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[12], line 146\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 146\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleakyrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x))\n\u001b[0;32m    147\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleakyrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))\n\u001b[0;32m    148\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleakyrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\conv.py:313\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\conv.py:309\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    307\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    308\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    310\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 2662400000 bytes."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wfdb\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Paths to the extracted datasets\n",
    "arrhythmia_extract_path = 'C:\\\\Users\\\\malik\\\\Desktop\\\\Disertation\\\\mit-bih-arrhythmia-database-1.0.0'\n",
    "noise_extract_path = 'C:\\\\Users\\\\malik\\\\Desktop\\\\Disertation\\\\mit-bih-noise-stress-test-database-1.0.0'\n",
    "\n",
    "print(\"Datasets extracted successfully!\")\n",
    "\n",
    "# Function to load ECG signals from the MIT-BIH Arrhythmia Database\n",
    "def load_arrhythmia_signals(path, record_list):\n",
    "    signals = []\n",
    "    for record in record_list:\n",
    "        record_path = os.path.join(path, record)\n",
    "        signal, _ = wfdb.rdsamp(record_path)\n",
    "        signals.append(signal[:, 0])  # Assuming single lead ECG (Lead II)\n",
    "    return np.array(signals)\n",
    "\n",
    "# Function to load noise signals from the MIT-BIH Noise Stress Test Database\n",
    "def load_noise_signals(path, noise_types):\n",
    "    noises = []\n",
    "    for noise in noise_types:\n",
    "        record_path = os.path.join(path, noise)\n",
    "        signal, _ = wfdb.rdsamp(record_path)\n",
    "        noises.append(signal[:, 0])  # Assuming single channel noise\n",
    "    return noises\n",
    "\n",
    "# Define the record list for the Arrhythmia Database\n",
    "arrhythmia_records = ['103', '105', '111', '116', '122', '205', '213', '219', '223', '230']\n",
    "# Define the noise types for the Noise Stress Test Database\n",
    "noise_types = ['bw', 'em', 'ma']  # Baseline Wander, Electrode Motion, Muscle Artifact\n",
    "\n",
    "# Load the signals\n",
    "arrhythmia_signals = load_arrhythmia_signals(arrhythmia_extract_path, arrhythmia_records)\n",
    "noise_signals = load_noise_signals(noise_extract_path, noise_types)\n",
    "\n",
    "print(\"ECG and noise signals loaded successfully!\")\n",
    "\n",
    "# Prepare noisy ECG signals by adding noise to the clean ECG signals\n",
    "def add_noise_to_signals(clean_signals, noise_signals, snr_dB):\n",
    "    noisy_signals = []\n",
    "    for clean_signal in clean_signals:\n",
    "        noise = noise_signals[np.random.choice(len(noise_signals))]\n",
    "        noise = noise[:len(clean_signal)]  # Ensure the noise length matches the signal length\n",
    "        noise_power = np.mean(noise ** 2)\n",
    "        signal_power = np.mean(clean_signal ** 2)\n",
    "        scale_factor = np.sqrt(signal_power / (noise_power * 10**(snr_dB / 10)))\n",
    "        noisy_signal = clean_signal + scale_factor * noise\n",
    "        noisy_signals.append(noisy_signal)\n",
    "    return np.array(noisy_signals)\n",
    "\n",
    "# Example SNR values\n",
    "snr_values = [0, 1, 2, 3, 4]\n",
    "\n",
    "# Generate noisy signals for each SNR value\n",
    "noisy_signals = []\n",
    "for snr in snr_values:\n",
    "    noisy_signals.extend(add_noise_to_signals(arrhythmia_signals, noise_signals, snr))\n",
    "\n",
    "noisy_signals = np.array(noisy_signals)\n",
    "\n",
    "print(\"Noisy ECG signals prepared successfully!\")\n",
    "\n",
    "# Custom Dataset\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, noisy_signals, clean_signals):\n",
    "        self.noisy_signals = noisy_signals\n",
    "        self.clean_signals = clean_signals\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.noisy_signals)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        noisy_signal = self.noisy_signals[idx]\n",
    "        clean_signal = self.clean_signals[idx % len(self.clean_signals)]\n",
    "        return torch.tensor(noisy_signal, dtype=torch.float32), torch.tensor(clean_signal, dtype=torch.float32)\n",
    "\n",
    "# Generator Model: Convolutional Auto-Encoder with Skip Connections\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoder_conv1 = nn.Conv1d(1, 16, kernel_size=5, stride=2, padding=2)    # 512 × 1 -> 512 × 16\n",
    "        self.encoder_conv2 = nn.Conv1d(16, 32, kernel_size=5, stride=2, padding=2)   # 512 × 16 -> 512 × 32\n",
    "        self.encoder_conv3 = nn.Conv1d(32, 64, kernel_size=5, stride=2, padding=2)   # 512 × 32 -> 512 × 64\n",
    "        self.encoder_conv4 = nn.Conv1d(64, 128, kernel_size=5, stride=2, padding=2)  # 512 × 64 -> 256 × 128\n",
    "        self.encoder_conv5 = nn.Conv1d(128, 256, kernel_size=5, stride=2, padding=2) # 256 × 128 -> 128 × 256\n",
    "        self.encoder_conv6 = nn.Conv1d(256, 512, kernel_size=5, stride=2, padding=2) # 128 × 256 -> 64 × 512\n",
    "        self.encoder_conv7 = nn.Conv1d(512, 1024, kernel_size=5, stride=2, padding=2)# 64 × 512 -> 32 × 1024\n",
    "        self.encoder_conv8 = nn.Conv1d(1024, 2048, kernel_size=5, stride=2, padding=2)# 32 × 1024 -> 16 × 2048\n",
    "        \n",
    "        self.decoder_conv1 = nn.ConvTranspose1d(2048, 1024, kernel_size=5, stride=2, padding=2, output_padding=1) # 16 × 2048 -> 32 × 1024\n",
    "        self.decoder_conv2 = nn.ConvTranspose1d(1024, 512, kernel_size=5, stride=2, padding=2, output_padding=1)  # 32 × 1024 -> 64 × 512\n",
    "        self.decoder_conv3 = nn.ConvTranspose1d(512, 256, kernel_size=5, stride=2, padding=2, output_padding=1)   # 64 × 512 -> 128 × 256\n",
    "        self.decoder_conv4 = nn.ConvTranspose1d(256, 128, kernel_size=5, stride=2, padding=2, output_padding=1)   # 128 × 256 -> 256 × 128\n",
    "        self.decoder_conv5 = nn.ConvTranspose1d(128, 64, kernel_size=5, stride=2, padding=2, output_padding=1)    # 256 × 128 -> 512 × 64\n",
    "        self.decoder_conv6 = nn.ConvTranspose1d(64, 32, kernel_size=5, stride=2, padding=2, output_padding=1)     # 512 × 64 -> 512 × 32\n",
    "        self.decoder_conv7 = nn.ConvTranspose1d(32, 16, kernel_size=5, stride=2, padding=2, output_padding=1)     # 512 × 32 -> 512 × 16\n",
    "        self.decoder_conv8 = nn.ConvTranspose1d(16, 1, kernel_size=5, stride=2, padding=2, output_padding=1)      # 512 × 16 -> 512 × 1\n",
    "        \n",
    "        self.prelu = nn.PReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder with skip connections\n",
    "        e1 = self.prelu(self.encoder_conv1(x))\n",
    "        e2 = self.prelu(self.encoder_conv2(e1))\n",
    "        e3 = self.prelu(self.encoder_conv3(e2))\n",
    "        e4 = self.prelu(self.encoder_conv4(e3))\n",
    "        e5 = self.prelu(self.encoder_conv5(e4))\n",
    "        e6 = self.prelu(self.encoder_conv6(e5))\n",
    "        e7 = self.prelu(self.encoder_conv7(e6))\n",
    "        e8 = self.prelu(self.encoder_conv8(e7))\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        d1 = self.prelu(self.decoder_conv1(e8)) + e7\n",
    "        d2 = self.prelu(self.decoder_conv2(d1)) + e6\n",
    "        d3 = self.prelu(self.decoder_conv3(d2)) + e5\n",
    "        d4 = self.prelu(self.decoder_conv4(d3)) + e4\n",
    "        d5 = self.prelu(self.decoder_conv5(d4)) + e3\n",
    "        d6 = self.prelu(self.decoder_conv6(d5)) + e2\n",
    "        d7 = self.prelu(self.decoder_conv7(d6)) + e1\n",
    "        d8 = self.prelu(self.decoder_conv8(d7))\n",
    "        \n",
    "        return d8\n",
    "\n",
    "# Discriminator Model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(2, 64, kernel_size=5, stride=2, padding=2)   # 512 -> 256\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, stride=2, padding=2) # 256 -> 128\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=5, stride=2, padding=2)# 128 -> 64\n",
    "        self.conv4 = nn.Conv1d(256, 512, kernel_size=5, stride=2, padding=2)# 64 -> 32\n",
    "        self.conv5 = nn.Conv1d(512, 1024, kernel_size=5, stride=2, padding=2)# 32 -> 16\n",
    "        self.conv6 = nn.Conv1d(1024, 2048, kernel_size=5, stride=2, padding=2)# 16 -> 8\n",
    "        self.leakyrelu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(2048 * 8, 1)  # Adjust the linear layer input size to match flattened output size\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leakyrelu(self.conv1(x))\n",
    "        x = self.leakyrelu(self.conv2(x))\n",
    "        x = self.leakyrelu(self.conv3(x))\n",
    "        x = self.leakyrelu(self.conv4(x))\n",
    "        x = self.leakyrelu(self.conv5(x))\n",
    "        x = self.leakyrelu(self.conv6(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Loss Functions\n",
    "def generator_loss(disc_output, gen_output, clean_signal, lambda_dist=0.7, lambda_max=0.2):\n",
    "    adv_loss = nn.BCELoss()(disc_output, torch.ones_like(disc_output))\n",
    "    dist_loss = torch.mean(torch.sqrt(torch.sum((gen_output - clean_signal) ** 2, dim=1)))\n",
    "    max_loss = torch.mean(torch.max(torch.abs(gen_output - clean_signal), dim=1)[0])\n",
    "    return adv_loss + lambda_dist * dist_loss + lambda_max * max_loss\n",
    "\n",
    "def discriminator_loss(disc_real_output, disc_fake_output):\n",
    "    real_loss = nn.BCELoss()(disc_real_output, torch.ones_like(disc_real_output))\n",
    "    fake_loss = nn.BCELoss()(disc_fake_output, torch.zeros_like(disc_fake_output))\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "# Training Function\n",
    "def train(generator, discriminator, dataloader, num_epochs=100, lr=0.0001):\n",
    "    optimizer_G = optim.RMSprop(generator.parameters(), lr=lr)\n",
    "    optimizer_D = optim.RMSprop(discriminator.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for noisy_signal, clean_signal in dataloader:\n",
    "            noisy_signal, clean_signal = noisy_signal.unsqueeze(1), clean_signal.unsqueeze(1)\n",
    "            \n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            real_output = discriminator(torch.cat((clean_signal, noisy_signal), 1))\n",
    "            fake_signal = generator(noisy_signal)\n",
    "            fake_output = discriminator(torch.cat((fake_signal.detach(), noisy_signal), 1))\n",
    "            d_loss = discriminator_loss(real_output, fake_output)\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            fake_signal = generator(noisy_signal)\n",
    "            fake_output = discriminator(torch.cat((fake_signal, noisy_signal), 1))\n",
    "            g_loss = generator_loss(fake_output, fake_signal, clean_signal)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Generator Loss: {g_loss.item()}, Discriminator Loss: {d_loss.item()}\")\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = ECGDataset(noisy_signals, arrhythmia_signals)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    generator = Generator()\n",
    "    discriminator = Discriminator()\n",
    "\n",
    "    train(generator, discriminator, dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
