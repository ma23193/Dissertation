{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['100.atr',\n",
       "  '100.dat',\n",
       "  '100.hea',\n",
       "  '100.xws',\n",
       "  '101.atr',\n",
       "  '101.dat',\n",
       "  '101.hea',\n",
       "  '101.xws',\n",
       "  '102-0.atr',\n",
       "  '102.atr',\n",
       "  '102.dat',\n",
       "  '102.hea',\n",
       "  '102.xws',\n",
       "  '103.atr',\n",
       "  '103.dat',\n",
       "  '103.hea',\n",
       "  '103.xws',\n",
       "  '104.atr',\n",
       "  '104.dat',\n",
       "  '104.hea',\n",
       "  '104.xws',\n",
       "  '105.atr',\n",
       "  '105.dat',\n",
       "  '105.hea',\n",
       "  '105.xws',\n",
       "  '106.atr',\n",
       "  '106.dat',\n",
       "  '106.hea',\n",
       "  '106.xws',\n",
       "  '107.atr',\n",
       "  '107.dat',\n",
       "  '107.hea',\n",
       "  '107.xws',\n",
       "  '108.atr',\n",
       "  '108.at_',\n",
       "  '108.dat',\n",
       "  '108.hea',\n",
       "  '108.xws',\n",
       "  '109.atr',\n",
       "  '109.dat',\n",
       "  '109.hea',\n",
       "  '109.xws',\n",
       "  '111.atr',\n",
       "  '111.dat',\n",
       "  '111.hea',\n",
       "  '111.xws',\n",
       "  '112.atr',\n",
       "  '112.dat',\n",
       "  '112.hea',\n",
       "  '112.xws',\n",
       "  '113.atr',\n",
       "  '113.dat',\n",
       "  '113.hea',\n",
       "  '113.xws',\n",
       "  '114.atr',\n",
       "  '114.dat',\n",
       "  '114.hea',\n",
       "  '114.xws',\n",
       "  '115.atr',\n",
       "  '115.dat',\n",
       "  '115.hea',\n",
       "  '115.xws',\n",
       "  '116.atr',\n",
       "  '116.dat',\n",
       "  '116.hea',\n",
       "  '116.xws',\n",
       "  '117.atr',\n",
       "  '117.at_',\n",
       "  '117.dat',\n",
       "  '117.hea',\n",
       "  '117.xws',\n",
       "  '118.atr',\n",
       "  '118.dat',\n",
       "  '118.hea',\n",
       "  '118.xws',\n",
       "  '119.atr',\n",
       "  '119.at_',\n",
       "  '119.dat',\n",
       "  '119.hea',\n",
       "  '119.xws',\n",
       "  '121.atr',\n",
       "  '121.dat',\n",
       "  '121.hea',\n",
       "  '121.xws',\n",
       "  '122.atr',\n",
       "  '122.dat',\n",
       "  '122.hea',\n",
       "  '122.xws',\n",
       "  '123.atr',\n",
       "  '123.dat',\n",
       "  '123.hea',\n",
       "  '123.xws',\n",
       "  '124.atr',\n",
       "  '124.dat',\n",
       "  '124.hea',\n",
       "  '124.xws',\n",
       "  '200.atr',\n",
       "  '200.dat',\n",
       "  '200.hea',\n",
       "  '200.xws',\n",
       "  '201.atr',\n",
       "  '201.dat',\n",
       "  '201.hea',\n",
       "  '201.xws',\n",
       "  '202.atr',\n",
       "  '202.dat',\n",
       "  '202.hea',\n",
       "  '202.xws',\n",
       "  '203.at-',\n",
       "  '203.atr',\n",
       "  '203.at_',\n",
       "  '203.dat',\n",
       "  '203.hea',\n",
       "  '203.xws',\n",
       "  '205.atr',\n",
       "  '205.dat',\n",
       "  '205.hea',\n",
       "  '205.xws',\n",
       "  '207.atr',\n",
       "  '207.dat',\n",
       "  '207.hea',\n",
       "  '207.xws',\n",
       "  '208.atr',\n",
       "  '208.dat',\n",
       "  '208.hea',\n",
       "  '208.xws',\n",
       "  '209.atr',\n",
       "  '209.at_',\n",
       "  '209.dat',\n",
       "  '209.hea',\n",
       "  '209.xws',\n",
       "  '210.atr',\n",
       "  '210.dat',\n",
       "  '210.hea',\n",
       "  '210.xws',\n",
       "  '212.atr',\n",
       "  '212.dat',\n",
       "  '212.hea',\n",
       "  '212.xws',\n",
       "  '213.atr',\n",
       "  '213.dat',\n",
       "  '213.hea',\n",
       "  '213.xws',\n",
       "  '214.atr',\n",
       "  '214.at_',\n",
       "  '214.dat',\n",
       "  '214.hea',\n",
       "  '214.xws',\n",
       "  '215.atr',\n",
       "  '215.at_',\n",
       "  '215.dat',\n",
       "  '215.hea',\n",
       "  '215.xws',\n",
       "  '217.atr',\n",
       "  '217.dat',\n",
       "  '217.hea',\n",
       "  '217.xws',\n",
       "  '219.atr',\n",
       "  '219.dat',\n",
       "  '219.hea',\n",
       "  '219.xws',\n",
       "  '220.atr',\n",
       "  '220.dat',\n",
       "  '220.hea',\n",
       "  '220.xws',\n",
       "  '221.atr',\n",
       "  '221.dat',\n",
       "  '221.hea',\n",
       "  '221.xws',\n",
       "  '222.atr',\n",
       "  '222.at_',\n",
       "  '222.dat',\n",
       "  '222.hea',\n",
       "  '222.xws',\n",
       "  '223.atr',\n",
       "  '223.dat',\n",
       "  '223.hea',\n",
       "  '223.xws',\n",
       "  '228.atr',\n",
       "  '228.dat',\n",
       "  '228.hea',\n",
       "  '228.xws',\n",
       "  '230.atr',\n",
       "  '230.dat',\n",
       "  '230.hea',\n",
       "  '230.xws',\n",
       "  '231.atr',\n",
       "  '231.dat',\n",
       "  '231.hea',\n",
       "  '231.xws',\n",
       "  '232.atr',\n",
       "  '232.dat',\n",
       "  '232.hea',\n",
       "  '232.xws',\n",
       "  '233.atr',\n",
       "  '233.dat',\n",
       "  '233.hea',\n",
       "  '233.xws',\n",
       "  '234.atr',\n",
       "  '234.dat',\n",
       "  '234.hea',\n",
       "  '234.xws',\n",
       "  'ANNOTATORS',\n",
       "  'mitdbdir',\n",
       "  'RECORDS',\n",
       "  'SHA256SUMS.txt',\n",
       "  'x_mitdb'],\n",
       " ['118e00.atr',\n",
       "  '118e00.dat',\n",
       "  '118e00.hea',\n",
       "  '118e00.xws',\n",
       "  '118e06.atr',\n",
       "  '118e06.dat',\n",
       "  '118e06.hea',\n",
       "  '118e06.xws',\n",
       "  '118e12.atr',\n",
       "  '118e12.dat',\n",
       "  '118e12.hea',\n",
       "  '118e12.xws',\n",
       "  '118e18.atr',\n",
       "  '118e18.dat',\n",
       "  '118e18.hea',\n",
       "  '118e18.xws',\n",
       "  '118e24.atr',\n",
       "  '118e24.dat',\n",
       "  '118e24.hea',\n",
       "  '118e24.xws',\n",
       "  '118e_6.atr',\n",
       "  '118e_6.dat',\n",
       "  '118e_6.hea',\n",
       "  '118e_6.xws',\n",
       "  '119e00.atr',\n",
       "  '119e00.dat',\n",
       "  '119e00.hea',\n",
       "  '119e00.xws',\n",
       "  '119e06.atr',\n",
       "  '119e06.dat',\n",
       "  '119e06.hea',\n",
       "  '119e06.xws',\n",
       "  '119e12.atr',\n",
       "  '119e12.dat',\n",
       "  '119e12.hea',\n",
       "  '119e12.xws',\n",
       "  '119e18.atr',\n",
       "  '119e18.dat',\n",
       "  '119e18.hea',\n",
       "  '119e18.xws',\n",
       "  '119e24.atr',\n",
       "  '119e24.dat',\n",
       "  '119e24.hea',\n",
       "  '119e24.xws',\n",
       "  '119e_6.atr',\n",
       "  '119e_6.dat',\n",
       "  '119e_6.hea',\n",
       "  '119e_6.xws',\n",
       "  'ANNOTATORS',\n",
       "  'bw.dat',\n",
       "  'bw.hea',\n",
       "  'bw.hea-',\n",
       "  'bw.xws',\n",
       "  'em.dat',\n",
       "  'em.hea',\n",
       "  'em.hea-',\n",
       "  'em.xws',\n",
       "  'ma.dat',\n",
       "  'ma.hea',\n",
       "  'ma.hea-',\n",
       "  'ma.xws',\n",
       "  'nstdb.doc',\n",
       "  'nstdb.txt',\n",
       "  'nstdbgen',\n",
       "  'nstdbgen-',\n",
       "  'old',\n",
       "  'RECORDS',\n",
       "  'SHA256SUMS.txt'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Paths to the uploaded zip files\n",
    "mit_bih_path = 'M:\\\\Dissertation\\\\mit-bih-arrhythmia-database-1.0.0.zip'\n",
    "noise_test_path = 'M:\\\\Dissertation\\\\mit-bih-noise-stress-test-database-1.0.0.zip'\n",
    "extract_dir = 'M:\\\\Dissertation\\\\New folder'\n",
    "\n",
    "# Function to unzip the files\n",
    "def unzip_file(zip_path, extract_to):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "# Unzip the files\n",
    "unzip_file(mit_bih_path, extract_dir)\n",
    "unzip_file(noise_test_path, extract_dir)\n",
    "\n",
    "# List the extracted files to verify\n",
    "os.listdir('M:\\\\Dissertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0'), os.listdir('M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "649984"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wfdb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, raw_signals, noisy_signals):\n",
    "        self.raw_signals = raw_signals\n",
    "        self.noisy_signals = noisy_signals\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_signals)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        raw_signal = self.raw_signals[idx]\n",
    "        noisy_signal = self.noisy_signals[idx]\n",
    "        return torch.tensor(raw_signal, dtype=torch.float32), torch.tensor(noisy_signal, dtype=torch.float32)\n",
    "\n",
    "def load_mit_bih_data(records, target_length=650000):\n",
    "    raw_signals = []\n",
    "    noisy_signals = []\n",
    "    \n",
    "    for record in records:\n",
    "        raw_record = wfdb.rdrecord(f'M:\\\\Dissertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0/{record}')\n",
    "        raw_signal = raw_record.p_signal[:, 0]  # Use the first channel for simplicity\n",
    "        \n",
    "        # Load noise and add it to the raw signal\n",
    "        noise_record = wfdb.rdrecord(f'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0/em')  # Example for EM noise\n",
    "        noise_signal = noise_record.p_signal[:, 0]\n",
    "        \n",
    "        # Ensure the signals are of the same length\n",
    "        min_length = min(len(raw_signal), len(noise_signal), target_length)\n",
    "        raw_signal = raw_signal[:min_length]\n",
    "        noise_signal = noise_signal[:min_length]\n",
    "        noisy_signal = raw_signal + noise_signal\n",
    "        \n",
    "        # Pad signals to target length\n",
    "        if min_length < target_length:\n",
    "            raw_signal = np.pad(raw_signal, (0, target_length - min_length), 'constant')\n",
    "            noisy_signal = np.pad(noisy_signal, (0, target_length - min_length), 'constant')\n",
    "        \n",
    "        raw_signals.append(raw_signal)\n",
    "        noisy_signals.append(noisy_signal)\n",
    "    \n",
    "    return np.array(raw_signals), np.array(noisy_signals)\n",
    "\n",
    "# Select 10 records for the experiment\n",
    "records = ['103', '105', '111', '116', '122', '205','213', '219', '223', '230']\n",
    "target_length = 649984\n",
    "raw_signals, noisy_signals = load_mit_bih_data(records, target_length=target_length)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = ECGDataset(raw_signals, noisy_signals)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/5] [Batch 0/1] [D loss: 0.6934985518455505] [G loss: 0.7065462470054626]\n",
      "[Epoch 1/5] [Batch 0/1] [D loss: 0.6899080276489258] [G loss: 0.6821267008781433]\n",
      "[Epoch 2/5] [Batch 0/1] [D loss: 0.6868503093719482] [G loss: 0.6739291548728943]\n",
      "[Epoch 3/5] [Batch 0/1] [D loss: 0.6832177639007568] [G loss: 0.6760920286178589]\n",
      "[Epoch 4/5] [Batch 0/1] [D loss: 0.6786555647850037] [G loss: 0.6850621104240417]\n"
     ]
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(16, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(512, 1024, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(1024, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(32, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(16, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv1d(2, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(512, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x).view(x.size(0), -1)\n",
    "\n",
    "def train(generator, discriminator, dataloader, num_epochs=5, lr=0.0002):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (raw_signals, noisy_signals) in enumerate(dataloader):\n",
    "            batch_size = raw_signals.size(0)\n",
    "            \n",
    "            # Ensure the signals have the same length\n",
    "            min_length = min(raw_signals.shape[-1], noisy_signals.shape[-1])\n",
    "            raw_signals = raw_signals[:, :min_length]\n",
    "            noisy_signals = noisy_signals[:, :min_length]\n",
    "\n",
    "            # Denoise the noisy signals\n",
    "            noisy_signals = noisy_signals.unsqueeze(1)  # Add channel dimension\n",
    "            raw_signals = raw_signals.unsqueeze(1)\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            gen_signals = generator(noisy_signals)\n",
    "            \n",
    "            # Update valid and fake labels to match the discriminator output size\n",
    "            disc_output_size = discriminator(torch.cat((gen_signals, noisy_signals), 1)).size()\n",
    "            valid = torch.ones(disc_output_size).to(gen_signals.device)\n",
    "            fake = torch.zeros(disc_output_size).to(gen_signals.device)\n",
    "            \n",
    "            g_loss = criterion(discriminator(torch.cat((gen_signals, noisy_signals), 1)), valid)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            real_loss = criterion(discriminator(torch.cat((raw_signals, noisy_signals), 1)), valid)\n",
    "            fake_loss = criterion(discriminator(torch.cat((gen_signals.detach(), noisy_signals), 1)), fake)\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            print(f\"[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(dataloader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Train the models\n",
    "train(generator, discriminator, dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
