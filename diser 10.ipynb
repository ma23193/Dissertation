{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e460e1c2-35b0-41be-867e-7d64b6904010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wfdb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, raw_signals, noisy_signals):\n",
    "        self.raw_signals = raw_signals\n",
    "        self.noisy_signals = noisy_signals\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_signals)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        raw_signal = self.raw_signals[idx]\n",
    "        noisy_signal = self.noisy_signals[idx]\n",
    "        return torch.tensor(raw_signal, dtype=torch.float32), torch.tensor(noisy_signal, dtype=torch.float32)\n",
    "\n",
    "def add_noise(signal, noise, snr):\n",
    "    signal_power = np.mean(signal ** 2)\n",
    "    noise_power = np.mean(noise ** 2)\n",
    "    factor = (signal_power / noise_power) / (10 ** (snr / 10))\n",
    "    noisy_signal = signal + noise * np.sqrt(factor)\n",
    "    return noisy_signal\n",
    "\n",
    "def load_mit_bih_data(records, noise_type, snr_levels, target_length=650000):\n",
    "    raw_signals = []\n",
    "    noisy_signals_dict = {snr: [] for snr in snr_levels}\n",
    "    \n",
    "    for record in records:\n",
    "        raw_record = wfdb.rdrecord(f'M:\\\\Dissertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0/{record}')\n",
    "        raw_signal = raw_record.p_signal[:, 0]  # Use the first channel for simplicity\n",
    "        \n",
    "        # Load noise and add it to the raw signal\n",
    "        noise_record = wfdb.rdrecord(f'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0/{noise_type}')\n",
    "        noise_signal = noise_record.p_signal[:, 0]\n",
    "        \n",
    "        # Ensure the signals are of the same length\n",
    "        min_length = min(len(raw_signal), len(noise_signal), target_length)\n",
    "        raw_signal = raw_signal[:min_length]\n",
    "        noise_signal = noise_signal[:min_length]\n",
    "        \n",
    "        # Pad signals to target length\n",
    "        if min_length < target_length:\n",
    "            raw_signal = np.pad(raw_signal, (0, target_length - min_length), 'constant')\n",
    "            noise_signal = np.pad(noise_signal, (0, target_length - min_length), 'constant')\n",
    "        \n",
    "        raw_signals.append(raw_signal)\n",
    "        \n",
    "        for snr in snr_levels:\n",
    "            noisy_signal = add_noise(raw_signal, noise_signal, snr)\n",
    "            noisy_signals_dict[snr].append(noisy_signal)\n",
    "    \n",
    "    return np.array(raw_signals), {snr: np.array(noisy_signals_dict[snr]) for snr in snr_levels}\n",
    "\n",
    "# Select records and noise types for the experiment\n",
    "records = ['103', '105', '111', '116', '122', '205', '213', '219', '223', '230']\n",
    "noise_types = ['em', 'bw', 'ma']\n",
    "combined_noise_types = ['em+bw', 'ma+em', 'ma+em+bw']\n",
    "snr_levels = [0, 1, 2, 3, 4, 5]\n",
    "target_length = 649984\n",
    "\n",
    "raw_signals, noisy_signals_dict = {}, {}\n",
    "for noise_type in noise_types:\n",
    "    raw_signals[noise_type], noisy_signals_dict[noise_type] = load_mit_bih_data(records, noise_type, snr_levels, target_length)\n",
    "\n",
    "# For combined noise types, combine the corresponding noises\n",
    "for combined_noise in combined_noise_types:\n",
    "    components = combined_noise.split('+')\n",
    "    combined_raw_signals, combined_noisy_signals = [], {snr: [] for snr in snr_levels}\n",
    "    for i in range(len(records)):\n",
    "        combined_signal = np.zeros(target_length)\n",
    "        for component in components:\n",
    "            raw_signal = raw_signals[component][i]\n",
    "            combined_signal += raw_signal / len(components)  # Average the signals\n",
    "        combined_raw_signals.append(combined_signal)\n",
    "        for snr in snr_levels:\n",
    "            combined_noise_signal = np.zeros(target_length)\n",
    "            for component in components:\n",
    "                noise_signal = noisy_signals_dict[component][snr][i]\n",
    "                combined_noise_signal += noise_signal / len(components)  # Average the noises\n",
    "            combined_noisy_signals[snr].append(combined_noise_signal)\n",
    "    raw_signals[combined_noise] = np.array(combined_raw_signals)\n",
    "    noisy_signals_dict[combined_noise] = {snr: np.array(combined_noisy_signals[snr]) for snr in snr_levels}\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "datasets = {}\n",
    "for noise_type in noise_types + combined_noise_types:\n",
    "    for snr in snr_levels:\n",
    "        datasets[(noise_type, snr)] = ECGDataset(raw_signals[noise_type], noisy_signals_dict[noise_type][snr])\n",
    "\n",
    "dataloaders = {key: DataLoader(dataset, batch_size=16, shuffle=True) for key, dataset in datasets.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e535b5b1-b009-48bb-a616-8479f27ecac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_snr(original, denoised):\n",
    "    noise = original - denoised\n",
    "    snr = 10 * np.log10(np.sum(original**2) / np.sum(noise**2))\n",
    "    return snr\n",
    "\n",
    "def calculate_rmse(original, denoised):\n",
    "    mse = np.mean((original - denoised) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "543ed098-a5ae-4416-bd66-09bf6e5fa271",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 64\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Initialize models\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[43mGenerator\u001b[49m()\n\u001b[0;32m     65\u001b[0m discriminator \u001b[38;5;241m=\u001b[39m Discriminator()\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Train the models and collect results\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Generator' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def train(generator, discriminator, dataloaders, num_epochs=5, lr=0.0002):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    \n",
    "    results = {'Noise_Type': [], 'SNR_Level': [], 'Epoch': [], 'Batch': [], 'D_loss': [], 'G_loss': [], 'SNR': [], 'RMSE': []}\n",
    "    \n",
    "    for (noise_type, snr), dataloader in dataloaders.items():\n",
    "        for epoch in range(num_epochs):\n",
    "            for i, (raw_signals, noisy_signals) in enumerate(dataloader):\n",
    "                batch_size = raw_signals.size(0)\n",
    "                \n",
    "                # Ensure the signals have the same length\n",
    "                min_length = min(raw_signals.shape[-1], noisy_signals.shape[-1])\n",
    "                raw_signals = raw_signals[:, :min_length]\n",
    "                noisy_signals = noisy_signals[:, :min_length]\n",
    "\n",
    "                # Denoise the noisy signals\n",
    "                noisy_signals = noisy_signals.unsqueeze(1)  # Add channel dimension\n",
    "                raw_signals = raw_signals.unsqueeze(1)\n",
    "\n",
    "                # Train Generator\n",
    "                optimizer_G.zero_grad()\n",
    "                gen_signals = generator(noisy_signals)\n",
    "                \n",
    "                # Update valid and fake labels to match the discriminator output size\n",
    "                disc_output_size = discriminator(torch.cat((gen_signals, noisy_signals), 1)).size()\n",
    "                valid = torch.ones(disc_output_size).to(gen_signals.device)\n",
    "                fake = torch.zeros(disc_output_size).to(gen_signals.device)\n",
    "                \n",
    "                g_loss = criterion(discriminator(torch.cat((gen_signals, noisy_signals), 1)), valid)\n",
    "                g_loss.backward()\n",
    "                optimizer_G.step()\n",
    "\n",
    "                # Train Discriminator\n",
    "                optimizer_D.zero_grad()\n",
    "                real_loss = criterion(discriminator(torch.cat((raw_signals, noisy_signals), 1)), valid)\n",
    "                fake_loss = criterion(discriminator(torch.cat((gen_signals.detach(), noisy_signals), 1)), fake)\n",
    "                d_loss = (real_loss + fake_loss) / 2\n",
    "                d_loss.backward()\n",
    "                optimizer_D.step()\n",
    "                \n",
    "                # Calculate SNR and RMSE\n",
    "                snr_value = calculate_snr(raw_signals.squeeze().cpu().numpy(), gen_signals.squeeze().cpu().detach().numpy())\n",
    "                rmse_value = calculate_rmse(raw_signals.squeeze().cpu().numpy(), gen_signals.squeeze().cpu().detach().numpy())\n",
    "\n",
    "                # Store results\n",
    "                results['Noise_Type'].append(noise_type)\n",
    "                results['SNR_Level'].append(snr)\n",
    "                results['Epoch'].append(epoch + 1)\n",
    "                results['Batch'].append(i + 1)\n",
    "                results['D_loss'].append(d_loss.item())\n",
    "                results['G_loss'].append(g_loss.item())\n",
    "                results['SNR'].append(snr_value)\n",
    "                results['RMSE'].append(rmse_value)\n",
    "                \n",
    "                print(f\"[{noise_type} SNR {snr}] [Epoch {epoch + 1}/{num_epochs}] [Batch {i + 1}/{len(dataloader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}] [SNR: {snr_value}] [RMSE: {rmse_value}]\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Train the models and collect results\n",
    "results = train(generator, discriminator, dataloaders)\n",
    "\n",
    "# Create a DataFrame to display results\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results)\n",
    "\n",
    "# Display the DataFrame as a table\n",
    "import ace_tools as tools; tools.display_dataframe_to_user(name=\"Denoising Results of Proposed Method\", dataframe=df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bc7582-49fa-48d8-914e-fa2919a46aba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98819342-44a6-4fa2-896b-fc649402f192",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
