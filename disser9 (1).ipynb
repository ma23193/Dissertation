{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nolwsarGudKD",
    "outputId": "43f38bec-5c47-4c2f-f9b5-f7ccd9fd9bde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal length: 650000\n"
     ]
    }
   ],
   "source": [
    "import wfdb\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def list_records(base_dir):\n",
    "    return [f.split('.')[0] for f in os.listdir(base_dir) if f.endswith('.dat')]\n",
    "\n",
    "def load_all_data(data_dir, noise_dir):\n",
    "    data_records = list_records(data_dir)\n",
    "    noise_records = list_records(noise_dir)\n",
    "\n",
    "    all_noisy_signals = []\n",
    "    all_clean_signals = []\n",
    "\n",
    "    signal_length = None\n",
    "\n",
    "    for record_name in data_records:\n",
    "        noise_record_name = next((nr for nr in noise_records if nr.startswith(record_name[:3])), None)\n",
    "        if not noise_record_name:\n",
    "            continue\n",
    "\n",
    "        record_path = os.path.join(data_dir, record_name)\n",
    "        noise_record_path = os.path.join(noise_dir, noise_record_name)\n",
    "\n",
    "        record = wfdb.rdrecord(record_path)\n",
    "        noise_record = wfdb.rdrecord(noise_record_path)\n",
    "\n",
    "        signal = record.p_signal\n",
    "        noise_signal = noise_record.p_signal[:len(signal)]\n",
    "\n",
    "        noisy_signal = signal + noise_signal\n",
    "\n",
    "        if signal_length is None:\n",
    "            signal_length = len(signal)\n",
    "        elif signal_length != len(signal):\n",
    "            raise ValueError(\"Signal lengths are not consistent across the dataset.\")\n",
    "\n",
    "        all_noisy_signals.append(noisy_signal)\n",
    "        all_clean_signals.append(signal)\n",
    "\n",
    "    return np.array(all_noisy_signals), np.array(all_clean_signals), signal_length\n",
    "\n",
    "data_dir = 'M:\\Dissertation\\mit-bih-arrhythmia-database-1.0.0'\n",
    "noise_dir = 'M:\\Dissertation\\mit-bih-noise-stress-test-database-1.0.0'\n",
    "\n",
    "noisy_signals, clean_signals, signal_length = load_all_data(data_dir, noise_dir)\n",
    "\n",
    "# Check the determined signal length\n",
    "print(f\"Signal length: {signal_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "meks8Kabs6jP"
   },
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__Fill_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[650000,512] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu [Op:Fill] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Use the exact length determined from the dataset\u001b[39;00m\n\u001b[0;32m     53\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (signal_length, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 54\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m generator\u001b[38;5;241m.\u001b[39msummary()\n",
      "Cell \u001b[1;32mIn[49], line 7\u001b[0m, in \u001b[0;36mbuild_generator\u001b[1;34m(input_shape)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Encoder\u001b[39;00m\n\u001b[0;32m      6\u001b[0m e1 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mConv1D(\u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m)(inputs)\n\u001b[1;32m----> 7\u001b[0m e1 \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPReLU\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43me1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m e2 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mConv1D(\u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m16\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m)(e1)\n\u001b[0;32m      9\u001b[0m e2 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mPReLU()(e2)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\dtensor\\python\\api.py:64\u001b[0m, in \u001b[0;36mcall_with_layout\u001b[1;34m(fn, layout, *args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m relayout(fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), layout)\n\u001b[1;32m---> 64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__Fill_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[650000,512] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu [Op:Fill] name: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "def build_generator(input_shape):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    e1 = tf.keras.layers.Conv1D(512, 1, padding='same', activation='linear')(inputs)\n",
    "    e1 = tf.keras.layers.PReLU()(e1)\n",
    "    e2 = tf.keras.layers.Conv1D(512, 16, padding='same', activation='linear')(e1)\n",
    "    e2 = tf.keras.layers.PReLU()(e2)\n",
    "    e3 = tf.keras.layers.Conv1D(512, 32, padding='same', activation='linear')(e2)\n",
    "    e3 = tf.keras.layers.PReLU()(e3)\n",
    "    e4 = tf.keras.layers.Conv1D(512, 64, padding='same', activation='linear')(e3)\n",
    "    e4 = tf.keras.layers.PReLU()(e4)\n",
    "    e5 = tf.keras.layers.Conv1D(256, 128, padding='same', activation='linear')(e4)\n",
    "    e5 = tf.keras.layers.PReLU()(e5)\n",
    "    e6 = tf.keras.layers.Conv1D(128, 256, padding='same', activation='linear')(e5)\n",
    "    e6 = tf.keras.layers.PReLU()(e6)\n",
    "    e7 = tf.keras.layers.Conv1D(64, 512, padding='same', activation='linear')(e6)\n",
    "    e7 = tf.keras.layers.PReLU()(e7)\n",
    "    e8 = tf.keras.layers.Conv1D(32, 1024, padding='same', activation='linear')(e7)\n",
    "    e8 = tf.keras.layers.PReLU()(e8)\n",
    "\n",
    "    # Decoder with skip connections\n",
    "    d7 = tf.keras.layers.Conv1DTranspose(64, 512, padding='same', activation='linear')(e8)\n",
    "    d7 = tf.keras.layers.PReLU()(d7)\n",
    "    d7 = tf.keras.layers.Concatenate()([d7, e7])\n",
    "    d6 = tf.keras.layers.Conv1DTranspose(128, 256, padding='same', activation='linear')(d7)\n",
    "    d6 = tf.keras.layers.PReLU()(d6)\n",
    "    d6 = tf.keras.layers.Concatenate()([d6, e6])\n",
    "    d5 = tf.keras.layers.Conv1DTranspose(256, 128, padding='same', activation='linear')(d6)\n",
    "    d5 = tf.keras.layers.PReLU()(d5)\n",
    "    d5 = tf.keras.layers.Concatenate()([d5, e5])\n",
    "    d4 = tf.keras.layers.Conv1DTranspose(512, 64, padding='same', activation='linear')(d5)\n",
    "    d4 = tf.keras.layers.PReLU()(d4)\n",
    "    d4 = tf.keras.layers.Concatenate()([d4, e4])\n",
    "    d3 = tf.keras.layers.Conv1DTranspose(512, 32, padding='same', activation='linear')(d4)\n",
    "    d3 = tf.keras.layers.PReLU()(d3)\n",
    "    d3 = tf.keras.layers.Concatenate()([d3, e3])\n",
    "    d2 = tf.keras.layers.Conv1DTranspose(512, 16, padding='same', activation='linear')(d3)\n",
    "    d2 = tf.keras.layers.PReLU()(d2)\n",
    "    d2 = tf.keras.layers.Concatenate()([d2, e2])\n",
    "    d1 = tf.keras.layers.Conv1DTranspose(512, 1, padding='same', activation='linear')(d2)\n",
    "    d1 = tf.keras.layers.PReLU()(d1)\n",
    "    d1 = tf.keras.layers.Concatenate()([d1, e1])\n",
    "\n",
    "    outputs = tf.keras.layers.Conv1DTranspose(1, 1, padding='same', activation='linear')(d1)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs, name='generator')\n",
    "    return model\n",
    "\n",
    "# Use the exact length determined from the dataset\n",
    "\n",
    "input_shape = (signal_length, 1)\n",
    "generator = build_generator(input_shape)\n",
    "generator.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Px9gxnsx5fKf"
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_discriminator(input_shape):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    d = tf.keras.layers.Conv1D(64, 5, strides=2, padding='same')(inputs)\n",
    "    d = tf.keras.layers.BatchNormalization()(d)\n",
    "    d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)\n",
    "    d = tf.keras.layers.Conv1D(128, 5, strides=2, padding='same')(d)\n",
    "    d = tf.keras.layers.BatchNormalization()(d)\n",
    "    d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)\n",
    "    d = tf.keras.layers.Conv1D(256, 5, strides=2, padding='same')(d)\n",
    "    d = tf.keras.layers.BatchNormalization()(d)\n",
    "    d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)\n",
    "    d = tf.keras.layers.Conv1D(512, 5, strides=2, padding='same')(d)\n",
    "    d = tf.keras.layers.BatchNormalization()(d)\n",
    "    d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "    d = tf.keras.layers.Flatten()(d)\n",
    "    d = tf.keras.layers.Dense(1, activation='sigmoid')(d)\n",
    "\n",
    "    model = tf.keras.Model(inputs, d, name='discriminator')\n",
    "    return model\n",
    "\n",
    "# The discriminator input should match the combined shape of noisy and clean signals\n",
    "discriminator_input_shape = (signal_length, 2)\n",
    "discriminator = build_discriminator(discriminator_input_shape)\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yhGVE-1Gs80o"
   },
   "outputs": [],
   "source": [
    "noisy_signals = noisy_signals.reshape(-1, signal_length, 1)\n",
    "clean_signals = clean_signals.reshape(-1, signal_length, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPF0NmDcs-al"
   },
   "outputs": [],
   "source": [
    "def create_dataset(noisy_signals, clean_signals, batch_size=32):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((noisy_signals, clean_signals))\n",
    "    dataset = dataset.shuffle(buffer_size=1024).batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "batch_size = 32\n",
    "dataset = create_dataset(noisy_signals, clean_signals, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0NN2cK4js_6_"
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "def generator_loss(fake_output, real_signals, generated_signals):\n",
    "    gen_loss = tf.reduce_mean(tf.square(generated_signals - real_signals))\n",
    "    return gen_loss\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = tf.keras.losses.binary_crossentropy(tf.ones_like(real_output), real_output, from_logits=True)\n",
    "    fake_loss = tf.keras.losses.binary_crossentropy(tf.zeros_like(fake_output), fake_output, from_logits=True)\n",
    "    disc_loss = real_loss + fake_loss\n",
    "    return disc_loss\n",
    "\n",
    "# Training step function with GradientTape\n",
    "@tf.function\n",
    "def train_step(noisy_signals, real_signals):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_signals = generator(noisy_signals, training=True)\n",
    "        generated_signals = tf.cast(generated_signals, tf.float32)\n",
    "\n",
    "        # Ensure noisy_signals is also float32\n",
    "        noisy_signals = tf.cast(noisy_signals, tf.float32)\n",
    "\n",
    "        real_concat = tf.concat([real_signals, noisy_signals], axis=2)\n",
    "        fake_concat = tf.concat([generated_signals, noisy_signals], axis=2)\n",
    "\n",
    "        real_output = discriminator(real_concat, training=True)\n",
    "        fake_output = discriminator(fake_concat, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output, real_signals, generated_signals)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        epoch_gen_loss_avg = tf.keras.metrics.Mean()\n",
    "        epoch_disc_loss_avg = tf.keras.metrics.Mean()\n",
    "\n",
    "        for noisy_signals_batch, real_signals_batch in dataset:\n",
    "            gen_loss, disc_loss = train_step(noisy_signals_batch, real_signals_batch)\n",
    "            epoch_gen_loss_avg.update_state(gen_loss)\n",
    "            epoch_disc_loss_avg.update_state(disc_loss)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Generator Loss: {epoch_gen_loss_avg.result()}, Discriminator Loss: {epoch_disc_loss_avg.result()}')\n",
    "\n",
    "# Number of epochs for training\n",
    "epochs = 5\n",
    "train(dataset, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
