{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ma23193/Dissertation/blob/main/diser6(update).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vzwj_SSePgJ",
        "outputId": "1bce2bee-04f0-4174-b367-42478ee5ba63"
      },
      "id": "-vzwj_SSePgJ",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "cf92d797-3ab8-450d-87f7-2370f980f445",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf92d797-3ab8-450d-87f7-2370f980f445",
        "outputId": "12e2ad8c-af65-4ce7-9f80-240f93ecd702"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available arrhythmia records: ['100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '111', '112', '113', '114', '115', '116', '117', '118', '119', '121', '122', '123', '124', '200', '201', '202', '203', '205', '207', '208', '209', '210', '212', '213', '214', '215', '217', '219', '220', '221', '222', '223', '228', '230', '231', '232', '233', '234']\n",
            "Available noise records: ['118e00', '118e06', '118e12', '118e18', '118e24', '118e_6', '119e00', '119e06', '119e12', '119e18', '119e24', '119e_6', 'bw', 'em', 'ma']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Function to list available records in a directory\n",
        "def list_records(directory):\n",
        "    records = [f.split('.')[0] for f in os.listdir(directory) if f.endswith('.dat')]\n",
        "    return sorted(set(records))\n",
        "\n",
        "# Paths to the folders containing the extracted datasets\n",
        "arrhythmia_folder_path = '/content/drive/MyDrive/Dissertation/mit-bih-arrhythmia-database-1.0.0'\n",
        "noise_folder_path = '/content/drive/MyDrive/Dissertation/mit-bih-noise-stress-test-database-1.0.0'\n",
        "\n",
        "arrhythmia_records = list_records(arrhythmia_folder_path)\n",
        "noise_records = list_records(noise_folder_path)\n",
        "\n",
        "print(f\"Available arrhythmia records: {arrhythmia_records}\")\n",
        "print(f\"Available noise records: {noise_records}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "10b0c0c7-9ff7-4c2f-a73e-98f08556231e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10b0c0c7-9ff7-4c2f-a73e-98f08556231e",
        "outputId": "8a1e19da-fa66-4c57-9307-4c1930aa2aac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean data shape: (9750000,)\n",
            "Noisy data shape: (9750000,)\n",
            "Train loader created with 3046 samples.\n",
            "Test loader created with 761 samples.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import wfdb\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load ECG data\n",
        "def load_ecg_data(record_path):\n",
        "    record = wfdb.rdrecord(record_path)\n",
        "    signal = record.p_signal[:, 0]  # Use only the first channel\n",
        "    return signal\n",
        "\n",
        "# Preprocess data\n",
        "def preprocess_data(arrhythmia_folder, noise_folder, train_records, test_records):\n",
        "    clean_signals = []\n",
        "    noise_signals = []\n",
        "\n",
        "    # Load arrhythmia records\n",
        "    for record in train_records + test_records:\n",
        "        record_path = os.path.join(arrhythmia_folder, record)\n",
        "        if os.path.exists(record_path + '.dat'):\n",
        "            signal = load_ecg_data(record_path)\n",
        "            clean_signals.append(signal)\n",
        "\n",
        "    # Load noise records\n",
        "    for record in noise_records:\n",
        "        record_path = os.path.join(noise_folder, record)\n",
        "        if os.path.exists(record_path + '.dat'):\n",
        "            signal = load_ecg_data(record_path)\n",
        "            noise_signals.append(signal)\n",
        "\n",
        "    if clean_signals:\n",
        "        clean_signals = np.concatenate(clean_signals, axis=0)\n",
        "    else:\n",
        "        clean_signals = np.empty((0,))\n",
        "\n",
        "    if noise_signals:\n",
        "        noise_signals = np.concatenate(noise_signals, axis=0)\n",
        "    else:\n",
        "        noise_signals = np.empty((0,))\n",
        "\n",
        "    min_length = min(len(clean_signals), len(noise_signals))\n",
        "    clean_signals = clean_signals[:min_length]\n",
        "    noise_signals = noise_signals[:min_length]\n",
        "\n",
        "    if clean_signals.size > 0 and noise_signals.size > 0:\n",
        "        # Create noisy signals by adding noise to clean signals\n",
        "        noisy_signals = clean_signals + noise_signals\n",
        "        # Normalize signals\n",
        "        scaler = MinMaxScaler()\n",
        "        clean_signals = scaler.fit_transform(clean_signals.reshape(-1, 1)).flatten()\n",
        "        noisy_signals = scaler.transform(noisy_signals.reshape(-1, 1)).flatten()\n",
        "    else:\n",
        "        noisy_signals = np.empty((0,))\n",
        "\n",
        "    return clean_signals, noisy_signals\n",
        "\n",
        "# Define the records to use for training and testing\n",
        "train_records = ['100', '101', '102', '103', '104', '105', '106', '107', '108', '109']\n",
        "test_records = ['111', '112', '113', '114', '115', '116', '117', '118', '119', '121']\n",
        "\n",
        "# Filter out unavailable records\n",
        "train_records = [rec for rec in train_records if rec in arrhythmia_records]\n",
        "test_records = [rec for rec in test_records if rec in arrhythmia_records]\n",
        "\n",
        "# Paths to the folders containing the extracted datasets\n",
        "arrhythmia_folder_path = '/content/drive/MyDrive/Dissertation/mit-bih-arrhythmia-database-1.0.0'\n",
        "noise_folder_path = '/content/drive/MyDrive/Dissertation/mit-bih-noise-stress-test-database-1.0.0'\n",
        "\n",
        "clean_data, noisy_data = preprocess_data(arrhythmia_folder_path, noise_folder_path, train_records, test_records)\n",
        "\n",
        "print(f\"Clean data shape: {clean_data.shape}\")\n",
        "print(f\"Noisy data shape: {noisy_data.shape}\")\n",
        "\n",
        "class ECGDataset(Dataset):\n",
        "    def __init__(self, clean_data, noisy_data, segment_length=2560):\n",
        "        self.clean_data = clean_data\n",
        "        self.noisy_data = noisy_data\n",
        "        self.segment_length = segment_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.clean_data) // self.segment_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start_idx = idx * self.segment_length\n",
        "        end_idx = start_idx + self.segment_length\n",
        "        clean_sample = self.clean_data[start_idx:end_idx]\n",
        "        noisy_sample = self.noisy_data[start_idx:end_idx]\n",
        "        return torch.Tensor(noisy_sample).unsqueeze(0), torch.Tensor(clean_sample).unsqueeze(0)\n",
        "\n",
        "segment_length = 2560  # As specified in the paper\n",
        "train_size = int(0.8 * len(clean_data))\n",
        "test_size = len(clean_data) - train_size\n",
        "\n",
        "train_clean = clean_data[:train_size]\n",
        "test_clean = clean_data[train_size:]\n",
        "train_noisy = noisy_data[:train_size]\n",
        "test_noisy = noisy_data[train_size:]\n",
        "\n",
        "train_dataset = ECGDataset(train_clean, train_noisy, segment_length=segment_length)\n",
        "test_dataset = ECGDataset(test_clean, test_noisy, segment_length=segment_length)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Train loader created with {len(train_loader.dataset)} samples.\")\n",
        "print(f\"Test loader created with {len(test_loader.dataset)} samples.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "083dbc98-1a2c-4ea8-9bfd-9fb19a0c3cbb",
      "metadata": {
        "id": "083dbc98-1a2c-4ea8-9bfd-9fb19a0c3cbb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv1d(1, 512, kernel_size=4, stride=2, padding=1),  # (N, 512, 1280)\n",
        "            nn.PReLU(),\n",
        "            nn.Conv1d(512, 512, kernel_size=4, stride=2, padding=1),  # (N, 512, 640)\n",
        "            nn.PReLU(),\n",
        "            nn.Conv1d(512, 512, kernel_size=4, stride=2, padding=1),  # (N, 512, 320)\n",
        "            nn.PReLU(),\n",
        "            nn.Conv1d(512, 512, kernel_size=4, stride=2, padding=1),  # (N, 512, 160)\n",
        "            nn.PReLU(),\n",
        "            nn.Conv1d(512, 256, kernel_size=4, stride=2, padding=1),  # (N, 256, 80)\n",
        "            nn.PReLU(),\n",
        "            nn.Conv1d(256, 128, kernel_size=4, stride=2, padding=1),  # (N, 128, 40)\n",
        "            nn.PReLU(),\n",
        "            nn.Conv1d(128, 64, kernel_size=4, stride=2, padding=1),  # (N, 64, 20)\n",
        "            nn.PReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose1d(64, 128, kernel_size=4, stride=2, padding=1),  # (N, 128, 40)\n",
        "            nn.PReLU(),\n",
        "            nn.ConvTranspose1d(128, 256, kernel_size=4, stride=2, padding=1),  # (N, 256, 80)\n",
        "            nn.PReLU(),\n",
        "            nn.ConvTranspose1d(256, 512, kernel_size=4, stride=2, padding=1),  # (N, 512, 160)\n",
        "            nn.PReLU(),\n",
        "            nn.ConvTranspose1d(512, 512, kernel_size=4, stride=2, padding=1),  # (N, 512, 320)\n",
        "            nn.PReLU(),\n",
        "            nn.ConvTranspose1d(512, 512, kernel_size=4, stride=2, padding=1),  # (N, 512, 640)\n",
        "            nn.PReLU(),\n",
        "            nn.ConvTranspose1d(512, 512, kernel_size=4, stride=2, padding=1),  # (N, 512, 1280)\n",
        "            nn.PReLU(),\n",
        "            nn.ConvTranspose1d(512, 1, kernel_size=4, stride=2, padding=1)  # (N, 1, 2560)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = []\n",
        "        for layer in self.encoder:\n",
        "            x = layer(x)\n",
        "            encoded.append(x.clone())  # Save the output for skip connections\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        for i, layer in enumerate(self.decoder):\n",
        "            if isinstance(layer, nn.ConvTranspose1d):\n",
        "                x = layer(x)\n",
        "                if i < len(self.decoder) - 1:\n",
        "                    x = nn.functional.prelu(x)\n",
        "                    x += encoded[-(i + 1)]  # Skip connection\n",
        "\n",
        "        print(f\"Generator output shape: {x.shape}\")\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "b8c67e16-0f83-41ae-b089-633bdbdb7289",
      "metadata": {
        "id": "b8c67e16-0f83-41ae-b089-633bdbdb7289"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        # Encoder layers\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv1d(1, 512, kernel_size=4, stride=2, padding=1),  # (N, 512, 1280)\n",
        "            nn.PReLU(),\n",
        "            nn.Conv1d(512, 256, kernel_size=4, stride=2, padding=1),  # (N, 256, 640)\n",
        "            nn.PReLU(),\n",
        "            nn.Conv1d(256, 128, kernel_size=4, stride=2, padding=1),  # (N, 128, 320)\n",
        "            nn.PReLU(),\n",
        "            nn.Conv1d(128, 64, kernel_size=4, stride=2, padding=1),  # (N, 64, 160)\n",
        "            nn.PReLU(),\n",
        "            nn.Conv1d(64, 32, kernel_size=4, stride=2, padding=1),  # (N, 32, 80)\n",
        "            nn.PReLU(),\n",
        "            nn.Conv1d(32, 16, kernel_size=4, stride=2, padding=1),  # (N, 16, 40)\n",
        "            nn.PReLU(),\n",
        "            nn.Conv1d(16, 1, kernel_size=4, stride=2, padding=1)  # (N, 1, 20)\n",
        "        )\n",
        "\n",
        "        # Decoder layers (mirrored from encoder)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose1d(1, 16, kernel_size=4, stride=2, padding=1),  # (N, 16, 40)\n",
        "            nn.PReLU(),\n",
        "            nn.ConvTranspose1d(16, 32, kernel_size=4, stride=2, padding=1),  # (N, 32, 80)\n",
        "            nn.PReLU(),\n",
        "            nn.ConvTranspose1d(32, 64, kernel_size=4, stride=2, padding=1),  # (N, 64, 160)\n",
        "            nn.PReLU(),\n",
        "            nn.ConvTranspose1d(64, 128, kernel_size=4, stride=2, padding=1),  # (N, 128, 320)\n",
        "            nn.PReLU(),\n",
        "            nn.ConvTranspose1d(128, 256, kernel_size=4, stride=2, padding=1),  # (N, 256, 640)\n",
        "            nn.PReLU(),\n",
        "            nn.ConvTranspose1d(256, 512, kernel_size=4, stride=2, padding=1),  # (N, 512, 1280)\n",
        "            nn.PReLU(),\n",
        "            nn.ConvTranspose1d(512, 1, kernel_size=4, stride=2, padding=1)  # (N, 1, 2560)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = []\n",
        "        # Encoder pass\n",
        "        for layer in self.encoder:\n",
        "            x = layer(x)\n",
        "            encoded.append(x)  # Store output for skip connections\n",
        "\n",
        "        # Decoder pass with skip connections\n",
        "        for i, layer in enumerate(self.decoder):\n",
        "            x = layer(x)\n",
        "            if i < len(self.decoder) - 1:  # Skip the last layer\n",
        "                x = nn.PReLU()(x)  # Apply PReLU activation\n",
        "                # Perform skip connection only if shapes match\n",
        "                if encoded[-(i + 1)].shape[2] == x.shape[2]:\n",
        "                    x += encoded[-(i + 1)]  # Skip connection\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "173cebe1-6905-45c4-a6ac-4bd6e9d2ae9f",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "173cebe1-6905-45c4-a6ac-4bd6e9d2ae9f",
        "outputId": "bbe93a25-be49-48c1-ab89-0e7d347d55b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([4, 81920])\n",
            "Shape after linear layer: torch.Size([4, 1])\n",
            "Shape after conv layers: torch.Size([4, 81920])\n",
            "Shape after linear layer: torch.Size([4, 1])\n",
            "Shape after conv layers: torch.Size([4, 81920])\n",
            "Shape after linear layer: torch.Size([4, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([4, 81920])\n",
            "Shape after linear layer: torch.Size([4, 1])\n",
            "Shape after conv layers: torch.Size([4, 81920])\n",
            "Shape after linear layer: torch.Size([4, 1])\n",
            "Shape after conv layers: torch.Size([4, 81920])\n",
            "Shape after linear layer: torch.Size([4, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([4, 81920])\n",
            "Shape after linear layer: torch.Size([4, 1])\n",
            "Shape after conv layers: torch.Size([4, 81920])\n",
            "Shape after linear layer: torch.Size([4, 1])\n",
            "Shape after conv layers: torch.Size([4, 81920])\n",
            "Shape after linear layer: torch.Size([4, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([4, 81920])\n",
            "Shape after linear layer: torch.Size([4, 1])\n",
            "Shape after conv layers: torch.Size([4, 81920])\n",
            "Shape after linear layer: torch.Size([4, 1])\n",
            "Shape after conv layers: torch.Size([4, 81920])\n",
            "Shape after linear layer: torch.Size([4, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([4, 81920])\n",
            "Shape after linear layer: torch.Size([4, 1])\n",
            "Shape after conv layers: torch.Size([4, 81920])\n",
            "Shape after linear layer: torch.Size([4, 1])\n",
            "Shape after conv layers: torch.Size([4, 81920])\n",
            "Shape after linear layer: torch.Size([4, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([4, 81920])\n",
            "Shape after linear layer: torch.Size([4, 1])\n",
            "Shape after conv layers: torch.Size([4, 81920])\n",
            "Shape after linear layer: torch.Size([4, 1])\n",
            "Shape after conv layers: torch.Size([4, 81920])\n",
            "Shape after linear layer: torch.Size([4, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([4, 81920])\n",
            "Shape after linear layer: torch.Size([4, 1])\n",
            "Shape after conv layers: torch.Size([4, 81920])\n",
            "Shape after linear layer: torch.Size([4, 1])\n",
            "Shape after conv layers: torch.Size([4, 81920])\n",
            "Shape after linear layer: torch.Size([4, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([4, 81920])\n",
            "Shape after linear layer: torch.Size([4, 1])\n",
            "Shape after conv layers: torch.Size([4, 81920])\n",
            "Shape after linear layer: torch.Size([4, 1])\n",
            "Shape after conv layers: torch.Size([4, 81920])\n",
            "Shape after linear layer: torch.Size([4, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([4, 81920])\n",
            "Shape after linear layer: torch.Size([4, 1])\n",
            "Shape after conv layers: torch.Size([4, 81920])\n",
            "Shape after linear layer: torch.Size([4, 1])\n",
            "Shape after conv layers: torch.Size([4, 81920])\n",
            "Shape after linear layer: torch.Size([4, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([32, 81920])\n",
            "Shape after linear layer: torch.Size([32, 1])\n",
            "Shape after conv layers: torch.Size([4, 81920])\n",
            "Shape after linear layer: torch.Size([4, 1])\n",
            "Shape after conv layers: torch.Size([4, 81920])\n",
            "Shape after linear layer: torch.Size([4, 1])\n",
            "Shape after conv layers: torch.Size([4, 81920])\n",
            "Shape after linear layer: torch.Size([4, 1])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Example dataset and dataloader (replace with your actual dataset)\n",
        "train_dataset = TensorDataset(torch.randn(100, 1, 2560), torch.randn(100, 1, 2560))\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Initialize models and optimizers\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002)\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (noisy, clean) in enumerate(train_loader):\n",
        "        noisy, clean = noisy.to(device), clean.to(device)\n",
        "\n",
        "        ############################\n",
        "        # Train Discriminator\n",
        "        ############################\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Real batch\n",
        "        real_labels = torch.ones(clean.size(0), 1).to(device)\n",
        "        outputs_real = discriminator(torch.cat((noisy, clean), dim=1))\n",
        "        d_loss_real = criterion(outputs_real, real_labels)\n",
        "\n",
        "        # Fake batch\n",
        "        fake_clean = generator(noisy)\n",
        "        fake_inputs = torch.cat((noisy, fake_clean), dim=1)\n",
        "        fake_labels = torch.zeros(noisy.size(0), 1).to(device)\n",
        "        outputs_fake = discriminator(fake_inputs.detach())\n",
        "        d_loss_fake = criterion(outputs_fake, fake_labels)\n",
        "\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        ############################\n",
        "        # Train Generator\n",
        "        ############################\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        outputs_gen = discriminator(torch.cat((noisy, generator(noisy)), dim=1))\n",
        "        g_loss = criterion(outputs_gen, real_labels)\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        ############################\n",
        "        # Print training progress\n",
        "        ############################\n",
        "        if (i+1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], '\n",
        "                  f'D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "e4aefda1-896d-4643-8788-d52b9a3196ab",
      "metadata": {
        "id": "e4aefda1-896d-4643-8788-d52b9a3196ab"
      },
      "outputs": [],
      "source": [
        "def add_noise(signal, noise_type, snr_db):\n",
        "    \"\"\"\n",
        "    Add noise of a specific type to the signal at a given SNR level.\n",
        "\n",
        "    Args:\n",
        "        signal (numpy array): Clean ECG signal.\n",
        "        noise_type (str): Type of noise ('BW', 'EM', 'MA', 'EM+BW', 'MA+BW', 'MA+EM', 'MA+EM+BW').\n",
        "        snr_db (float): Desired signal-to-noise ratio in dB.\n",
        "\n",
        "    Returns:\n",
        "        noisy_signal (numpy array): Noisy ECG signal.\n",
        "    \"\"\"\n",
        "    noise = np.zeros_like(signal)\n",
        "    if 'BW' in noise_type:\n",
        "        noise += np.random.normal(0, 1, signal.shape)\n",
        "    if 'EM' in noise_type:\n",
        "        noise += np.random.normal(0, 1, signal.shape) * np.random.exponential(1, signal.shape)\n",
        "    if 'MA' in noise_type:\n",
        "        noise += np.convolve(np.random.normal(0, 1, signal.size), np.ones(10)/10, mode='same')\n",
        "\n",
        "    signal_power = np.mean(signal**2)\n",
        "    noise_power = np.mean(noise**2)\n",
        "    noise_variance = signal_power / (10**(snr_db / 10))\n",
        "    noise = noise * np.sqrt(noise_variance / noise_power)\n",
        "\n",
        "    noisy_signal = signal + noise\n",
        "    return noisy_signal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6dec1ee-80be-4adf-801b-d3aea9e19706",
      "metadata": {
        "scrolled": true,
        "id": "d6dec1ee-80be-4adf-801b-d3aea9e19706"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "def add_noise(signal, noise_type, snr_db):\n",
        "    noise = np.zeros_like(signal)\n",
        "    if 'BW' in noise_type:\n",
        "        noise += np.random.normal(0, 1, signal.shape)\n",
        "    if 'EM' in noise_type:\n",
        "        noise += np.random.normal(0, 1, signal.shape) * np.random.exponential(1, signal.shape)\n",
        "    if 'MA' in noise_type:\n",
        "        noise += np.convolve(np.random.normal(0, 1, signal.size), np.ones(10)/10, mode='same')\n",
        "\n",
        "    signal_power = np.mean(signal**2)\n",
        "    noise_power = np.mean(noise**2)\n",
        "    noise_variance = signal_power / (10**(snr_db / 10))\n",
        "    noise = noise * np.sqrt(noise_variance / noise_power)\n",
        "\n",
        "    noisy_signal = signal + noise\n",
        "    return noisy_signal\n",
        "\n",
        "def calculate_snr(clean, denoised):\n",
        "    noise = clean - denoised\n",
        "    signal_power = np.mean(clean**2)\n",
        "    noise_power = np.mean(noise**2)\n",
        "    snr = 10 * np.log10(signal_power / noise_power)\n",
        "    return snr\n",
        "\n",
        "def evaluate_model(generator, test_loader, noise_types, snr_levels):\n",
        "    generator.eval()\n",
        "    results = {noise_type: {snr: {\"snr\": [], \"rmse\": []} for snr in snr_levels} for noise_type in noise_types}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for noisy, clean in test_loader:\n",
        "            clean = clean.numpy()\n",
        "            for noise_type in noise_types:\n",
        "                for snr_db in snr_levels:\n",
        "                    noisy_signals = np.array([add_noise(c.flatten(), noise_type, snr_db) for c in clean])\n",
        "                    noisy_tensor = torch.Tensor(noisy_signals).unsqueeze(1).to(device)\n",
        "                    clean_tensor = torch.Tensor(clean).to(device)\n",
        "\n",
        "                    denoised = generator(noisy_tensor).cpu().numpy().squeeze()\n",
        "\n",
        "                    snr = calculate_snr(clean, denoised)\n",
        "                    rmse = np.sqrt(np.mean((clean - denoised) ** 2))\n",
        "\n",
        "                    results[noise_type][snr_db][\"snr\"].append(snr)\n",
        "                    results[noise_type][snr_db][\"rmse\"].append(rmse)\n",
        "\n",
        "    avg_results = {noise_type: {snr: {\"snr\": np.mean(results[noise_type][snr][\"snr\"]),\n",
        "                                      \"rmse\": np.mean(results[noise_type][snr][\"rmse\"])}\n",
        "                                for snr in snr_levels}\n",
        "                   for noise_type in noise_types}\n",
        "\n",
        "    table_data = []\n",
        "    for snr_db in snr_levels:\n",
        "        row = []\n",
        "        for noise_type in noise_types:\n",
        "            row.append(avg_results[noise_type][snr_db]['snr'])\n",
        "            row.append(avg_results[noise_type][snr_db]['rmse'])\n",
        "        table_data.append(row)\n",
        "\n",
        "    columns = []\n",
        "    for noise_type in noise_types:\n",
        "        columns.append(f\"{noise_type}_SNR\")\n",
        "        columns.append(f\"{noise_type}_RMSE\")\n",
        "\n",
        "    df = pd.DataFrame(table_data, columns=columns, index=[f\"{snr}dB\" for snr in snr_levels])\n",
        "\n",
        "    print(df)\n",
        "    return df\n",
        "\n",
        "# Define the noise types and SNR levels to evaluate\n",
        "noise_types = ['BW', 'EM', 'MA', 'EM+BW', 'MA+BW', 'MA+EM', 'MA+EM+BW']\n",
        "snr_levels = [0, 1, 2, 3, 4, 5]\n",
        "\n",
        "# Evaluate the model and get the results in table form\n",
        "avg_results_df = evaluate_model(generator, test_loader, noise_types, snr_levels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "51feac54-3c53-4ea7-af6a-a2fc3b70ec19",
      "metadata": {
        "id": "51feac54-3c53-4ea7-af6a-a2fc3b70ec19"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}