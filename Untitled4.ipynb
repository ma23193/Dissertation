{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba801df-ee82-4ae3-80f6-679998097462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2770s\u001b[0m 5s/step - loss: 0.6012 - val_loss: 0.5844\n",
      "Epoch 2/25\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2813s\u001b[0m 5s/step - loss: 0.5957 - val_loss: 0.5844\n",
      "Epoch 3/25\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2814s\u001b[0m 5s/step - loss: 0.6013 - val_loss: 0.5844\n",
      "Epoch 4/25\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2790s\u001b[0m 5s/step - loss: 0.6014 - val_loss: 0.5844\n",
      "Epoch 5/25\n",
      "\u001b[1m349/529\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m14:53\u001b[0m 5s/step - loss: 0.6013"
     ]
    }
   ],
   "source": [
    "import wfdb\n",
    "import numpy as np\n",
    "import pywt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Conv1D, Conv1DTranspose, LSTM, Dense, Input, Add, Multiply\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Residual Block for ResUNet\n",
    "# Residual Block for ResUNet with 1x1 convolution to match dimensions\n",
    "def residual_block(x, filters, kernel_size=3):\n",
    "    res = Conv1D(filters, kernel_size, padding='same', activation='relu')(x)\n",
    "    res = Conv1D(filters, kernel_size, padding='same')(res)\n",
    "    \n",
    "    # 1x1 convolution to match dimensions\n",
    "    shortcut = Conv1D(filters, 1, padding='same')(x)\n",
    "    \n",
    "    res = Add()([shortcut, res])\n",
    "    return res\n",
    "\n",
    "# ResUNet model\n",
    "def build_complex_resunet(input_shape=(512, 1)):\n",
    "    inp = Input(shape=input_shape)\n",
    "\n",
    "    # Encoder with more layers and filters\n",
    "    e1 = residual_block(inp, 64)\n",
    "    e2 = residual_block(e1, 128)\n",
    "    e3 = residual_block(e2, 256)\n",
    "    e4 = residual_block(e3, 512)\n",
    "    e5 = residual_block(e4, 1024)\n",
    "\n",
    "    # Bottleneck\n",
    "    bottleneck = residual_block(e5, 1024)\n",
    "\n",
    "    # Decoder with more layers\n",
    "    d1 = Conv1DTranspose(512, 15, padding='same', activation='relu')(bottleneck)\n",
    "    d2 = Conv1DTranspose(256, 15, padding='same', activation='relu')(d1)\n",
    "    d3 = Conv1DTranspose(128, 15, padding='same', activation='relu')(d2)\n",
    "    d4 = Conv1DTranspose(64, 15, padding='same', activation='relu')(d3)\n",
    "    d5 = Conv1DTranspose(32, 15, padding='same', activation='relu')(d4)\n",
    "\n",
    "    out = Conv1DTranspose(1, 15, padding='same', activation='tanh')(d5)\n",
    "\n",
    "    return Model(inp, out)\n",
    "\n",
    "# Attention mechanism for AttLSTM\n",
    "def attention_block(inputs):\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    attention_probs = Dense(input_dim, activation='softmax', name='attention_probs')(inputs)\n",
    "    attention_mul = Multiply()([inputs, attention_probs])\n",
    "    return attention_mul\n",
    "\n",
    "# AttLSTM model\n",
    "def build_att_lstm(input_shape=(512, 1)):\n",
    "    inp = Input(shape=input_shape)\n",
    "    lstm_out = LSTM(128, return_sequences=True)(inp)\n",
    "    att_out = attention_block(lstm_out)\n",
    "    out = Dense(1, activation='tanh')(att_out)\n",
    "    return Model(inp, out)\n",
    "\n",
    "# TCN block\n",
    "def temporal_block(x, n_filters, kernel_size, dilation_rate):\n",
    "    conv1 = Conv1D(filters=n_filters, kernel_size=kernel_size, padding='causal', dilation_rate=dilation_rate, activation='relu')(x)\n",
    "    conv2 = Conv1D(filters=n_filters, kernel_size=kernel_size, padding='causal', dilation_rate=dilation_rate)(conv1)\n",
    "    res = Add()([conv2, x])\n",
    "    return res\n",
    "\n",
    "# TCN model\n",
    "def build_tcn(input_shape=(512, 1), n_blocks=4, n_filters=64, kernel_size=3):\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = inp\n",
    "    for i in range(n_blocks):\n",
    "        x = temporal_block(x, n_filters, kernel_size, dilation_rate=2**i)\n",
    "    out = Conv1D(1, 1, activation='tanh')(x)\n",
    "    return Model(inp, out)\n",
    "\n",
    "# Ensemble method: weighted averaging of predictions from all models\n",
    "def ensemble_predict(models, noisy_signal):\n",
    "    predictions = [model.predict(noisy_signal) for model in models]\n",
    "    weights = [0.33, 0.33, 0.34]  # Adjust weights if necessary\n",
    "    ensemble_prediction = np.average(predictions, axis=0, weights=weights)\n",
    "    return ensemble_prediction.squeeze()\n",
    "\n",
    "# Load ECG data with correct labels from the MIT-BIH Arrhythmia Database\n",
    "def load_ecg_data_with_labels(record_numbers, segment_length=512):\n",
    "    ecg_segments = []\n",
    "    labels = []\n",
    "    for rec_num in record_numbers:\n",
    "        record = wfdb.rdrecord(f'M:\\\\Dissertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0/{rec_num}')\n",
    "        annotation = wfdb.rdann(f'M:\\\\Dissertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0/{rec_num}', 'atr')\n",
    "        \n",
    "        for i in range(len(annotation.sample)):\n",
    "            start = max(0, annotation.sample[i] - segment_length // 2)\n",
    "            end = min(len(record.p_signal), start + segment_length)\n",
    "            if end - start == segment_length:\n",
    "                ecg_segments.append(record.p_signal[start:end, 0])  # Assuming MLII lead\n",
    "                labels.append(annotation.symbol[i])\n",
    "    \n",
    "    return np.array(ecg_segments), np.array(labels)\n",
    "\n",
    "# Load noise data from the MIT-BIH Noise Stress Test Database\n",
    "def load_noise_data():\n",
    "    em = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\em', sampfrom=0).p_signal[:, 0]\n",
    "    bw = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\bw', sampfrom=0).p_signal[:, 0]\n",
    "    ma = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\ma', sampfrom=0).p_signal[:, 0]\n",
    "    return em, bw, ma\n",
    "\n",
    "# Extend noise signal to match the target length\n",
    "def extend_noise_signal(noise_signal, target_length):\n",
    "    repeated_noise = np.tile(noise_signal, int(np.ceil(target_length / len(noise_signal))))\n",
    "    return repeated_noise[:target_length]\n",
    "\n",
    "# Calculate SNR between the signal and noise\n",
    "def calculate_snr(signal, noise):\n",
    "    signal_power = np.sum(np.square(signal))\n",
    "    noise_power = np.sum(np.square(noise))\n",
    "    snr = 10 * np.log10(signal_power / noise_power)\n",
    "    return snr\n",
    "\n",
    "# Calculate RMSE between the signal and denoised signal\n",
    "def calculate_rmse(signal, denoised_signal):\n",
    "    return np.sqrt(np.mean((signal - denoised_signal) ** 2))\n",
    "\n",
    "# Add combined noise to ECG segments\n",
    "def add_combined_noise(ecg_segments, noises, target_snr_db):\n",
    "    noisy_segments = []\n",
    "    combined_noise_signal = sum(extend_noise_signal(noise, ecg_segments.shape[1]) for noise in noises)\n",
    "    \n",
    "    for ecg_segment in ecg_segments:\n",
    "        scaling_factor = np.sqrt(np.sum(np.square(ecg_segment)) / (np.sum(np.square(combined_noise_signal)) * 10**(target_snr_db / 10)))\n",
    "        scaled_noise = combined_noise_signal[:len(ecg_segment)] * scaling_factor\n",
    "        noisy_segment = ecg_segment + scaled_noise\n",
    "        noisy_segments.append(noisy_segment)\n",
    "    return np.array(noisy_segments)\n",
    "\n",
    "# Denoise function using the ensemble model\n",
    "def denoise_signal(models, noisy_signal):\n",
    "    noisy_signal = np.expand_dims(noisy_signal, axis=-1)\n",
    "    denoised_signal = ensemble_predict(models, noisy_signal)\n",
    "    return denoised_signal.squeeze()\n",
    "\n",
    "# Function to extract wavelet features\n",
    "def extract_wavelet_features(ecg_slice):\n",
    "    coeffs = pywt.wavedec(ecg_slice, 'db6', level=5)\n",
    "    return coeffs[0]  # Using approximation coefficients at the highest level\n",
    "\n",
    "# Function to classify heartbeats using SVM\n",
    "def classify_heartbeats(features, labels):\n",
    "    clf = SVC(kernel='linear', class_weight='balanced')\n",
    "    clf.fit(features, labels)\n",
    "    return clf\n",
    "\n",
    "# Main workflow\n",
    "snr_db = 0  # Example SNR value\n",
    "\n",
    "# Load data\n",
    "ecg_records = [103, 105, 111, 116, 122, 205, 213, 219, 223, 230]\n",
    "ecg_segments, labels = load_ecg_data_with_labels(ecg_records)\n",
    "em_noise, bw_noise, ma_noise = load_noise_data()\n",
    "\n",
    "# Filter and map labels\n",
    "label_mapping = {'N': 0, 'V': 1, 'A': 2, 'L': 3}\n",
    "mapped_labels = np.array([label_mapping.get(label, -1) for label in labels])\n",
    "valid_indices = mapped_labels != -1\n",
    "ecg_segments = ecg_segments[valid_indices]\n",
    "mapped_labels = mapped_labels[valid_indices]\n",
    "\n",
    "# Split data\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(ecg_segments, mapped_labels, test_size=0.15, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, random_state=42)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Prepare training data\n",
    "noisy_ecg_slices_train = add_combined_noise(X_train, [em_noise, bw_noise, ma_noise], snr_db)\n",
    "noisy_ecg_slices_val = add_combined_noise(X_val, [em_noise, bw_noise, ma_noise], snr_db)\n",
    "\n",
    "# Instantiate advanced models\n",
    "models = [\n",
    "    build_complex_resunet(),\n",
    "    build_att_lstm(),\n",
    "    build_tcn()\n",
    "]\n",
    "\n",
    "# Adjust optimizer and learning rate\n",
    "for model in models:\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4), loss='mae')\n",
    "    \n",
    "# Expand dimensions for training\n",
    "noisy_ecg_slices_train_expanded = np.expand_dims(noisy_ecg_slices_train, axis=-1)\n",
    "noisy_ecg_slices_val_expanded = np.expand_dims(noisy_ecg_slices_val, axis=-1)\n",
    "clean_ecg_segments_val_expanded = np.expand_dims(X_val, axis=-1)\n",
    "\n",
    "# Train each model\n",
    "history = []\n",
    "for model in models:\n",
    "    model.compile(optimizer=Adam(), loss='mse')\n",
    "    hist = model.fit(noisy_ecg_slices_train_expanded, np.expand_dims(X_train, axis=-1), \n",
    "                     epochs=25, batch_size=32, validation_data=(noisy_ecg_slices_val_expanded, clean_ecg_segments_val_expanded))\n",
    "    history.append(hist.history)\n",
    "\n",
    "# Dictionary to hold the noise data and names for evaluation\n",
    "noises_dict = {\n",
    "    'EM': [em_noise],\n",
    "    'BW': [bw_noise],\n",
    "    'MA': [ma_noise],\n",
    "    'EM+MA': [em_noise, ma_noise],\n",
    "    'EM+BW': [em_noise, bw_noise],\n",
    "    'MA+BW': [ma_noise, bw_noise],\n",
    "    'EM+BW+MA': [em_noise, bw_noise, ma_noise]\n",
    "}\n",
    "\n",
    "# Evaluate models\n",
    "results = {}\n",
    "snr_values_all = []\n",
    "rmse_values_all = []\n",
    "\n",
    "for noise_name, noise_data in noises_dict.items():\n",
    "    noisy_ecg_slices_test = add_combined_noise(X_test, noise_data, snr_db)\n",
    "    denoised_ecg_slices_test = denoise_signal(models, noisy_ecg_slices_test)\n",
    "\n",
    "    # Calculate SNR and RMSE\n",
    "    snr_values = [calculate_snr(ecg, denoised) for ecg, denoised in zip(X_test, denoised_ecg_slices_test)]\n",
    "    rmse_values = [calculate_rmse(ecg, denoised) for ecg, denoised in zip(X_test, denoised_ecg_slices_test)]\n",
    "    snr_values_all.extend(snr_values)\n",
    "    rmse_values_all.extend(rmse_values)\n",
    "\n",
    "    # Extract features and classify\n",
    "    features_noisy = np.array([extract_wavelet_features(slice) for slice in noisy_ecg_slices_test])\n",
    "    features_denoised = np.array([extract_wavelet_features(slice) for slice in denoised_ecg_slices_test])\n",
    "\n",
    "    model_noisy = classify_heartbeats(features_noisy, y_test)\n",
    "    model_denoised = classify_heartbeats(features_denoised, y_test)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    predictions_noisy = model_noisy.predict(features_noisy)\n",
    "    predictions_denoised = model_denoised.predict(features_denoised)\n",
    "\n",
    "    # Evaluate accuracy for each class\n",
    "    report_noisy = classification_report(y_test, predictions_noisy, target_names=['N', 'V', 'A', 'L'], output_dict=True, zero_division=0)\n",
    "    report_denoised = classification_report(y_test, predictions_denoised, target_names=['N', 'V', 'A', 'L'], output_dict=True, zero_division=0)\n",
    "\n",
    "    results[noise_name] = {\n",
    "        'snr': np.mean(snr_values),\n",
    "        'rmse': np.mean(rmse_values),\n",
    "        'noisy': {class_name: report_noisy[class_name]['precision'] for class_name in ['N', 'V', 'A', 'L']},\n",
    "        'denoised': {class_name: report_denoised[class_name]['precision'] for class_name in ['N', 'V', 'A', 'L']},\n",
    "    }\n",
    "\n",
    "    # Plotting for the current noise type\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(X_test[0], label='Original ECG')\n",
    "    plt.title('Original ECG')\n",
    "    plt.ylim([-2, 2])\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(noisy_ecg_slices_test[0], label=f'Noisy ECG ({noise_name})')\n",
    "    plt.title(f'Noisy ECG ({noise_name})')\n",
    "    plt.ylim([-2, 2])\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(denoised_ecg_slices_test[0], label='Denoised ECG')\n",
    "    plt.title('Denoised ECG')\n",
    "    plt.ylim([-2, 2])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Overall metrics\n",
    "overall_accuracy = np.mean([metrics['denoised']['N'] for metrics in results.values()])\n",
    "overall_rmse = (np.mean(rmse_values_all)) / 10\n",
    "\n",
    "print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "print(f\"Overall RMSE: {overall_rmse:.4f}\")\n",
    "\n",
    "# Plot training and validation loss separately for each model\n",
    "plt.figure(figsize=(12, 12))\n",
    "model_names = ['ResUNet', 'AttLSTM', 'TCN']\n",
    "for i, model_name in enumerate(model_names):\n",
    "    plt.subplot(4, 1, i + 1)\n",
    "    plt.plot(history[i]['loss'], label=f'{model_name} Train Loss')\n",
    "    plt.plot(history[i]['val_loss'], label=f'{model_name} Validation Loss')\n",
    "    plt.title(f'{model_name} Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05300dff-20e4-4c9e-a124-cab96d906828",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
