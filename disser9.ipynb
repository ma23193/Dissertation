{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPslkNRviqf+FTJTTmmOV8h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ma23193/Dissertation/blob/main/disser9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uw1qZl2sszQ-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(noisy_path, clean_path):\n",
        "    noisy_signals = np.load(noisy_path)\n",
        "    clean_signals = np.load(clean_path)\n",
        "\n",
        "    # Ensure the data is in the right shape\n",
        "    noisy_signals = noisy_signals[..., np.newaxis]\n",
        "    clean_signals = clean_signals[..., np.newaxis]\n",
        "\n",
        "    return noisy_signals, clean_signals\n",
        "\n",
        "noisy_path = 'path/to/your/noisy_signals.npy'\n",
        "clean_path = 'path/to/your/clean_signals.npy'\n",
        "\n",
        "noisy_signals, clean_signals = load_data(noisy_path, clean_path)\n"
      ],
      "metadata": {
        "id": "5uQS3YuUs2jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(noisy_signals, clean_signals, batch_size=32):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((noisy_signals, clean_signals))\n",
        "    dataset = dataset.shuffle(buffer_size=1024).batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "batch_size = 32\n",
        "dataset = create_dataset(noisy_signals, clean_signals, batch_size)\n"
      ],
      "metadata": {
        "id": "IMJ-jE2-s48f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_generator(input_shape):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    e1 = layers.Conv1D(512, 1, padding='same', activation='linear')(inputs)\n",
        "    e1 = layers.PReLU()(e1)\n",
        "    e2 = layers.Conv1D(512, 16, padding='same', activation='linear')(e1)\n",
        "    e2 = layers.PReLU()(e2)\n",
        "    e3 = layers.Conv1D(512, 32, padding='same', activation='linear')(e2)\n",
        "    e3 = layers.PReLU()(e3)\n",
        "    e4 = layers.Conv1D(512, 64, padding='same', activation='linear')(e3)\n",
        "    e4 = layers.PReLU()(e4)\n",
        "    e5 = layers.Conv1D(256, 128, padding='same', activation='linear')(e4)\n",
        "    e5 = layers.PReLU()(e5)\n",
        "    e6 = layers.Conv1D(128, 256, padding='same', activation='linear')(e5)\n",
        "    e6 = layers.PReLU()(e6)\n",
        "    e7 = layers.Conv1D(64, 512, padding='same', activation='linear')(e6)\n",
        "    e7 = layers.PReLU()(e7)\n",
        "    e8 = layers.Conv1D(32, 1024, padding='same', activation='linear')(e7)\n",
        "    e8 = layers.PReLU()(e8)\n",
        "\n",
        "    # Decoder with skip connections\n",
        "    d7 = layers.Conv1DTranspose(64, 512, padding='same', activation='linear')(e8)\n",
        "    d7 = layers.PReLU()(d7)\n",
        "    d7 = layers.Concatenate()([d7, e7])\n",
        "    d6 = layers.Conv1DTranspose(128, 256, padding='same', activation='linear')(d7)\n",
        "    d6 = layers.PReLU()(d6)\n",
        "    d6 = layers.Concatenate()([d6, e6])\n",
        "    d5 = layers.Conv1DTranspose(256, 128, padding='same', activation='linear')(d6)\n",
        "    d5 = layers.PReLU()(d5)\n",
        "    d5 = layers.Concatenate()([d5, e5])\n",
        "    d4 = layers.Conv1DTranspose(512, 64, padding='same', activation='linear')(d5)\n",
        "    d4 = layers.PReLU()(d4)\n",
        "    d4 = layers.Concatenate()([d4, e4])\n",
        "    d3 = layers.Conv1DTranspose(512, 32, padding='same', activation='linear')(d4)\n",
        "    d3 = layers.PReLU()(d3)\n",
        "    d3 = layers.Concatenate()([d3, e3])\n",
        "    d2 = layers.Conv1DTranspose(512, 16, padding='same', activation='linear')(d3)\n",
        "    d2 = layers.PReLU()(d2)\n",
        "    d2 = layers.Concatenate()([d2, e2])\n",
        "    d1 = layers.Conv1DTranspose(512, 1, padding='same', activation='linear')(d2)\n",
        "    d1 = layers.PReLU()(d1)\n",
        "    d1 = layers.Concatenate()([d1, e1])\n",
        "\n",
        "    outputs = layers.Conv1DTranspose(1, 1, padding='same', activation='linear')(d1)\n",
        "\n",
        "    model = models.Model(inputs, outputs, name='generator')\n",
        "    return model\n",
        "\n",
        "generator = build_generator((None, 1))\n",
        "generator.summary()\n"
      ],
      "metadata": {
        "id": "meks8Kabs6jP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_discriminator(input_shape):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    d = layers.Conv1D(64, 5, strides=2, padding='same')(inputs)\n",
        "    d = layers.BatchNormalization()(d)\n",
        "    d = layers.LeakyReLU(alpha=0.2)(d)\n",
        "    d = layers.Conv1D(128, 5, strides=2, padding='same')(d)\n",
        "    d = layers.BatchNormalization()(d)\n",
        "    d = layers.LeakyReLU(alpha=0.2)(d)\n",
        "    d = layers.Conv1D(256, 5, strides=2, padding='same')(d)\n",
        "    d = layers.BatchNormalization()(d)\n",
        "    d = layers.LeakyReLU(alpha=0.2)(d)\n",
        "    d = layers.Conv1D(512, 5, strides=2, padding='same')(d)\n",
        "    d = layers.BatchNormalization()(d)\n",
        "    d = layers.LeakyReLU(alpha=0.2)(d)\n",
        "\n",
        "    d = layers.Flatten()(d)\n",
        "    d = layers.Dense(1, activation='sigmoid')(d)\n",
        "\n",
        "    model = models.Model(inputs, d, name='discriminator')\n",
        "    return model\n",
        "\n",
        "discriminator = build_discriminator((None, 2))\n",
        "discriminator.summary()\n"
      ],
      "metadata": {
        "id": "yhGVE-1Gs80o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generator_loss(fake_output, real_signal, generated_signal):\n",
        "    ldist = tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.square(generated_signal - real_signal), axis=1)))\n",
        "    lmax = tf.reduce_max(tf.abs(generated_signal - real_signal))\n",
        "    lsgan_loss = tf.reduce_mean(tf.square(fake_output - 1))\n",
        "    return lsgan_loss + 0.7 * ldist + 0.2 * lmax\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = tf.reduce_mean(tf.square(real_output - 1))\n",
        "    fake_loss = tf.reduce_mean(tf.square(fake_output))\n",
        "    return real_loss + fake_loss\n"
      ],
      "metadata": {
        "id": "kPF0NmDcs-al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "@tf.function\n",
        "def train_step(noisy_signals, real_signals):\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_signals = generator(noisy_signals, training=True)\n",
        "\n",
        "        real_output = discriminator(tf.concat([real_signals, noisy_signals], axis=2), training=True)\n",
        "        fake_output = discriminator(tf.concat([generated_signals, noisy_signals], axis=2), training=True)\n",
        "\n",
        "        gen_loss = generator_loss(fake_output, real_signals, generated_signals)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "    return gen_loss, disc_loss\n",
        "\n",
        "def train(dataset, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        for noisy_signals, real_signals in dataset:\n",
        "            gen_loss, disc_loss = train_step(noisy_signals, real_signals)\n",
        "        print(f'Epoch {epoch+1}, Gen Loss: {gen_loss.numpy()}, Disc Loss: {disc_loss.numpy()}')\n"
      ],
      "metadata": {
        "id": "0NN2cK4js_6_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}