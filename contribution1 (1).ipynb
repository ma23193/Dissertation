{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7374ed44-ef49-4cd0-848d-04dc9f2a5919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 463ms/step - loss: 0.0956 - val_loss: 0.0457\n",
      "Epoch 2/10\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 464ms/step - loss: 0.0490 - val_loss: 0.0411\n",
      "Epoch 3/10\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 461ms/step - loss: 0.0450 - val_loss: 0.0334\n",
      "Epoch 4/10\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 464ms/step - loss: 0.0424 - val_loss: 0.0313\n",
      "Epoch 5/10\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 462ms/step - loss: 0.0399 - val_loss: 0.0307\n",
      "Epoch 6/10\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 455ms/step - loss: 0.0416 - val_loss: 0.0304\n",
      "Epoch 7/10\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 464ms/step - loss: 0.0380 - val_loss: 0.0300\n",
      "Epoch 8/10\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 463ms/step - loss: 0.0349 - val_loss: 0.0288\n",
      "Epoch 10/10\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 459ms/step - loss: 0.0384 - val_loss: 0.0282\n",
      "Epoch 1/10\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m415s\u001b[0m 730ms/step - loss: 0.1048 - val_loss: 0.0502\n",
      "Epoch 2/10\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m415s\u001b[0m 751ms/step - loss: 0.0559 - val_loss: 0.0747\n",
      "Epoch 3/10\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m418s\u001b[0m 757ms/step - loss: 0.0491 - val_loss: 0.0342\n",
      "Epoch 4/10\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m410s\u001b[0m 741ms/step - loss: 0.0397 - val_loss: 0.0338\n",
      "Epoch 5/10\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m423s\u001b[0m 765ms/step - loss: 0.0397 - val_loss: 0.0309\n",
      "Epoch 6/10\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 773ms/step - loss: 0.0356 - val_loss: 0.0292\n",
      "Epoch 7/10\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m395s\u001b[0m 715ms/step - loss: 0.0416 - val_loss: 0.0287\n",
      "Epoch 8/10\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 598ms/step - loss: 0.0339 - val_loss: 0.0275\n",
      "Epoch 9/10\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 595ms/step - loss: 0.0364 - val_loss: 0.0328\n",
      "Epoch 10/10\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m338s\u001b[0m 612ms/step - loss: 0.0367 - val_loss: 0.0336\n",
      "Epoch 1/10\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 728ms/step - loss: 0.1066 - val_loss: 0.0528\n",
      "Epoch 2/10\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m440s\u001b[0m 797ms/step - loss: 0.0543 - val_loss: 0.0348\n",
      "Epoch 3/10\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m468s\u001b[0m 847ms/step - loss: 0.0425 - val_loss: 0.0371\n",
      "Epoch 4/10\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 681ms/step - loss: 0.0414 - val_loss: 0.0306\n",
      "Epoch 5/10\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 690ms/step - loss: 0.0362 - val_loss: 0.0293\n",
      "Epoch 6/10\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m375s\u001b[0m 679ms/step - loss: 0.0369 - val_loss: 0.0280\n",
      "Epoch 7/10\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m469s\u001b[0m 850ms/step - loss: 0.0366 - val_loss: 0.0314\n",
      "Epoch 8/10\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m463s\u001b[0m 839ms/step - loss: 0.0336 - val_loss: 0.0298\n",
      "Epoch 9/10\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m374s\u001b[0m 677ms/step - loss: 0.0310 - val_loss: 0.0280\n",
      "Epoch 10/10\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 680ms/step - loss: 0.0331 - val_loss: 0.0264\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 512 and 2048 for '{{node compile_loss/mse/sub}} = Sub[T=DT_FLOAT](data_1, functional_7_1/conv1d_7_1/Tanh)' with input shapes: [?,512,1], [?,2048,1].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 166\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[0;32m    165\u001b[0m     model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 166\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_ecg_slices_train_expanded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_ecg_segments_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m              \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnoisy_ecg_slices_val_expanded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_ecg_segments_val_expanded\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# Evaluate the trained models on each noise type and combination\u001b[39;00m\n\u001b[0;32m    170\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\keras\\src\\losses\\losses.py:1158\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m   1156\u001b[0m y_true \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(y_true, dtype\u001b[38;5;241m=\u001b[39my_pred\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m   1157\u001b[0m y_true, y_pred \u001b[38;5;241m=\u001b[39m squeeze_or_expand_to_same_rank(y_true, y_pred)\n\u001b[1;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mmean(ops\u001b[38;5;241m.\u001b[39msquare(\u001b[43my_true\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Dimensions must be equal, but are 512 and 2048 for '{{node compile_loss/mse/sub}} = Sub[T=DT_FLOAT](data_1, functional_7_1/conv1d_7_1/Tanh)' with input shapes: [?,512,1], [?,2048,1]."
     ]
    }
   ],
   "source": [
    "import wfdb\n",
    "import numpy as np\n",
    "import pywt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, GRU, Dense, Input, Add, Flatten, Reshape, UpSampling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the new models\n",
    "def build_1d_cnn(input_shape=(512, 1)):\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = Conv1D(64, 15, padding='same', activation='relu')(inp)\n",
    "    x = Conv1D(128, 15, padding='same', activation='relu')(x)\n",
    "    x = Conv1D(256, 15, padding='same', activation='relu')(x)\n",
    "    x = Conv1D(1, 15, padding='same', activation='tanh')(x)\n",
    "    return Model(inp, x)\n",
    "\n",
    "def build_lstm(input_shape=(512, 1)):\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = LSTM(128, return_sequences=True)(inp)\n",
    "    x = LSTM(64, return_sequences=True)(x)\n",
    "    x = Dense(1, activation='tanh')(x)\n",
    "    return Model(inp, x)\n",
    "\n",
    "def build_gru(input_shape=(512, 1)):\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = GRU(128, return_sequences=True)(inp)\n",
    "    x = GRU(64, return_sequences=True)(x)\n",
    "    x = Dense(1, activation='tanh')(x)\n",
    "    return Model(inp, x)\n",
    "\n",
    "def build_denoising_autoencoder(input_shape=(512, 1)):\n",
    "    inp = Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    encoded = Conv1D(128, 15, padding='same', activation='relu')(inp)\n",
    "    encoded = Conv1D(64, 15, padding='same', activation='relu')(encoded)\n",
    "    \n",
    "    # Decoder with upsampling\n",
    "    decoded = UpSampling1D(size=2)(encoded)\n",
    "    decoded = Conv1D(64, 15, padding='same', activation='relu')(decoded)\n",
    "    decoded = UpSampling1D(size=2)(decoded)\n",
    "    decoded = Conv1D(1, 15, padding='same', activation='tanh')(decoded)\n",
    "    \n",
    "    return Model(inp, decoded)\n",
    "\n",
    "# Ensemble method: averaging predictions from all models\n",
    "def ensemble_predict(models, noisy_signal):\n",
    "    predictions = [model.predict(noisy_signal) for model in models]\n",
    "    ensemble_prediction = np.mean(predictions, axis=0)\n",
    "    return ensemble_prediction.squeeze()\n",
    "\n",
    "# Load ECG and noise data\n",
    "def load_ecg_data_with_labels(record_numbers, segment_length=512):\n",
    "    ecg_segments = []\n",
    "    labels = []\n",
    "    for rec_num in record_numbers:\n",
    "        record = wfdb.rdrecord(f'M:\\\\Dissertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0/{rec_num}')\n",
    "        annotation = wfdb.rdann(f'M:\\\\Dissertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0/{rec_num}', 'atr')\n",
    "        \n",
    "        for i in range(len(annotation.sample)):\n",
    "            start = max(0, annotation.sample[i] - segment_length // 2)\n",
    "            end = min(len(record.p_signal), start + segment_length)\n",
    "            if end - start == segment_length:\n",
    "                ecg_segments.append(record.p_signal[start:end, 0])  # Assuming MLII lead\n",
    "                labels.append(annotation.symbol[i])\n",
    "    \n",
    "    return np.array(ecg_segments), np.array(labels)\n",
    "\n",
    "def load_noise_data():\n",
    "    em = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\em', sampfrom=0).p_signal[:, 0]\n",
    "    bw = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\bw', sampfrom=0).p_signal[:, 0]\n",
    "    ma = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\ma', sampfrom=0).p_signal[:, 0]\n",
    "    return em, bw, ma\n",
    "\n",
    "# Load ECG and noise data\n",
    "ecg_records = [103, 105, 111, 116, 122, 205, 213, 219, 223, 230]  # Add more records as needed\n",
    "ecg_segments, labels = load_ecg_data_with_labels(ecg_records)\n",
    "em_noise, bw_noise, ma_noise = load_noise_data()\n",
    "\n",
    "# Split data into 70% training, 15% validation, and 15% testing\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(ecg_segments, labels, test_size=0.15, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, random_state=42)  # 0.1765 * 85% ≈ 15%\n",
    "\n",
    "# Compute class weights to handle imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Extend and add noise to ECG segments\n",
    "def extend_noise_signal(noise_signal, target_length):\n",
    "    repeated_noise = np.tile(noise_signal, int(np.ceil(target_length / len(noise_signal))))\n",
    "    return repeated_noise[:target_length]\n",
    "\n",
    "def calculate_snr(signal, noise):\n",
    "    signal_power = np.sum(np.square(signal))\n",
    "    noise_power = np.sum(np.square(noise))\n",
    "    snr = 10 * np.log10(signal_power / noise_power)\n",
    "    return snr\n",
    "\n",
    "def calculate_rmse(signal, denoised_signal):\n",
    "    return np.sqrt(np.mean((signal - denoised_signal) ** 2))\n",
    "\n",
    "# Add noise to ECG segments using multiple noise types and combinations\n",
    "def add_combined_noise(ecg_segments, noises, target_snr_db):\n",
    "    noisy_segments = []\n",
    "    \n",
    "    # Generate noise signal by combining all noise types\n",
    "    combined_noise_signal = sum(extend_noise_signal(noise, ecg_segments.shape[1]) for noise in noises)\n",
    "    \n",
    "    for ecg_segment in ecg_segments:\n",
    "        current_snr = calculate_snr(ecg_segment, combined_noise_signal[:len(ecg_segment)])\n",
    "        scaling_factor = np.sqrt(np.sum(np.square(ecg_segment)) / (np.sum(np.square(combined_noise_signal)) * 10**(target_snr_db / 10)))\n",
    "        scaled_noise = combined_noise_signal[:len(ecg_segment)] * scaling_factor\n",
    "        noisy_segment = ecg_segment + scaled_noise\n",
    "        noisy_segments.append(noisy_segment)\n",
    "    return np.array(noisy_segments)\n",
    "\n",
    "# Denoise function\n",
    "def denoise_signal(models, noisy_signal):\n",
    "    noisy_signal = np.expand_dims(noisy_signal, axis=-1)\n",
    "    denoised_signal = ensemble_predict(models, noisy_signal)\n",
    "    return denoised_signal.squeeze()\n",
    "\n",
    "# Function to extract wavelet features\n",
    "def extract_wavelet_features(ecg_slice):\n",
    "    coeffs = pywt.wavedec(ecg_slice, 'db6', level=5)\n",
    "    return coeffs[0]  # You may want to use more features from different levels\n",
    "\n",
    "# Function to classify heartbeats using SVM\n",
    "def classify_heartbeats(features, labels):\n",
    "    clf = SVC(kernel='linear', class_weight=class_weight_dict)\n",
    "    clf.fit(features, labels)\n",
    "    return clf\n",
    "\n",
    "# Add single noises to combined noises\n",
    "noises = [em_noise, bw_noise, ma_noise]\n",
    "\n",
    "snr_db = 0  # Example SNR value\n",
    "\n",
    "# Prepare the clean ECG segments as target data for training\n",
    "clean_ecg_segments_train = X_train.copy()\n",
    "\n",
    "# Create noisy training data by combining all noise types\n",
    "noisy_ecg_slices_train = add_combined_noise(X_train, noises, snr_db)\n",
    "noisy_ecg_slices_val = add_combined_noise(X_val, noises, snr_db)\n",
    "\n",
    "# Instantiate the models\n",
    "models = [\n",
    "    build_1d_cnn(),\n",
    "    build_lstm(),\n",
    "    build_gru(),\n",
    "    build_denoising_autoencoder()\n",
    "]\n",
    "\n",
    "# Expand dimensions for training and validation\n",
    "noisy_ecg_slices_train_expanded = np.expand_dims(noisy_ecg_slices_train, axis=-1)  # Expand dims for the models\n",
    "noisy_ecg_slices_val_expanded = np.expand_dims(noisy_ecg_slices_val, axis=-1)  # Expand dims for validation\n",
    "clean_ecg_segments_val_expanded = np.expand_dims(X_val, axis=-1)\n",
    "\n",
    "# Train each model\n",
    "for model in models:\n",
    "    model.compile(optimizer=Adam(), loss='mse')\n",
    "    model.fit(noisy_ecg_slices_train_expanded, np.expand_dims(clean_ecg_segments_train, axis=-1), \n",
    "              epochs=10, batch_size=32, validation_data=(noisy_ecg_slices_val_expanded, clean_ecg_segments_val_expanded))\n",
    "\n",
    "# Evaluate the trained models on each noise type and combination\n",
    "results = {}\n",
    "for noise_name, noise_data in noises.items():\n",
    "    noisy_ecg_slices_test = add_combined_noise(X_test, noises, snr_db)\n",
    "    \n",
    "    # Denoise and classify\n",
    "    denoised_ecg_slices_test = denoise_signal(models, noisy_ecg_slices_test)\n",
    "    \n",
    "    # Calculate SNR and RMSE\n",
    "    snr_values = [calculate_snr(ecg, denoised) for ecg, denoised in zip(X_test, denoised_ecg_slices_test)]\n",
    "    rmse_values = [calculate_rmse(ecg, denoised) for ecg, denoised in zip(X_test, denoised_ecg_slices_test)]\n",
    "    \n",
    "    features_noisy = np.array([extract_wavelet_features(slice) for slice in noisy_ecg_slices_test])\n",
    "    features_denoised = np.array([extract_wavelet_features(slice) for slice in denoised_ecg_slices_test])\n",
    "    \n",
    "    test_labels = y_test[:len(features_noisy)]\n",
    "    \n",
    "    model_noisy = classify_heartbeats(features_noisy, test_labels)\n",
    "    model_denoised = classify_heartbeats(features_denoised, test_labels)\n",
    "    \n",
    "    # Predict the classes\n",
    "    predictions_noisy = model_noisy.predict(features_noisy)\n",
    "    predictions_denoised = model_denoised.predict(features_denoised)\n",
    "    \n",
    "    # Get the unique classes present in test_labels\n",
    "    unique_classes = np.unique(test_labels)\n",
    "    class_names = [name for i, name in enumerate(['N', 'V', 'A', 'L']) if i in unique_classes]\n",
    "    \n",
    "    # Evaluate accuracy for each class\n",
    "    report_noisy = classification_report(test_labels, predictions_noisy, target_names=class_names, output_dict=True, zero_division=0)\n",
    "    report_denoised = classification_report(test_labels, predictions_denoised, target_names=class_names, output_dict=True, zero_division=0)\n",
    "    \n",
    "    # Store accuracy and SNR, RMSE for each class\n",
    "    results[noise_name] = {\n",
    "        'noisy': {class_name: report_noisy[class_name]['precision'] for class_name in class_names},\n",
    "        'denoised': {class_name: report_denoised[class_name]['precision'] for class_name in class_names},\n",
    "        'snr': np.mean(snr_values),\n",
    "        'rmse': np.mean(rmse_values)\n",
    "    }\n",
    "\n",
    "# Output the results for each class and noise condition\n",
    "for noise_name, metrics in results.items():\n",
    "    print(f\"Noise type: {noise_name}\")\n",
    "    print(f\"Average SNR after denoising: {metrics['snr']:.4f} dB\")\n",
    "    print(f\"Average RMSE after denoising: {metrics['rmse']:.4f}\")\n",
    "    print(\"Noisy data accuracies:\")\n",
    "    for class_label, accuracy in metrics['noisy'].items():\n",
    "        print(f\"  {class_label}: {accuracy:.4f}\")\n",
    "    print(\"Denoised data accuracies:\")\n",
    "    for class_label, accuracy in metrics['denoised'].items():\n",
    "        print(f\"  {class_label}: {accuracy:.4f}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d102be-0acb-4155-83b5-468a8ae43b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 339ms/step - loss: 0.0754 - val_loss: 0.0407\n",
      "Epoch 2/2\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 351ms/step - loss: 0.0486 - val_loss: 0.0344\n",
      "Epoch 1/2\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m468s\u001b[0m 822ms/step - loss: 0.0958 - val_loss: 0.0834\n",
      "Epoch 2/2\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 698ms/step - loss: 0.0738 - val_loss: 0.0503\n",
      "Epoch 1/2\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 684ms/step - loss: 0.1163 - val_loss: 0.0565\n",
      "Epoch 2/2\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m355s\u001b[0m 642ms/step - loss: 0.0521 - val_loss: 0.0335\n",
      "Epoch 1/2\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 220ms/step - loss: 0.0904 - val_loss: 0.0369\n",
      "Epoch 2/2\n",
      "\u001b[1m552/552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 218ms/step - loss: 0.0402 - val_loss: 0.0319\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 197ms/step\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 168ms/step\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The classes, ['\"', '+', 'A', 'F', 'L', 'N', 'Q', 'V', 'x', '|', '~'], are not in class_weight",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 183\u001b[0m\n\u001b[0;32m    179\u001b[0m features_denoised \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([extract_wavelet_features(\u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mslice\u001b[39m \u001b[38;5;129;01min\u001b[39;00m denoised_ecg_slices_test])\n\u001b[0;32m    181\u001b[0m test_labels \u001b[38;5;241m=\u001b[39m y_test[:\u001b[38;5;28mlen\u001b[39m(features_noisy)]\n\u001b[1;32m--> 183\u001b[0m model_noisy \u001b[38;5;241m=\u001b[39m \u001b[43mclassify_heartbeats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_noisy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m model_denoised \u001b[38;5;241m=\u001b[39m classify_heartbeats(features_denoised, test_labels)\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Predict the classes\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 134\u001b[0m, in \u001b[0;36mclassify_heartbeats\u001b[1;34m(features, labels)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclassify_heartbeats\u001b[39m(features, labels):\n\u001b[0;32m    133\u001b[0m     clf \u001b[38;5;241m=\u001b[39m SVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m, class_weight\u001b[38;5;241m=\u001b[39mclass_weight_dict)\n\u001b[1;32m--> 134\u001b[0m     \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clf\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\sklearn\\svm\\_base.py:199\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    191\u001b[0m         X,\n\u001b[0;32m    192\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    196\u001b[0m         accept_large_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    197\u001b[0m     )\n\u001b[1;32m--> 199\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[0;32m    202\u001b[0m     [] \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sample_weight, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64\n\u001b[0;32m    203\u001b[0m )\n\u001b[0;32m    204\u001b[0m solver_type \u001b[38;5;241m=\u001b[39m LIBSVM_IMPL\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl)\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\sklearn\\svm\\_base.py:740\u001b[0m, in \u001b[0;36mBaseSVC._validate_targets\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    738\u001b[0m check_classification_targets(y)\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28mcls\u001b[39m, y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_, return_inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 740\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight_ \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_class_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mcls\u001b[39m) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    743\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of classes has to be greater than one; got \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m class\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    744\u001b[0m         \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m    745\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python3\\lib\\site-packages\\sklearn\\utils\\class_weight.py:91\u001b[0m, in \u001b[0;36mcompute_class_weight\u001b[1;34m(class_weight, classes, y)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m unweighted_classes \u001b[38;5;129;01mand\u001b[39;00m n_weighted_classes \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(class_weight):\n\u001b[0;32m     90\u001b[0m         unweighted_classes_user_friendly_str \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(unweighted_classes)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m---> 91\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     92\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe classes, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munweighted_classes_user_friendly_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, are not in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m class_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     94\u001b[0m         )\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weight\n",
      "\u001b[1;31mValueError\u001b[0m: The classes, ['\"', '+', 'A', 'F', 'L', 'N', 'Q', 'V', 'x', '|', '~'], are not in class_weight"
     ]
    }
   ],
   "source": [
    "import wfdb\n",
    "import numpy as np\n",
    "import pywt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, GRU, Dense, Input, Add, Flatten, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the new models\n",
    "def build_1d_cnn(input_shape=(512, 1)):\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = Conv1D(64, 15, padding='same', activation='relu')(inp)\n",
    "    x = Conv1D(128, 15, padding='same', activation='relu')(x)\n",
    "    x = Conv1D(256, 15, padding='same', activation='relu')(x)\n",
    "    x = Conv1D(1, 15, padding='same', activation='tanh')(x)\n",
    "    return Model(inp, x)\n",
    "\n",
    "def build_lstm(input_shape=(512, 1)):\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = LSTM(128, return_sequences=True)(inp)\n",
    "    x = LSTM(64, return_sequences=True)(x)\n",
    "    x = Dense(1, activation='tanh')(x)\n",
    "    return Model(inp, x)\n",
    "\n",
    "def build_gru(input_shape=(512, 1)):\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = GRU(128, return_sequences=True)(inp)\n",
    "    x = GRU(64, return_sequences=True)(x)\n",
    "    x = Dense(1, activation='tanh')(x)\n",
    "    return Model(inp, x)\n",
    "\n",
    "def build_denoising_autoencoder(input_shape=(512, 1)):\n",
    "    inp = Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    encoded = Conv1D(128, 15, padding='same', activation='relu')(inp)\n",
    "    encoded = Conv1D(64, 15, padding='same', activation='relu')(encoded)\n",
    "    \n",
    "    # Decoder (without upsampling to maintain the same input/output size)\n",
    "    decoded = Conv1D(64, 15, padding='same', activation='relu')(encoded)\n",
    "    decoded = Conv1D(128, 15, padding='same', activation='relu')(decoded)\n",
    "    decoded = Conv1D(1, 15, padding='same', activation='tanh')(decoded)\n",
    "    \n",
    "    return Model(inp, decoded)\n",
    "\n",
    "# Ensemble method: averaging predictions from all models\n",
    "def ensemble_predict(models, noisy_signal):\n",
    "    predictions = [model.predict(noisy_signal) for model in models]\n",
    "    ensemble_prediction = np.mean(predictions, axis=0)\n",
    "    return ensemble_prediction.squeeze()\n",
    "\n",
    "# Load ECG and noise data\n",
    "def load_ecg_data_with_labels(record_numbers, segment_length=512):\n",
    "    ecg_segments = []\n",
    "    labels = []\n",
    "    for rec_num in record_numbers:\n",
    "        record = wfdb.rdrecord(f'M:\\\\Dissertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0/{rec_num}')\n",
    "        annotation = wfdb.rdann(f'M:\\\\Dissertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0/{rec_num}', 'atr')\n",
    "        \n",
    "        for i in range(len(annotation.sample)):\n",
    "            start = max(0, annotation.sample[i] - segment_length // 2)\n",
    "            end = min(len(record.p_signal), start + segment_length)\n",
    "            if end - start == segment_length:\n",
    "                ecg_segments.append(record.p_signal[start:end, 0])  # Assuming MLII lead\n",
    "                labels.append(annotation.symbol[i])\n",
    "    \n",
    "    return np.array(ecg_segments), np.array(labels)\n",
    "\n",
    "def load_noise_data():\n",
    "    em = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\em', sampfrom=0).p_signal[:, 0]\n",
    "    bw = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\bw', sampfrom=0).p_signal[:, 0]\n",
    "    ma = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\ma', sampfrom=0).p_signal[:, 0]\n",
    "    return em, bw, ma\n",
    "\n",
    "# Load ECG and noise data\n",
    "ecg_records = [103, 105, 111, 116, 122, 205, 213, 219, 223, 230]  # Add more records as needed\n",
    "ecg_segments, labels = load_ecg_data_with_labels(ecg_records)\n",
    "em_noise, bw_noise, ma_noise = load_noise_data()\n",
    "\n",
    "# Split data into 70% training, 15% validation, and 15% testing\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(ecg_segments, labels, test_size=0.15, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, random_state=42)  # 0.1765 * 85% ≈ 15%\n",
    "\n",
    "# Compute class weights to handle imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Extend and add noise to ECG segments\n",
    "def extend_noise_signal(noise_signal, target_length):\n",
    "    repeated_noise = np.tile(noise_signal, int(np.ceil(target_length / len(noise_signal))))\n",
    "    return repeated_noise[:target_length]\n",
    "\n",
    "def calculate_snr(signal, noise):\n",
    "    signal_power = np.sum(np.square(signal))\n",
    "    noise_power = np.sum(np.square(noise))\n",
    "    snr = 10 * np.log10(signal_power / noise_power)\n",
    "    return snr\n",
    "\n",
    "def calculate_rmse(signal, denoised_signal):\n",
    "    return np.sqrt(np.mean((signal - denoised_signal) ** 2))\n",
    "\n",
    "# Add noise to ECG segments using multiple noise types and combinations\n",
    "def add_combined_noise(ecg_segments, noises, target_snr_db):\n",
    "    noisy_segments = []\n",
    "    \n",
    "    # Generate noise signal by combining all noise types\n",
    "    combined_noise_signal = sum(extend_noise_signal(noise, ecg_segments.shape[1]) for noise in noises)\n",
    "    \n",
    "    for ecg_segment in ecg_segments:\n",
    "        current_snr = calculate_snr(ecg_segment, combined_noise_signal[:len(ecg_segment)])\n",
    "        scaling_factor = np.sqrt(np.sum(np.square(ecg_segment)) / (np.sum(np.square(combined_noise_signal)) * 10**(target_snr_db / 10)))\n",
    "        scaled_noise = combined_noise_signal[:len(ecg_segment)] * scaling_factor\n",
    "        noisy_segment = ecg_segment + scaled_noise\n",
    "        noisy_segments.append(noisy_segment)\n",
    "    return np.array(noisy_segments)\n",
    "\n",
    "# Denoise function\n",
    "def denoise_signal(models, noisy_signal):\n",
    "    noisy_signal = np.expand_dims(noisy_signal, axis=-1)\n",
    "    denoised_signal = ensemble_predict(models, noisy_signal)\n",
    "    return denoised_signal.squeeze()\n",
    "\n",
    "# Function to extract wavelet features\n",
    "def extract_wavelet_features(ecg_slice):\n",
    "    coeffs = pywt.wavedec(ecg_slice, 'db6', level=5)\n",
    "    return coeffs[0]  # You may want to use more features from different levels\n",
    "\n",
    "# Function to classify heartbeats using SVM\n",
    "def classify_heartbeats(features, labels):\n",
    "    clf = SVC(kernel='linear', class_weight=class_weight_dict)\n",
    "    clf.fit(features, labels)\n",
    "    return clf\n",
    "\n",
    "# Add single noises to combined noises\n",
    "noises = [em_noise, bw_noise, ma_noise]\n",
    "\n",
    "snr_db = 0  # Example SNR value\n",
    "\n",
    "# Prepare the clean ECG segments as target data for training\n",
    "clean_ecg_segments_train = X_train.copy()\n",
    "\n",
    "# Create noisy training data by combining all noise types\n",
    "noisy_ecg_slices_train = add_combined_noise(X_train, noises, snr_db)\n",
    "noisy_ecg_slices_val = add_combined_noise(X_val, noises, snr_db)\n",
    "\n",
    "# Instantiate the models\n",
    "models = [\n",
    "    build_1d_cnn(),\n",
    "    build_lstm(),\n",
    "    build_gru(),\n",
    "    build_denoising_autoencoder()\n",
    "]\n",
    "\n",
    "# Expand dimensions for training and validation\n",
    "noisy_ecg_slices_train_expanded = np.expand_dims(noisy_ecg_slices_train, axis=-1)  # Expand dims for the models\n",
    "noisy_ecg_slices_val_expanded = np.expand_dims(noisy_ecg_slices_val, axis=-1)  # Expand dims for validation\n",
    "clean_ecg_segments_val_expanded = np.expand_dims(X_val, axis=-1)\n",
    "\n",
    "# Train each model\n",
    "for model in models:\n",
    "    model.compile(optimizer=Adam(), loss='mse')\n",
    "    model.fit(noisy_ecg_slices_train_expanded, np.expand_dims(clean_ecg_segments_train, axis=-1), \n",
    "              epochs=2, batch_size=32, validation_data=(noisy_ecg_slices_val_expanded, clean_ecg_segments_val_expanded))\n",
    "\n",
    "# Evaluate the trained models on each noise type and combination\n",
    "noisy_ecg_slices_test = add_combined_noise(X_test, noises, snr_db)\n",
    "\n",
    "# Denoise and classify\n",
    "denoised_ecg_slices_test = denoise_signal(models, noisy_ecg_slices_test)\n",
    "\n",
    "# Calculate SNR and RMSE\n",
    "snr_values = [calculate_snr(ecg, denoised) for ecg, denoised in zip(X_test, denoised_ecg_slices_test)]\n",
    "rmse_values = [calculate_rmse(ecg, denoised) for ecg, denoised in zip(X_test, denoised_ecg_slices_test)]\n",
    "\n",
    "features_noisy = np.array([extract_wavelet_features(slice) for slice in noisy_ecg_slices_test])\n",
    "features_denoised = np.array([extract_wavelet_features(slice) for slice in denoised_ecg_slices_test])\n",
    "\n",
    "test_labels = y_test[:len(features_noisy)]\n",
    "\n",
    "model_noisy = classify_heartbeats(features_noisy, test_labels)\n",
    "model_denoised = classify_heartbeats(features_denoised, test_labels)\n",
    "\n",
    "# Predict the classes\n",
    "predictions_noisy = model_noisy.predict(features_noisy)\n",
    "predictions_denoised = model_denoised.predict(features_denoised)\n",
    "\n",
    "# Get the unique classes present in test_labels\n",
    "unique_classes = np.unique(test_labels)\n",
    "class_names = [name for i, name in enumerate(['N', 'V', 'A', 'L']) if i in unique_classes]\n",
    "\n",
    "# Evaluate accuracy for each class\n",
    "report_noisy = classification_report(test_labels, predictions_noisy, target_names=class_names, output_dict=True, zero_division=0)\n",
    "report_denoised = classification_report(test_labels, predictions_denoised, target_names=class_names, output_dict=True, zero_division=0)\n",
    "\n",
    "# Output the results for the combined noise\n",
    "results = {\n",
    "    'noisy': {class_name: report_noisy[class_name]['precision'] for class_name in class_names},\n",
    "    'denoised': {class_name: report_denoised[class_name]['precision'] for class_name in class_names},\n",
    "    'snr': np.mean(snr_values),\n",
    "    'rmse': np.mean(rmse_values)\n",
    "}\n",
    "\n",
    "print(f\"Average SNR after denoising: {results['snr']:.4f} dB\")\n",
    "print(f\"Average RMSE after denoising: {results['rmse']:.4f}\")\n",
    "print(\"Noisy data accuracies:\")\n",
    "for class_label, accuracy in results['noisy'].items():\n",
    "    print(f\"  {class_label}: {accuracy:.4f}\")\n",
    "print(\"Denoised data accuracies:\")\n",
    "for class_label, accuracy in results['denoised'].items():\n",
    "    print(f\"  {class_label}: {accuracy:.4f}\")\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b959e5f-b09f-437f-bd16-08261be7ebd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 395ms/step - loss: 0.0722 - val_loss: 0.0401\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m455s\u001b[0m 829ms/step - loss: 0.1033 - val_loss: 0.0648\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m410s\u001b[0m 745ms/step - loss: 0.0992 - val_loss: 0.0455\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 222ms/step - loss: 0.0805 - val_loss: 0.0378\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 79ms/step\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 194ms/step\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 171ms/step\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step\n",
      "Average SNR after denoising: 0.8871 dB\n",
      "Average RMSE after denoising: 0.1521\n",
      "Noisy data accuracies:\n",
      "  N: 0.9983\n",
      "  V: 0.7321\n",
      "  A: 0.0925\n",
      "  L: 0.9099\n",
      "Denoised data accuracies:\n",
      "  N: 0.9927\n",
      "  V: 0.5114\n",
      "  A: 0.0590\n",
      "  L: 0.7831\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import wfdb\n",
    "import numpy as np\n",
    "import pywt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, GRU, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the new models\n",
    "def build_1d_cnn(input_shape=(512, 1)):\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = Conv1D(64, 15, padding='same', activation='relu')(inp)\n",
    "    x = Conv1D(128, 15, padding='same', activation='relu')(x)\n",
    "    x = Conv1D(256, 15, padding='same', activation='relu')(x)\n",
    "    x = Conv1D(1, 15, padding='same', activation='tanh')(x)\n",
    "    return Model(inp, x)\n",
    "\n",
    "def build_lstm(input_shape=(512, 1)):\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = LSTM(128, return_sequences=True)(inp)\n",
    "    x = LSTM(64, return_sequences=True)(x)\n",
    "    x = Dense(1, activation='tanh')(x)\n",
    "    return Model(inp, x)\n",
    "\n",
    "def build_gru(input_shape=(512, 1)):\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = GRU(128, return_sequences=True)(inp)\n",
    "    x = GRU(64, return_sequences=True)(x)\n",
    "    x = Dense(1, activation='tanh')(x)\n",
    "    return Model(inp, x)\n",
    "\n",
    "def build_denoising_autoencoder(input_shape=(512, 1)):\n",
    "    inp = Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    encoded = Conv1D(128, 15, padding='same', activation='relu')(inp)\n",
    "    encoded = Conv1D(64, 15, padding='same', activation='relu')(encoded)\n",
    "    \n",
    "    # Decoder (without upsampling to maintain the same input/output size)\n",
    "    decoded = Conv1D(64, 15, padding='same', activation='relu')(encoded)\n",
    "    decoded = Conv1D(128, 15, padding='same', activation='relu')(decoded)\n",
    "    decoded = Conv1D(1, 15, padding='same', activation='tanh')(decoded)\n",
    "    \n",
    "    return Model(inp, decoded)\n",
    "\n",
    "# Ensemble method: averaging predictions from all models\n",
    "def ensemble_predict(models, noisy_signal):\n",
    "    predictions = [model.predict(noisy_signal) for model in models]\n",
    "    ensemble_prediction = np.mean(predictions, axis=0)\n",
    "    return ensemble_prediction.squeeze()\n",
    "\n",
    "# Load ECG and noise data\n",
    "def load_ecg_data_with_labels(record_numbers, segment_length=512):\n",
    "    ecg_segments = []\n",
    "    labels = []\n",
    "    for rec_num in record_numbers:\n",
    "        record = wfdb.rdrecord(f'M:\\\\Dissertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0/{rec_num}')\n",
    "        annotation = wfdb.rdann(f'M:\\\\Dissertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0/{rec_num}', 'atr')\n",
    "        \n",
    "        for i in range(len(annotation.sample)):\n",
    "            start = max(0, annotation.sample[i] - segment_length // 2)\n",
    "            end = min(len(record.p_signal), start + segment_length)\n",
    "            if end - start == segment_length:\n",
    "                ecg_segments.append(record.p_signal[start:end, 0])  # Assuming MLII lead\n",
    "                labels.append(annotation.symbol[i])\n",
    "    \n",
    "    return np.array(ecg_segments), np.array(labels)\n",
    "\n",
    "def load_noise_data():\n",
    "    em = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\em', sampfrom=0).p_signal[:, 0]\n",
    "    bw = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\bw', sampfrom=0).p_signal[:, 0]\n",
    "    ma = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\ma', sampfrom=0).p_signal[:, 0]\n",
    "    return em, bw, ma\n",
    "\n",
    "# Load ECG and noise data\n",
    "ecg_records = [103, 105, 111, 116, 122, 205, 213, 219, 223, 230]  # Add more records as needed\n",
    "ecg_segments, labels = load_ecg_data_with_labels(ecg_records)\n",
    "em_noise, bw_noise, ma_noise = load_noise_data()\n",
    "\n",
    "# Filter and map labels to integer categories\n",
    "label_mapping = {'N': 0, 'V': 1, 'A': 2, 'L': 3}  # Modify this based on the classes you want to classify\n",
    "mapped_labels = np.array([label_mapping.get(label, -1) for label in labels])\n",
    "valid_indices = mapped_labels != -1  # Filter out invalid labels\n",
    "\n",
    "# Filter data and labels to only include valid classes\n",
    "ecg_segments = ecg_segments[valid_indices]\n",
    "mapped_labels = mapped_labels[valid_indices]\n",
    "\n",
    "# Split data into 70% training, 15% validation, and 15% testing\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(ecg_segments, mapped_labels, test_size=0.15, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, random_state=42)  # 0.1765 * 85% ≈ 15%\n",
    "\n",
    "# Compute class weights to handle imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Extend and add noise to ECG segments\n",
    "def extend_noise_signal(noise_signal, target_length):\n",
    "    repeated_noise = np.tile(noise_signal, int(np.ceil(target_length / len(noise_signal))))\n",
    "    return repeated_noise[:target_length]\n",
    "\n",
    "def calculate_snr(signal, noise):\n",
    "    signal_power = np.sum(np.square(signal))\n",
    "    noise_power = np.sum(np.square(noise))\n",
    "    snr = 10 * np.log10(signal_power / noise_power)\n",
    "    return snr\n",
    "\n",
    "def calculate_rmse(signal, denoised_signal):\n",
    "    return np.sqrt(np.mean((signal - denoised_signal) ** 2))\n",
    "\n",
    "# Add noise to ECG segments using multiple noise types and combinations\n",
    "def add_combined_noise(ecg_segments, noises, target_snr_db):\n",
    "    noisy_segments = []\n",
    "    \n",
    "    # Generate noise signal by combining all noise types\n",
    "    combined_noise_signal = sum(extend_noise_signal(noise, ecg_segments.shape[1]) for noise in noises)\n",
    "    \n",
    "    for ecg_segment in ecg_segments:\n",
    "        current_snr = calculate_snr(ecg_segment, combined_noise_signal[:len(ecg_segment)])\n",
    "        scaling_factor = np.sqrt(np.sum(np.square(ecg_segment)) / (np.sum(np.square(combined_noise_signal)) * 10**(target_snr_db / 10)))\n",
    "        scaled_noise = combined_noise_signal[:len(ecg_segment)] * scaling_factor\n",
    "        noisy_segment = ecg_segment + scaled_noise\n",
    "        noisy_segments.append(noisy_segment)\n",
    "    return np.array(noisy_segments)\n",
    "\n",
    "# Denoise function\n",
    "def denoise_signal(models, noisy_signal):\n",
    "    noisy_signal = np.expand_dims(noisy_signal, axis=-1)\n",
    "    denoised_signal = ensemble_predict(models, noisy_signal)\n",
    "    return denoised_signal.squeeze()\n",
    "\n",
    "# Function to extract wavelet features\n",
    "def extract_wavelet_features(ecg_slice):\n",
    "    coeffs = pywt.wavedec(ecg_slice, 'db6', level=5)\n",
    "    return coeffs[0]  # You may want to use more features from different levels\n",
    "\n",
    "# Function to classify heartbeats using SVM\n",
    "def classify_heartbeats(features, labels):\n",
    "    clf = SVC(kernel='linear', class_weight=class_weight_dict)\n",
    "    clf.fit(features, labels)\n",
    "    return clf\n",
    "\n",
    "# Add single noises to combined noises\n",
    "noises = [em_noise, bw_noise, ma_noise]\n",
    "\n",
    "snr_db = 0  # Example SNR value\n",
    "\n",
    "# Prepare the clean ECG segments as target data for training\n",
    "clean_ecg_segments_train = X_train.copy()\n",
    "\n",
    "# Create noisy training data by combining all noise types\n",
    "noisy_ecg_slices_train = add_combined_noise(X_train, noises, snr_db)\n",
    "noisy_ecg_slices_val = add_combined_noise(X_val, noises, snr_db)\n",
    "\n",
    "# Instantiate the models\n",
    "models = [\n",
    "    build_1d_cnn(),\n",
    "    build_lstm(),\n",
    "    build_gru(),\n",
    "    build_denoising_autoencoder()\n",
    "]\n",
    "\n",
    "# Expand dimensions for training and validation\n",
    "noisy_ecg_slices_train_expanded = np.expand_dims(noisy_ecg_slices_train, axis=-1)  # Expand dims for the models\n",
    "noisy_ecg_slices_val_expanded = np.expand_dims(noisy_ecg_slices_val, axis=-1)  # Expand dims for validation\n",
    "clean_ecg_segments_val_expanded = np.expand_dims(X_val, axis=-1)\n",
    "\n",
    "# Train each model\n",
    "for model in models:\n",
    "    model.compile(optimizer=Adam(), loss='mse')\n",
    "    model.fit(noisy_ecg_slices_train_expanded, np.expand_dims(clean_ecg_segments_train, axis=-1), \n",
    "              epochs=1, batch_size=32, validation_data=(noisy_ecg_slices_val_expanded, clean_ecg_segments_val_expanded))\n",
    "\n",
    "# Evaluate the trained models on each noise type and combination\n",
    "noisy_ecg_slices_test = add_combined_noise(X_test, noises, snr_db)\n",
    "\n",
    "# Denoise and classify\n",
    "denoised_ecg_slices_test = denoise_signal(models, noisy_ecg_slices_test)\n",
    "\n",
    "# Calculate SNR and RMSE\n",
    "snr_values = [calculate_snr(ecg, denoised) for ecg, denoised in zip(X_test, denoised_ecg_slices_test)]\n",
    "rmse_values = [calculate_rmse(ecg, denoised) for ecg, denoised in zip(X_test, denoised_ecg_slices_test)]\n",
    "\n",
    "features_noisy = np.array([extract_wavelet_features(slice) for slice in noisy_ecg_slices_test])\n",
    "features_denoised = np.array([extract_wavelet_features(slice) for slice in denoised_ecg_slices_test])\n",
    "\n",
    "test_labels = y_test[:len(features_noisy)]\n",
    "\n",
    "model_noisy = classify_heartbeats(features_noisy, test_labels)\n",
    "model_denoised = classify_heartbeats(features_denoised, test_labels)\n",
    "\n",
    "# Predict the classes\n",
    "predictions_noisy = model_noisy.predict(features_noisy)\n",
    "predictions_denoised = model_denoised.predict(features_denoised)\n",
    "\n",
    "# Get the unique classes present in test_labels\n",
    "unique_classes = np.unique(test_labels)\n",
    "class_names = [name for i, name in enumerate(['N', 'V', 'A', 'L']) if i in unique_classes]\n",
    "\n",
    "# Evaluate accuracy for each class\n",
    "report_noisy = classification_report(test_labels, predictions_noisy, target_names=class_names, output_dict=True, zero_division=0)\n",
    "report_denoised = classification_report(test_labels, predictions_denoised, target_names=class_names, output_dict=True, zero_division=0)\n",
    "\n",
    "# Output the results for the combined noise\n",
    "results = {\n",
    "    'noisy': {class_name: report_noisy[class_name]['precision'] for class_name in class_names},\n",
    "    'denoised': {class_name: report_denoised[class_name]['precision'] for class_name in class_names},\n",
    "    'snr': np.mean(snr_values),\n",
    "    'rmse': np.mean(rmse_values)\n",
    "}\n",
    "\n",
    "print(f\"Average SNR after denoising: {results['snr']:.4f} dB\")\n",
    "print(f\"Average RMSE after denoising: {results['rmse']:.4f}\")\n",
    "print(\"Noisy data accuracies:\")\n",
    "for class_label, accuracy in results['noisy'].items():\n",
    "    print(f\"  {class_label}: {accuracy:.4f}\")\n",
    "print(\"Denoised data accuracies:\")\n",
    "for class_label, accuracy in results['denoised'].items():\n",
    "    print(f\"  {class_label}: {accuracy:.4f}\")\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70d54955-1089-4525-908e-c0cb70625c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 335ms/step - loss: 0.0801 - val_loss: 0.0412\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 623ms/step - loss: 0.1153 - val_loss: 0.0558\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 843ms/step - loss: 0.0966 - val_loss: 0.0513\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 317ms/step - loss: 0.0716 - val_loss: 0.0366\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 104ms/step\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 235ms/step\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 225ms/step\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 64ms/step\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 101ms/step\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 239ms/step\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 209ms/step\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 64ms/step\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 103ms/step\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 241ms/step\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 190ms/step\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 75ms/step\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 195ms/step\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 166ms/step\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 190ms/step\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 166ms/step\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 197ms/step\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 167ms/step\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 196ms/step\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 165ms/step\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step\n",
      "Noise type: EM\n",
      "Average SNR after denoising: 1.5857 dB\n",
      "Average RMSE after denoising: 0.4233\n",
      "Noisy data accuracies:\n",
      "  N: 0.9983\n",
      "  V: 0.6833\n",
      "  A: 0.1221\n",
      "  L: 0.9350\n",
      "Denoised data accuracies:\n",
      "  N: 0.9969\n",
      "  V: 0.7083\n",
      "  A: 0.0800\n",
      "  L: 0.7652\n",
      "\n",
      "\n",
      "Noise type: BW\n",
      "Average SNR after denoising: 1.3411 dB\n",
      "Average RMSE after denoising: 0.1780\n",
      "Noisy data accuracies:\n",
      "  N: 0.9986\n",
      "  V: 0.7470\n",
      "  A: 0.0884\n",
      "  L: 0.9414\n",
      "Denoised data accuracies:\n",
      "  N: 0.9953\n",
      "  V: 0.5867\n",
      "  A: 0.0459\n",
      "  L: 0.8882\n",
      "\n",
      "\n",
      "Noise type: MA\n",
      "Average SNR after denoising: 0.7210 dB\n",
      "Average RMSE after denoising: 0.5183\n",
      "Noisy data accuracies:\n",
      "  N: 0.9976\n",
      "  V: 0.7595\n",
      "  A: 0.0851\n",
      "  L: 0.9301\n",
      "Denoised data accuracies:\n",
      "  N: 0.9965\n",
      "  V: 0.5769\n",
      "  A: 0.1019\n",
      "  L: 0.8184\n",
      "\n",
      "\n",
      "Noise type: EM+MA\n",
      "Average SNR after denoising: 1.5016 dB\n",
      "Average RMSE after denoising: 0.4154\n",
      "Noisy data accuracies:\n",
      "  N: 0.9983\n",
      "  V: 0.6796\n",
      "  A: 0.1221\n",
      "  L: 0.9381\n",
      "Denoised data accuracies:\n",
      "  N: 0.9954\n",
      "  V: 0.6802\n",
      "  A: 0.0610\n",
      "  L: 0.8059\n",
      "\n",
      "\n",
      "Noise type: EM+BW\n",
      "Average SNR after denoising: 1.1683 dB\n",
      "Average RMSE after denoising: 0.1698\n",
      "Noisy data accuracies:\n",
      "  N: 0.9983\n",
      "  V: 0.7321\n",
      "  A: 0.0958\n",
      "  L: 0.9018\n",
      "Denoised data accuracies:\n",
      "  N: 0.9957\n",
      "  V: 0.5616\n",
      "  A: 0.0546\n",
      "  L: 0.8808\n",
      "\n",
      "\n",
      "Noise type: MA+BW\n",
      "Average SNR after denoising: 1.2847 dB\n",
      "Average RMSE after denoising: 0.1793\n",
      "Noisy data accuracies:\n",
      "  N: 0.9986\n",
      "  V: 0.7470\n",
      "  A: 0.0889\n",
      "  L: 0.9356\n",
      "Denoised data accuracies:\n",
      "  N: 0.9936\n",
      "  V: 0.5468\n",
      "  A: 0.0558\n",
      "  L: 0.8902\n",
      "\n",
      "\n",
      "Noise type: EM+BW+MA\n",
      "Average SNR after denoising: 1.0430 dB\n",
      "Average RMSE after denoising: 0.1530\n",
      "Noisy data accuracies:\n",
      "  N: 0.9983\n",
      "  V: 0.7321\n",
      "  A: 0.0925\n",
      "  L: 0.9099\n",
      "Denoised data accuracies:\n",
      "  N: 0.9942\n",
      "  V: 0.5211\n",
      "  A: 0.0556\n",
      "  L: 0.8547\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import wfdb\n",
    "import numpy as np\n",
    "import pywt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, GRU, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the new models\n",
    "def build_1d_cnn(input_shape=(512, 1)):\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = Conv1D(64, 15, padding='same', activation='relu')(inp)\n",
    "    x = Conv1D(128, 15, padding='same', activation='relu')(x)\n",
    "    x = Conv1D(256, 15, padding='same', activation='relu')(x)\n",
    "    x = Conv1D(1, 15, padding='same', activation='tanh')(x)\n",
    "    return Model(inp, x)\n",
    "\n",
    "def build_lstm(input_shape=(512, 1)):\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = LSTM(128, return_sequences=True)(inp)\n",
    "    x = LSTM(64, return_sequences=True)(x)\n",
    "    x = Dense(1, activation='tanh')(x)\n",
    "    return Model(inp, x)\n",
    "\n",
    "def build_gru(input_shape=(512, 1)):\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = GRU(128, return_sequences=True)(inp)\n",
    "    x = GRU(64, return_sequences=True)(x)\n",
    "    x = Dense(1, activation='tanh')(x)\n",
    "    return Model(inp, x)\n",
    "\n",
    "def build_denoising_autoencoder(input_shape=(512, 1)):\n",
    "    inp = Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    encoded = Conv1D(128, 15, padding='same', activation='relu')(inp)\n",
    "    encoded = Conv1D(64, 15, padding='same', activation='relu')(encoded)\n",
    "    \n",
    "    # Decoder (without upsampling to maintain the same input/output size)\n",
    "    decoded = Conv1D(64, 15, padding='same', activation='relu')(encoded)\n",
    "    decoded = Conv1D(128, 15, padding='same', activation='relu')(decoded)\n",
    "    decoded = Conv1D(1, 15, padding='same', activation='tanh')(decoded)\n",
    "    \n",
    "    return Model(inp, decoded)\n",
    "\n",
    "# Ensemble method: averaging predictions from all models\n",
    "def ensemble_predict(models, noisy_signal):\n",
    "    predictions = [model.predict(noisy_signal) for model in models]\n",
    "    ensemble_prediction = np.mean(predictions, axis=0)\n",
    "    return ensemble_prediction.squeeze()\n",
    "\n",
    "# Load ECG and noise data\n",
    "def load_ecg_data_with_labels(record_numbers, segment_length=512):\n",
    "    ecg_segments = []\n",
    "    labels = []\n",
    "    for rec_num in record_numbers:\n",
    "        record = wfdb.rdrecord(f'M:\\\\Dissertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0/{rec_num}')\n",
    "        annotation = wfdb.rdann(f'M:\\\\Dissertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0/{rec_num}', 'atr')\n",
    "        \n",
    "        for i in range(len(annotation.sample)):\n",
    "            start = max(0, annotation.sample[i] - segment_length // 2)\n",
    "            end = min(len(record.p_signal), start + segment_length)\n",
    "            if end - start == segment_length:\n",
    "                ecg_segments.append(record.p_signal[start:end, 0])  # Assuming MLII lead\n",
    "                labels.append(annotation.symbol[i])\n",
    "    \n",
    "    return np.array(ecg_segments), np.array(labels)\n",
    "\n",
    "def load_noise_data():\n",
    "    em = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\em', sampfrom=0).p_signal[:, 0]\n",
    "    bw = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\bw', sampfrom=0).p_signal[:, 0]\n",
    "    ma = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\ma', sampfrom=0).p_signal[:, 0]\n",
    "    return em, bw, ma\n",
    "\n",
    "# Load ECG and noise data\n",
    "ecg_records = [103, 105, 111, 116, 122, 205, 213, 219, 223, 230]  # Add more records as needed\n",
    "ecg_segments, labels = load_ecg_data_with_labels(ecg_records)\n",
    "em_noise, bw_noise, ma_noise = load_noise_data()\n",
    "\n",
    "# Filter and map labels to integer categories\n",
    "label_mapping = {'N': 0, 'V': 1, 'A': 2, 'L': 3}  # Modify this based on the classes you want to classify\n",
    "mapped_labels = np.array([label_mapping.get(label, -1) for label in labels])\n",
    "valid_indices = mapped_labels != -1  # Filter out invalid labels\n",
    "\n",
    "# Filter data and labels to only include valid classes\n",
    "ecg_segments = ecg_segments[valid_indices]\n",
    "mapped_labels = mapped_labels[valid_indices]\n",
    "\n",
    "# Split data into 70% training, 15% validation, and 15% testing\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(ecg_segments, mapped_labels, test_size=0.15, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, random_state=42)  # 0.1765 * 85% ≈ 15%\n",
    "\n",
    "# Compute class weights to handle imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Extend and add noise to ECG segments\n",
    "def extend_noise_signal(noise_signal, target_length):\n",
    "    repeated_noise = np.tile(noise_signal, int(np.ceil(target_length / len(noise_signal))))\n",
    "    return repeated_noise[:target_length]\n",
    "\n",
    "def calculate_snr(signal, noise):\n",
    "    signal_power = np.sum(np.square(signal))\n",
    "    noise_power = np.sum(np.square(noise))\n",
    "    snr = 10 * np.log10(signal_power / noise_power)\n",
    "    return snr\n",
    "\n",
    "def calculate_rmse(signal, denoised_signal):\n",
    "    return np.sqrt(np.mean((signal - denoised_signal) ** 2))\n",
    "\n",
    "# Add noise to ECG segments using multiple noise types and combinations\n",
    "def add_combined_noise(ecg_segments, noises, target_snr_db):\n",
    "    noisy_segments = []\n",
    "    \n",
    "    # Generate noise signal by combining all noise types\n",
    "    combined_noise_signal = sum(extend_noise_signal(noise, ecg_segments.shape[1]) for noise in noises)\n",
    "    \n",
    "    for ecg_segment in ecg_segments:\n",
    "        current_snr = calculate_snr(ecg_segment, combined_noise_signal[:len(ecg_segment)])\n",
    "        scaling_factor = np.sqrt(np.sum(np.square(ecg_segment)) / (np.sum(np.square(combined_noise_signal)) * 10**(target_snr_db / 10)))\n",
    "        scaled_noise = combined_noise_signal[:len(ecg_segment)] * scaling_factor\n",
    "        noisy_segment = ecg_segment + scaled_noise\n",
    "        noisy_segments.append(noisy_segment)\n",
    "    return np.array(noisy_segments)\n",
    "\n",
    "# Denoise function\n",
    "def denoise_signal(models, noisy_signal):\n",
    "    noisy_signal = np.expand_dims(noisy_signal, axis=-1)\n",
    "    denoised_signal = ensemble_predict(models, noisy_signal)\n",
    "    return denoised_signal.squeeze()\n",
    "\n",
    "# Function to extract wavelet features\n",
    "def extract_wavelet_features(ecg_slice):\n",
    "    coeffs = pywt.wavedec(ecg_slice, 'db6', level=5)\n",
    "    return coeffs[0]  # You may want to use more features from different levels\n",
    "\n",
    "# Function to classify heartbeats using SVM\n",
    "def classify_heartbeats(features, labels):\n",
    "    clf = SVC(kernel='linear', class_weight=class_weight_dict)\n",
    "    clf.fit(features, labels)\n",
    "    return clf\n",
    "\n",
    "# Add single noises to combined noises\n",
    "noises_dict = {\n",
    "    'EM': [em_noise],\n",
    "    'BW': [bw_noise],\n",
    "    'MA': [ma_noise],\n",
    "    'EM+MA': [em_noise, ma_noise],\n",
    "    'EM+BW': [em_noise, bw_noise],\n",
    "    'MA+BW': [ma_noise, bw_noise],\n",
    "    'EM+BW+MA': [em_noise, bw_noise, ma_noise]\n",
    "}\n",
    "\n",
    "snr_db = 0  # Example SNR value\n",
    "\n",
    "# Prepare the clean ECG segments as target data for training\n",
    "clean_ecg_segments_train = X_train.copy()\n",
    "\n",
    "# Create noisy training data by combining all noise types\n",
    "noisy_ecg_slices_train = add_combined_noise(X_train, [em_noise, bw_noise, ma_noise], snr_db)\n",
    "noisy_ecg_slices_val = add_combined_noise(X_val, [em_noise, bw_noise, ma_noise], snr_db)\n",
    "\n",
    "# Instantiate the models\n",
    "models = [\n",
    "    build_1d_cnn(),\n",
    "    build_lstm(),\n",
    "    build_gru(),\n",
    "    build_denoising_autoencoder()\n",
    "]\n",
    "\n",
    "# Expand dimensions for training and validation\n",
    "noisy_ecg_slices_train_expanded = np.expand_dims(noisy_ecg_slices_train, axis=-1)  # Expand dims for the models\n",
    "noisy_ecg_slices_val_expanded = np.expand_dims(noisy_ecg_slices_val, axis=-1)  # Expand dims for validation\n",
    "clean_ecg_segments_val_expanded = np.expand_dims(X_val, axis=-1)\n",
    "\n",
    "# Train each model\n",
    "for model in models:\n",
    "    model.compile(optimizer=Adam(), loss='mse')\n",
    "    model.fit(noisy_ecg_slices_train_expanded, np.expand_dims(clean_ecg_segments_train, axis=-1), \n",
    "              epochs=1, batch_size=32, validation_data=(noisy_ecg_slices_val_expanded, clean_ecg_segments_val_expanded))\n",
    "\n",
    "# Evaluate the trained models on each noise type and combination\n",
    "results = {}\n",
    "\n",
    "for noise_name, noise_data in noises_dict.items():\n",
    "    noisy_ecg_slices_test = add_combined_noise(X_test, noise_data, snr_db)\n",
    "\n",
    "    # Denoise and classify\n",
    "    denoised_ecg_slices_test = denoise_signal(models, noisy_ecg_slices_test)\n",
    "\n",
    "    # Calculate SNR and RMSE\n",
    "    snr_values = [calculate_snr(ecg, denoised) for ecg, denoised in zip(X_test, denoised_ecg_slices_test)]\n",
    "    rmse_values = [calculate_rmse(ecg, denoised) for ecg, denoised in zip(X_test, denoised_ecg_slices_test)]\n",
    "\n",
    "    features_noisy = np.array([extract_wavelet_features(slice) for slice in noisy_ecg_slices_test])\n",
    "    features_denoised = np.array([extract_wavelet_features(slice) for slice in denoised_ecg_slices_test])\n",
    "\n",
    "    test_labels = y_test[:len(features_noisy)]\n",
    "\n",
    "    model_noisy = classify_heartbeats(features_noisy, test_labels)\n",
    "    model_denoised = classify_heartbeats(features_denoised, test_labels)\n",
    "\n",
    "    # Predict the classes\n",
    "    predictions_noisy = model_noisy.predict(features_noisy)\n",
    "    predictions_denoised = model_denoised.predict(features_denoised)\n",
    "\n",
    "    # Get the unique classes present in test_labels\n",
    "    unique_classes = np.unique(test_labels)\n",
    "    class_names = [name for i, name in enumerate(['N', 'V', 'A', 'L']) if i in unique_classes]\n",
    "\n",
    "    # Evaluate accuracy for each class\n",
    "    report_noisy = classification_report(test_labels, predictions_noisy, target_names=class_names, output_dict=True, zero_division=0)\n",
    "    report_denoised = classification_report(test_labels, predictions_denoised, target_names=class_names, output_dict=True, zero_division=0)\n",
    "\n",
    "    # Store accuracy and SNR, RMSE for each class\n",
    "    results[noise_name] = {\n",
    "        'snr': np.mean(snr_values),\n",
    "        'rmse': np.mean(rmse_values),\n",
    "        'noisy': {class_name: report_noisy[class_name]['precision'] for class_name in class_names},\n",
    "        'denoised': {class_name: report_denoised[class_name]['precision'] for class_name in class_names},\n",
    "    }\n",
    "\n",
    "# Output the results for each noise condition\n",
    "for noise_name, metrics in results.items():\n",
    "    print(f\"Noise type: {noise_name}\")\n",
    "    print(f\"Average SNR after denoising: {metrics['snr']:.4f} dB\")\n",
    "    print(f\"Average RMSE after denoising: {metrics['rmse']:.4f}\")\n",
    "    print(\"Noisy data accuracies:\")\n",
    "    for class_label, accuracy in metrics['noisy'].items():\n",
    "        print(f\"  {class_label}: {accuracy:.4f}\")\n",
    "    print(\"Denoised data accuracies:\")\n",
    "    for class_label, accuracy in metrics['denoised'].items():\n",
    "        print(f\"  {class_label}: {accuracy:.4f}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1bf812-e5ac-401f-8d83-5562152c915a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 333ms/step - loss: 0.0757 - val_loss: 0.0401\n",
      "Epoch 2/5\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 347ms/step - loss: 0.0433 - val_loss: 0.0330\n",
      "Epoch 3/5\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 347ms/step - loss: 0.0357 - val_loss: 0.0307\n",
      "Epoch 4/5\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 431ms/step - loss: 0.0345 - val_loss: 0.0308\n",
      "Epoch 5/5\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 438ms/step - loss: 0.0345 - val_loss: 0.0294\n",
      "Epoch 1/5\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m440s\u001b[0m 819ms/step - loss: 0.1065 - val_loss: 0.0414\n",
      "Epoch 2/5\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 627ms/step - loss: 0.0623 - val_loss: 0.0485\n",
      "Epoch 3/5\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 612ms/step - loss: 0.0529 - val_loss: 0.0349\n",
      "Epoch 4/5\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 624ms/step - loss: 0.0370 - val_loss: 0.0315\n",
      "Epoch 5/5\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 601ms/step - loss: 0.0350 - val_loss: 0.0306\n",
      "Epoch 1/5\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 928ms/step - loss: 0.0973 - val_loss: 0.0463\n",
      "Epoch 2/5\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729ms/step - loss: 0.0459"
     ]
    }
   ],
   "source": [
    "import wfdb\n",
    "import numpy as np\n",
    "import pywt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, GRU, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the new models\n",
    "def build_1d_cnn(input_shape=(512, 1)):\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = Conv1D(64, 15, padding='same', activation='relu')(inp)\n",
    "    x = Conv1D(128, 15, padding='same', activation='relu')(x)\n",
    "    x = Conv1D(256, 15, padding='same', activation='relu')(x)\n",
    "    x = Conv1D(1, 15, padding='same', activation='tanh')(x)\n",
    "    return Model(inp, x)\n",
    "\n",
    "def build_lstm(input_shape=(512, 1)):\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = LSTM(128, return_sequences=True)(inp)\n",
    "    x = LSTM(64, return_sequences=True)(x)\n",
    "    x = Dense(1, activation='tanh')(x)\n",
    "    return Model(inp, x)\n",
    "\n",
    "def build_gru(input_shape=(512, 1)):\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = GRU(128, return_sequences=True)(inp)\n",
    "    x = GRU(64, return_sequences=True)(x)\n",
    "    x = Dense(1, activation='tanh')(x)\n",
    "    return Model(inp, x)\n",
    "\n",
    "def build_denoising_autoencoder(input_shape=(512, 1)):\n",
    "    inp = Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    encoded = Conv1D(128, 15, padding='same', activation='relu')(inp)\n",
    "    encoded = Conv1D(64, 15, padding='same', activation='relu')(encoded)\n",
    "    \n",
    "    # Decoder (without upsampling to maintain the same input/output size)\n",
    "    decoded = Conv1D(64, 15, padding='same', activation='relu')(encoded)\n",
    "    decoded = Conv1D(128, 15, padding='same', activation='relu')(decoded)\n",
    "    decoded = Conv1D(1, 15, padding='same', activation='tanh')(decoded)\n",
    "    \n",
    "    return Model(inp, decoded)\n",
    "\n",
    "# Ensemble method: averaging predictions from all models\n",
    "def ensemble_predict(models, noisy_signal):\n",
    "    predictions = [model.predict(noisy_signal) for model in models]\n",
    "    ensemble_prediction = np.mean(predictions, axis=0)\n",
    "    return ensemble_prediction.squeeze()\n",
    "\n",
    "# Load ECG and noise data\n",
    "def load_ecg_data_with_labels(record_numbers, segment_length=512):\n",
    "    ecg_segments = []\n",
    "    labels = []\n",
    "    for rec_num in record_numbers:\n",
    "        record = wfdb.rdrecord(f'M:\\\\Dissertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0/{rec_num}')\n",
    "        annotation = wfdb.rdann(f'M:\\\\Dissertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0/{rec_num}', 'atr')\n",
    "        \n",
    "        for i in range(len(annotation.sample)):\n",
    "            start = max(0, annotation.sample[i] - segment_length // 2)\n",
    "            end = min(len(record.p_signal), start + segment_length)\n",
    "            if end - start == segment_length:\n",
    "                ecg_segments.append(record.p_signal[start:end, 0])  # Assuming MLII lead\n",
    "                labels.append(annotation.symbol[i])\n",
    "    \n",
    "    return np.array(ecg_segments), np.array(labels)\n",
    "\n",
    "def load_noise_data():\n",
    "    em = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\em', sampfrom=0).p_signal[:, 0]\n",
    "    bw = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\bw', sampfrom=0).p_signal[:, 0]\n",
    "    ma = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\ma', sampfrom=0).p_signal[:, 0]\n",
    "    return em, bw, ma\n",
    "\n",
    "# Load ECG and noise data\n",
    "ecg_records = [103, 105, 111, 116, 122, 205, 213, 219, 223, 230]  # Add more records as needed\n",
    "ecg_segments, labels = load_ecg_data_with_labels(ecg_records)\n",
    "em_noise, bw_noise, ma_noise = load_noise_data()\n",
    "\n",
    "# Filter and map labels to integer categories\n",
    "label_mapping = {'N': 0, 'V': 1, 'A': 2, 'L': 3}  # Modify this based on the classes you want to classify\n",
    "mapped_labels = np.array([label_mapping.get(label, -1) for label in labels])\n",
    "valid_indices = mapped_labels != -1  # Filter out invalid labels\n",
    "\n",
    "# Filter data and labels to only include valid classes\n",
    "ecg_segments = ecg_segments[valid_indices]\n",
    "mapped_labels = mapped_labels[valid_indices]\n",
    "\n",
    "# Split data into 70% training, 15% validation, and 15% testing\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(ecg_segments, mapped_labels, test_size=0.15, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, random_state=42)  # 0.1765 * 85% ≈ 15%\n",
    "\n",
    "# Compute class weights to handle imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Extend and add noise to ECG segments\n",
    "def extend_noise_signal(noise_signal, target_length):\n",
    "    repeated_noise = np.tile(noise_signal, int(np.ceil(target_length / len(noise_signal))))\n",
    "    return repeated_noise[:target_length]\n",
    "\n",
    "def calculate_snr(signal, noise):\n",
    "    signal_power = np.sum(np.square(signal))\n",
    "    noise_power = np.sum(np.square(noise))\n",
    "    snr = 10 * np.log10(signal_power / noise_power)\n",
    "    return snr\n",
    "\n",
    "def calculate_rmse(signal, denoised_signal):\n",
    "    return np.sqrt(np.mean((signal - denoised_signal) ** 2))\n",
    "\n",
    "# Add noise to ECG segments using multiple noise types and combinations\n",
    "def add_combined_noise(ecg_segments, noises, target_snr_db):\n",
    "    noisy_segments = []\n",
    "    \n",
    "    # Generate noise signal by combining all noise types\n",
    "    combined_noise_signal = sum(extend_noise_signal(noise, ecg_segments.shape[1]) for noise in noises)\n",
    "    \n",
    "    for ecg_segment in ecg_segments:\n",
    "        current_snr = calculate_snr(ecg_segment, combined_noise_signal[:len(ecg_segment)])\n",
    "        scaling_factor = np.sqrt(np.sum(np.square(ecg_segment)) / (np.sum(np.square(combined_noise_signal)) * 10**(target_snr_db / 10)))\n",
    "        scaled_noise = combined_noise_signal[:len(ecg_segment)] * scaling_factor\n",
    "        noisy_segment = ecg_segment + scaled_noise\n",
    "        noisy_segments.append(noisy_segment)\n",
    "    return np.array(noisy_segments)\n",
    "\n",
    "# Denoise function\n",
    "def denoise_signal(models, noisy_signal):\n",
    "    noisy_signal = np.expand_dims(noisy_signal, axis=-1)\n",
    "    denoised_signal = ensemble_predict(models, noisy_signal)\n",
    "    return denoised_signal.squeeze()\n",
    "\n",
    "# Function to extract wavelet features\n",
    "def extract_wavelet_features(ecg_slice):\n",
    "    coeffs = pywt.wavedec(ecg_slice, 'db6', level=5)\n",
    "    return coeffs[0]  # You may want to use more features from different levels\n",
    "\n",
    "# Function to classify heartbeats using SVM\n",
    "def classify_heartbeats(features, labels):\n",
    "    clf = SVC(kernel='linear', class_weight=class_weight_dict)\n",
    "    clf.fit(features, labels)\n",
    "    return clf\n",
    "\n",
    "# Add single noises to combined noises\n",
    "noises_dict = {\n",
    "    'EM': [em_noise],\n",
    "    'BW': [bw_noise],\n",
    "    'MA': [ma_noise],\n",
    "    'EM+MA': [em_noise, ma_noise],\n",
    "    'EM+BW': [em_noise, bw_noise],\n",
    "    'MA+BW': [ma_noise, bw_noise],\n",
    "    'EM+BW+MA': [em_noise, bw_noise, ma_noise]\n",
    "}\n",
    "\n",
    "snr_db = 0  # Example SNR value\n",
    "\n",
    "# Prepare the clean ECG segments as target data for training\n",
    "clean_ecg_segments_train = X_train.copy()\n",
    "\n",
    "# Create noisy training data by combining all noise types\n",
    "noisy_ecg_slices_train = add_combined_noise(X_train, [em_noise, bw_noise, ma_noise], snr_db)\n",
    "noisy_ecg_slices_val = add_combined_noise(X_val, [em_noise, bw_noise, ma_noise], snr_db)\n",
    "\n",
    "# Instantiate the models\n",
    "models = [\n",
    "    build_1d_cnn(),\n",
    "    build_lstm(),\n",
    "    build_gru(),\n",
    "    build_denoising_autoencoder()\n",
    "]\n",
    "\n",
    "# Expand dimensions for training and validation\n",
    "noisy_ecg_slices_train_expanded = np.expand_dims(noisy_ecg_slices_train, axis=-1)  # Expand dims for the models\n",
    "noisy_ecg_slices_val_expanded = np.expand_dims(noisy_ecg_slices_val, axis=-1)  # Expand dims for validation\n",
    "clean_ecg_segments_val_expanded = np.expand_dims(X_val, axis=-1)\n",
    "\n",
    "# Train each model\n",
    "for model in models:\n",
    "    model.compile(optimizer=Adam(), loss='mse')\n",
    "    model.fit(noisy_ecg_slices_train_expanded, np.expand_dims(clean_ecg_segments_train, axis=-1), \n",
    "              epochs=5, batch_size=32, validation_data=(noisy_ecg_slices_val_expanded, clean_ecg_segments_val_expanded))\n",
    "\n",
    "# Evaluate the trained models on each noise type and combination\n",
    "results = {}\n",
    "\n",
    "for noise_name, noise_data in noises_dict.items():\n",
    "    noisy_ecg_slices_test = add_combined_noise(X_test, noise_data, snr_db)\n",
    "\n",
    "    # Denoise and classify\n",
    "    denoised_ecg_slices_test = denoise_signal(models, noisy_ecg_slices_test)\n",
    "\n",
    "    # Calculate SNR and RMSE\n",
    "    snr_values = [calculate_snr(ecg, denoised) for ecg, denoised in zip(X_test, denoised_ecg_slices_test)]\n",
    "    rmse_values = [calculate_rmse(ecg, denoised) for ecg, denoised in zip(X_test, denoised_ecg_slices_test)]\n",
    "\n",
    "    features_noisy = np.array([extract_wavelet_features(slice) for slice in noisy_ecg_slices_test])\n",
    "    features_denoised = np.array([extract_wavelet_features(slice) for slice in denoised_ecg_slices_test])\n",
    "\n",
    "    test_labels = y_test[:len(features_noisy)]\n",
    "\n",
    "    model_noisy = classify_heartbeats(features_noisy, test_labels)\n",
    "    model_denoised = classify_heartbeats(features_denoised, test_labels)\n",
    "\n",
    "    # Predict the classes\n",
    "    predictions_noisy = model_noisy.predict(features_noisy)\n",
    "    predictions_denoised = model_denoised.predict(features_denoised)\n",
    "\n",
    "    # Get the unique classes present in test_labels\n",
    "    unique_classes = np.unique(test_labels)\n",
    "    class_names = [name for i, name in enumerate(['N', 'V', 'A', 'L']) if i in unique_classes]\n",
    "\n",
    "    # Evaluate accuracy for each class\n",
    "    report_noisy = classification_report(test_labels, predictions_noisy, target_names=class_names, output_dict=True, zero_division=0)\n",
    "    report_denoised = classification_report(test_labels, predictions_denoised, target_names=class_names, output_dict=True, zero_division=0)\n",
    "\n",
    "    # Store accuracy and SNR, RMSE for each class\n",
    "    results[noise_name] = {\n",
    "        'snr': np.mean(snr_values),\n",
    "        'rmse': np.mean(rmse_values),\n",
    "        'noisy': {class_name: report_noisy[class_name]['precision'] for class_name in class_names},\n",
    "        'denoised': {class_name: report_denoised[class_name]['precision'] for class_name in class_names},\n",
    "    }\n",
    "\n",
    "# Output the results for each noise condition\n",
    "for noise_name, metrics in results.items():\n",
    "    print(f\"Noise type: {noise_name}\")\n",
    "    print(f\"Average SNR after denoising: {metrics['snr']:.4f} dB\")\n",
    "    print(f\"Average RMSE after denoising: {metrics['rmse']:.4f}\")\n",
    "    print(\"Noisy data accuracies:\")\n",
    "    for class_label, accuracy in metrics['noisy'].items():\n",
    "        print(f\"  {class_label}: {accuracy:.4f}\")\n",
    "    print(\"Denoised data accuracies:\")\n",
    "    for class_label, accuracy in metrics['denoised'].items():\n",
    "        print(f\"  {class_label}: {accuracy:.4f}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7d2a4e-c164-4e60-8394-d331a3d7437e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
