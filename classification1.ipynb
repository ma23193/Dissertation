{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f540175a-7a6b-4943-a32c-60aef35b16b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python3\\lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 169ms/step\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 198ms/step\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 204ms/step\n",
      "{'EM': {'noisy': 0.9995279679018173, 'denoised': 0.9903233419872551}, 'BW': {'noisy': 0.9995279679018173, 'denoised': 0.9903233419872551}, 'MA': {'noisy': 0.9995279679018173, 'denoised': 0.9903233419872551}}\n"
     ]
    }
   ],
   "source": [
    "import wfdb\n",
    "import numpy as np\n",
    "import pywt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, Conv1DTranspose, LeakyReLU, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load ECG data with correct labels from the MIT-BIH Arrhythmia Database\n",
    "def load_ecg_data_with_labels(record_numbers, segment_length=512):\n",
    "    ecg_segments = []\n",
    "    labels = []\n",
    "    for rec_num in record_numbers:\n",
    "        record = wfdb.rdrecord(f'C:\\\\Users\\\\malik\\\\Desktop\\\\Disertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0/{rec_num}')\n",
    "        annotation = wfdb.rdann(f'C:\\\\Users\\\\malik\\\\Desktop\\\\Disertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0/{rec_num}', 'atr')\n",
    "        \n",
    "        for i in range(len(annotation.sample)):\n",
    "            start = max(0, annotation.sample[i] - segment_length // 2)\n",
    "            end = min(len(record.p_signal), start + segment_length)\n",
    "            if end - start == segment_length:\n",
    "                ecg_segments.append(record.p_signal[start:end, 0])  # Assuming MLII lead\n",
    "                labels.append(annotation.symbol[i])\n",
    "    \n",
    "    return np.array(ecg_segments), np.array(labels)\n",
    "\n",
    "# Load noise data from the correct directory and filenames\n",
    "def load_noise_data():\n",
    "    em = wfdb.rdrecord(r'C:\\\\Users\\\\malik\\\\Desktop\\\\Disertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\em', sampfrom=0).p_signal[:, 0]\n",
    "    bw = wfdb.rdrecord(r'C:\\\\Users\\\\malik\\\\Desktop\\\\Disertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\bw', sampfrom=0).p_signal[:, 0]\n",
    "    ma = wfdb.rdrecord(r'C:\\\\Users\\\\malik\\\\Desktop\\\\Disertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\ma', sampfrom=0).p_signal[:, 0]\n",
    "    return em, bw, ma\n",
    "\n",
    "# Instantiate the generator model\n",
    "def build_generator(input_shape=(512, 1)):\n",
    "    inp = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv1D(64, 15, padding='same')(inp)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Conv1D(128, 15, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = Conv1D(256, 15, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = Conv1D(512, 15, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Conv1DTranspose(256, 15, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Conv1DTranspose(128, 15, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Conv1DTranspose(64, 15, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    out = Conv1DTranspose(1, 15, padding='same', activation='tanh')(x)\n",
    "    \n",
    "    return Model(inp, out)\n",
    "\n",
    "generator = build_generator()\n",
    "\n",
    "# Load ECG and noise data\n",
    "ecg_records = [100, 101, 102]  # Add more records as needed\n",
    "ecg_segments, labels = load_ecg_data_with_labels(ecg_records)\n",
    "em_noise, bw_noise, ma_noise = load_noise_data()\n",
    "\n",
    "# Filter and map labels to integer categories\n",
    "label_mapping = {'N': 0, 'V': 1, 'A': 2, 'L': 3}\n",
    "mapped_labels = np.array([label_mapping.get(label, -1) for label in labels])\n",
    "valid_indices = mapped_labels != -1\n",
    "\n",
    "ecg_segments = ecg_segments[valid_indices]\n",
    "mapped_labels = mapped_labels[valid_indices]\n",
    "\n",
    "# Extend and add noise to ECG segments\n",
    "def extend_noise_signal(noise_signal, target_length):\n",
    "    repeated_noise = np.tile(noise_signal, int(np.ceil(target_length / len(noise_signal))))\n",
    "    return repeated_noise[:target_length]\n",
    "\n",
    "def add_noise_to_segments(ecg_segments, noise_signal, snr_db):\n",
    "    noisy_segments = []\n",
    "    noise_signal = extend_noise_signal(noise_signal, ecg_segments.shape[1])\n",
    "    for ecg_segment in ecg_segments:\n",
    "        snr = 10 ** (snr_db / 10)\n",
    "        noise_power = np.sum(ecg_segment ** 2) / (snr * len(ecg_segment))\n",
    "        noise = np.sqrt(noise_power) * noise_signal\n",
    "        noisy_segment = ecg_segment + noise\n",
    "        noisy_segments.append(noisy_segment)\n",
    "    return np.array(noisy_segments)\n",
    "\n",
    "# Denoise function\n",
    "def denoise_signal(generator, noisy_signal):\n",
    "    noisy_signal = np.expand_dims(noisy_signal, axis=-1)\n",
    "    denoised_signal = generator.predict(noisy_signal)\n",
    "    return denoised_signal.squeeze()\n",
    "\n",
    "# Function to extract wavelet features\n",
    "def extract_wavelet_features(ecg_slice):\n",
    "    coeffs = pywt.wavedec(ecg_slice, 'db6', level=5)\n",
    "    return coeffs[0]  # You may want to use more features from different levels\n",
    "\n",
    "# Function to classify heartbeats using SVM\n",
    "def classify_heartbeats(features, labels):\n",
    "    clf = SVC(kernel='linear')\n",
    "    clf.fit(features, labels)\n",
    "    return clf\n",
    "\n",
    "# Noise types\n",
    "noises = {'EM': em_noise, 'BW': bw_noise, 'MA': ma_noise}\n",
    "snr_db = 0  # Example SNR value\n",
    "\n",
    "results = {}\n",
    "\n",
    "for noise_name, noise_data in noises.items():\n",
    "    noisy_ecg_slices = add_noise_to_segments(ecg_segments, noise_data, snr_db)\n",
    "    denoised_ecg_slices = denoise_signal(generator, noisy_ecg_slices)\n",
    "    \n",
    "    features_noisy = np.array([extract_wavelet_features(slice) for slice in noisy_ecg_slices])\n",
    "    features_denoised = np.array([extract_wavelet_features(slice) for slice in denoised_ecg_slices])\n",
    "    \n",
    "    train_labels = mapped_labels[:len(features_noisy)]\n",
    "    \n",
    "    model_noisy = classify_heartbeats(features_noisy, train_labels)\n",
    "    model_denoised = classify_heartbeats(features_denoised, train_labels)\n",
    "    \n",
    "    accuracy_noisy = accuracy_score(train_labels, model_noisy.predict(features_noisy))\n",
    "    accuracy_denoised = accuracy_score(train_labels, model_denoised.predict(features_denoised))\n",
    "    \n",
    "    results[noise_name] = {'noisy': accuracy_noisy, 'denoised': accuracy_denoised}\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b94d58e8-b76a-4d97-a047-f60a4ad470c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python3\\lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 169ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Python3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Python3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 200ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Python3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Python3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 204ms/step\n",
      "Noise type: EM\n",
      "Noisy data accuracies:\n",
      "  N: 0.9995\n",
      "  V: 1.0000\n",
      "  A: 1.0000\n",
      "Denoised data accuracies:\n",
      "  N: 0.9903\n",
      "  V: 0.0000\n",
      "  A: 0.0000\n",
      "\n",
      "\n",
      "Noise type: BW\n",
      "Noisy data accuracies:\n",
      "  N: 0.9995\n",
      "  V: 1.0000\n",
      "  A: 1.0000\n",
      "Denoised data accuracies:\n",
      "  N: 0.9903\n",
      "  V: 0.0000\n",
      "  A: 0.0000\n",
      "\n",
      "\n",
      "Noise type: MA\n",
      "Noisy data accuracies:\n",
      "  N: 0.9995\n",
      "  V: 1.0000\n",
      "  A: 1.0000\n",
      "Denoised data accuracies:\n",
      "  N: 0.9903\n",
      "  V: 0.0000\n",
      "  A: 0.0000\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Python3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Python3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import wfdb\n",
    "import numpy as np\n",
    "import pywt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, Conv1DTranspose, LeakyReLU, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load ECG data with correct labels from the MIT-BIH Arrhythmia Database\n",
    "def load_ecg_data_with_labels(record_numbers, segment_length=512):\n",
    "    ecg_segments = []\n",
    "    labels = []\n",
    "    for rec_num in record_numbers:\n",
    "        record = wfdb.rdrecord(f'M:\\\\Dissertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0/{rec_num}')\n",
    "        annotation = wfdb.rdann(f'M:\\\\Dissertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0/{rec_num}', 'atr')\n",
    "        \n",
    "        for i in range(len(annotation.sample)):\n",
    "            start = max(0, annotation.sample[i] - segment_length // 2)\n",
    "            end = min(len(record.p_signal), start + segment_length)\n",
    "            if end - start == segment_length:\n",
    "                ecg_segments.append(record.p_signal[start:end, 0])  # Assuming MLII lead\n",
    "                labels.append(annotation.symbol[i])\n",
    "    \n",
    "    return np.array(ecg_segments), np.array(labels)\n",
    "\n",
    "# Load noise data from the correct directory and filenames\n",
    "def load_noise_data():\n",
    "    em = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\em', sampfrom=0).p_signal[:, 0]\n",
    "    bw = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\bw', sampfrom=0).p_signal[:, 0]\n",
    "    ma = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\ma', sampfrom=0).p_signal[:, 0]\n",
    "    return em, bw, ma\n",
    "\n",
    "# Instantiate the generator model\n",
    "def build_generator(input_shape=(512, 1)):\n",
    "    inp = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv1D(64, 15, padding='same')(inp)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Conv1D(128, 15, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = Conv1D(256, 15, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = Conv1D(512, 15, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Conv1DTranspose(256, 15, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Conv1DTranspose(128, 15, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Conv1DTranspose(64, 15, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    out = Conv1DTranspose(1, 15, padding='same', activation='tanh')(x)\n",
    "    \n",
    "    return Model(inp, out)\n",
    "\n",
    "generator = build_generator()\n",
    "\n",
    "# Load ECG and noise data\n",
    "ecg_records = [100, 101, 102]  # Add more records as needed\n",
    "ecg_segments, labels = load_ecg_data_with_labels(ecg_records)\n",
    "em_noise, bw_noise, ma_noise = load_noise_data()\n",
    "\n",
    "# Filter and map labels to integer categories\n",
    "label_mapping = {'N': 0, 'V': 1, 'A': 2, 'L': 3}\n",
    "mapped_labels = np.array([label_mapping.get(label, -1) for label in labels])\n",
    "valid_indices = mapped_labels != -1\n",
    "\n",
    "ecg_segments = ecg_segments[valid_indices]\n",
    "mapped_labels = mapped_labels[valid_indices]\n",
    "\n",
    "# Extend and add noise to ECG segments\n",
    "def extend_noise_signal(noise_signal, target_length):\n",
    "    repeated_noise = np.tile(noise_signal, int(np.ceil(target_length / len(noise_signal))))\n",
    "    return repeated_noise[:target_length]\n",
    "\n",
    "def add_noise_to_segments(ecg_segments, noise_signal, snr_db):\n",
    "    noisy_segments = []\n",
    "    noise_signal = extend_noise_signal(noise_signal, ecg_segments.shape[1])\n",
    "    for ecg_segment in ecg_segments:\n",
    "        snr = 10 ** (snr_db / 10)\n",
    "        noise_power = np.sum(ecg_segment ** 2) / (snr * len(ecg_segment))\n",
    "        noise = np.sqrt(noise_power) * noise_signal\n",
    "        noisy_segment = ecg_segment + noise\n",
    "        noisy_segments.append(noisy_segment)\n",
    "    return np.array(noisy_segments)\n",
    "\n",
    "# Denoise function\n",
    "def denoise_signal(generator, noisy_signal):\n",
    "    noisy_signal = np.expand_dims(noisy_signal, axis=-1)\n",
    "    denoised_signal = generator.predict(noisy_signal)\n",
    "    return denoised_signal.squeeze()\n",
    "\n",
    "# Function to extract wavelet features\n",
    "def extract_wavelet_features(ecg_slice):\n",
    "    coeffs = pywt.wavedec(ecg_slice, 'db6', level=5)\n",
    "    return coeffs[0]  # You may want to use more features from different levels\n",
    "\n",
    "# Function to classify heartbeats using SVM\n",
    "def classify_heartbeats(features, labels):\n",
    "    clf = SVC(kernel='linear')\n",
    "    clf.fit(features, labels)\n",
    "    return clf\n",
    "\n",
    "# Noise types\n",
    "noises = {'EM': em_noise, 'BW': bw_noise, 'MA': ma_noise}\n",
    "snr_db = 0  # Example SNR value\n",
    "\n",
    "results = {}\n",
    "\n",
    "for noise_name, noise_data in noises.items():\n",
    "    noisy_ecg_slices = add_noise_to_segments(ecg_segments, noise_data, snr_db)\n",
    "    denoised_ecg_slices = denoise_signal(generator, noisy_ecg_slices)\n",
    "    \n",
    "    features_noisy = np.array([extract_wavelet_features(slice) for slice in noisy_ecg_slices])\n",
    "    features_denoised = np.array([extract_wavelet_features(slice) for slice in denoised_ecg_slices])\n",
    "    \n",
    "    train_labels = mapped_labels[:len(features_noisy)]\n",
    "    \n",
    "    model_noisy = classify_heartbeats(features_noisy, train_labels)\n",
    "    model_denoised = classify_heartbeats(features_denoised, train_labels)\n",
    "    \n",
    "    predictions_noisy = model_noisy.predict(features_noisy)\n",
    "    predictions_denoised = model_denoised.predict(features_denoised)\n",
    "    \n",
    "    # Get the unique classes present in train_labels\n",
    "    unique_classes = np.unique(train_labels)\n",
    "    class_names = [name for i, name in enumerate(['N', 'V', 'A', 'L']) if i in unique_classes]\n",
    "    \n",
    "    # Evaluate accuracy for each class\n",
    "    report_noisy = classification_report(train_labels, predictions_noisy, target_names=class_names, output_dict=True)\n",
    "    report_denoised = classification_report(train_labels, predictions_denoised, target_names=class_names, output_dict=True)\n",
    "    \n",
    "    # Store accuracy for each class\n",
    "    results[noise_name] = {\n",
    "        'noisy': {class_name: report_noisy[class_name]['precision'] for class_name in class_names},\n",
    "        'denoised': {class_name: report_denoised[class_name]['precision'] for class_name in class_names}\n",
    "    }\n",
    "\n",
    "# Output the results for each class and noise condition\n",
    "for noise_name, metrics in results.items():\n",
    "    print(f\"Noise type: {noise_name}\")\n",
    "    print(\"Noisy data accuracies:\")\n",
    "    for class_label, accuracy in metrics['noisy'].items():\n",
    "        print(f\"  {class_label}: {accuracy:.4f}\")\n",
    "    print(\"Denoised data accuracies:\")\n",
    "    for class_label, accuracy in metrics['denoised'].items():\n",
    "        print(f\"  {class_label}: {accuracy:.4f}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e2961fc-d8c4-40cd-bbbb-5469d56705a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python3\\lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m756/756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 212ms/step\n",
      "\u001b[1m756/756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 233ms/step\n",
      "\u001b[1m756/756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 238ms/step\n",
      "Noise type: EM\n",
      "Noisy data accuracies:\n",
      "  N: 0.9967\n",
      "  V: 0.7387\n",
      "  A: 0.0693\n",
      "  L: 0.9083\n",
      "Denoised data accuracies:\n",
      "  N: 0.9942\n",
      "  V: 0.7437\n",
      "  A: 0.0500\n",
      "  L: 0.9081\n",
      "\n",
      "\n",
      "Noise type: BW\n",
      "Noisy data accuracies:\n",
      "  N: 0.9974\n",
      "  V: 0.7555\n",
      "  A: 0.0691\n",
      "  L: 0.9243\n",
      "Denoised data accuracies:\n",
      "  N: 0.9947\n",
      "  V: 0.6510\n",
      "  A: 0.0557\n",
      "  L: 0.9286\n",
      "\n",
      "\n",
      "Noise type: MA\n",
      "Noisy data accuracies:\n",
      "  N: 0.9974\n",
      "  V: 0.7559\n",
      "  A: 0.0679\n",
      "  L: 0.9119\n",
      "Denoised data accuracies:\n",
      "  N: 0.9937\n",
      "  V: 0.5963\n",
      "  A: 0.0576\n",
      "  L: 0.8997\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import wfdb\n",
    "import numpy as np\n",
    "import pywt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, Conv1DTranspose, LeakyReLU, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load ECG data with correct labels from the MIT-BIH Arrhythmia Database\n",
    "def load_ecg_data_with_labels(record_numbers, segment_length=512):\n",
    "    ecg_segments = []\n",
    "    labels = []\n",
    "    for rec_num in record_numbers:\n",
    "        record = wfdb.rdrecord(f'M:\\\\Dissertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0/{rec_num}')\n",
    "        annotation = wfdb.rdann(f'M:\\\\Dissertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0/{rec_num}', 'atr')\n",
    "        \n",
    "        for i in range(len(annotation.sample)):\n",
    "            start = max(0, annotation.sample[i] - segment_length // 2)\n",
    "            end = min(len(record.p_signal), start + segment_length)\n",
    "            if end - start == segment_length:\n",
    "                ecg_segments.append(record.p_signal[start:end, 0])  # Assuming MLII lead\n",
    "                labels.append(annotation.symbol[i])\n",
    "    \n",
    "    return np.array(ecg_segments), np.array(labels)\n",
    "\n",
    "# Load noise data from the correct directory and filenames\n",
    "def load_noise_data():\n",
    "    em = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\em', sampfrom=0).p_signal[:, 0]\n",
    "    bw = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\bw', sampfrom=0).p_signal[:, 0]\n",
    "    ma = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\ma', sampfrom=0).p_signal[:, 0]\n",
    "    return em, bw, ma\n",
    "\n",
    "# Instantiate the generator model\n",
    "def build_generator(input_shape=(512, 1)):\n",
    "    inp = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv1D(64, 15, padding='same')(inp)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Conv1D(128, 15, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = Conv1D(256, 15, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = Conv1D(512, 15, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Conv1DTranspose(256, 15, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Conv1DTranspose(128, 15, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Conv1DTranspose(64, 15, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    out = Conv1DTranspose(1, 15, padding='same', activation='tanh')(x)\n",
    "    \n",
    "    return Model(inp, out)\n",
    "\n",
    "generator = build_generator()\n",
    "\n",
    "# Load ECG and noise data\n",
    "ecg_records = [103, 105, 111, 116, 122, 205, 213, 219, 223, 230]  # Add more records as needed\n",
    "ecg_segments, labels = load_ecg_data_with_labels(ecg_records)\n",
    "em_noise, bw_noise, ma_noise = load_noise_data()\n",
    "\n",
    "# Filter and map labels to integer categories\n",
    "label_mapping = {'N': 0, 'V': 1, 'A': 2, 'L': 3}\n",
    "mapped_labels = np.array([label_mapping.get(label, -1) for label in labels])\n",
    "valid_indices = mapped_labels != -1\n",
    "\n",
    "ecg_segments = ecg_segments[valid_indices]\n",
    "mapped_labels = mapped_labels[valid_indices]\n",
    "\n",
    "# Compute class weights to handle imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(mapped_labels), y=mapped_labels)\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Extend and add noise to ECG segments\n",
    "def extend_noise_signal(noise_signal, target_length):\n",
    "    repeated_noise = np.tile(noise_signal, int(np.ceil(target_length / len(noise_signal))))\n",
    "    return repeated_noise[:target_length]\n",
    "\n",
    "def add_noise_to_segments(ecg_segments, noise_signal, snr_db):\n",
    "    noisy_segments = []\n",
    "    noise_signal = extend_noise_signal(noise_signal, ecg_segments.shape[1])\n",
    "    for ecg_segment in ecg_segments:\n",
    "        snr = 10 ** (snr_db / 10)\n",
    "        noise_power = np.sum(ecg_segment ** 2) / (snr * len(ecg_segment))\n",
    "        noise = np.sqrt(noise_power) * noise_signal\n",
    "        noisy_segment = ecg_segment + noise\n",
    "        noisy_segments.append(noisy_segment)\n",
    "    return np.array(noisy_segments)\n",
    "\n",
    "# Denoise function\n",
    "def denoise_signal(generator, noisy_signal):\n",
    "    noisy_signal = np.expand_dims(noisy_signal, axis=-1)\n",
    "    denoised_signal = generator.predict(noisy_signal)\n",
    "    return denoised_signal.squeeze()\n",
    "\n",
    "# Function to extract wavelet features\n",
    "def extract_wavelet_features(ecg_slice):\n",
    "    coeffs = pywt.wavedec(ecg_slice, 'db6', level=5)\n",
    "    return coeffs[0]  # You may want to use more features from different levels\n",
    "\n",
    "# Function to classify heartbeats using SVM\n",
    "def classify_heartbeats(features, labels):\n",
    "    clf = SVC(kernel='linear', class_weight=class_weight_dict)\n",
    "    clf.fit(features, labels)\n",
    "    return clf\n",
    "\n",
    "# Noise types\n",
    "noises = {'EM': em_noise, 'BW': bw_noise, 'MA': ma_noise}\n",
    "snr_db = 0  # Example SNR value\n",
    "\n",
    "results = {}\n",
    "\n",
    "for noise_name, noise_data in noises.items():\n",
    "    noisy_ecg_slices = add_noise_to_segments(ecg_segments, noise_data, snr_db)\n",
    "    denoised_ecg_slices = denoise_signal(generator, noisy_ecg_slices)\n",
    "    \n",
    "    features_noisy = np.array([extract_wavelet_features(slice) for slice in noisy_ecg_slices])\n",
    "    features_denoised = np.array([extract_wavelet_features(slice) for slice in denoised_ecg_slices])\n",
    "    \n",
    "    train_labels = mapped_labels[:len(features_noisy)]\n",
    "    \n",
    "    model_noisy = classify_heartbeats(features_noisy, train_labels)\n",
    "    model_denoised = classify_heartbeats(features_denoised, train_labels)\n",
    "    \n",
    "    predictions_noisy = model_noisy.predict(features_noisy)\n",
    "    predictions_denoised = model_denoised.predict(features_denoised)\n",
    "    \n",
    "    # Get the unique classes present in train_labels\n",
    "    unique_classes = np.unique(train_labels)\n",
    "    class_names = [name for i, name in enumerate(['N', 'V', 'A', 'L']) if i in unique_classes]\n",
    "    \n",
    "    # Evaluate accuracy for each class\n",
    "    report_noisy = classification_report(train_labels, predictions_noisy, target_names=class_names, output_dict=True, zero_division=0)\n",
    "    report_denoised = classification_report(train_labels, predictions_denoised, target_names=class_names, output_dict=True, zero_division=0)\n",
    "    \n",
    "    # Store accuracy for each class\n",
    "    results[noise_name] = {\n",
    "        'noisy': {class_name: report_noisy[class_name]['precision'] for class_name in class_names},\n",
    "        'denoised': {class_name: report_denoised[class_name]['precision'] for class_name in class_names}\n",
    "    }\n",
    "\n",
    "# Output the results for each class and noise condition\n",
    "for noise_name, metrics in results.items():\n",
    "    print(f\"Noise type: {noise_name}\")\n",
    "    print(\"Noisy data accuracies:\")\n",
    "    for class_label, accuracy in metrics['noisy'].items():\n",
    "        print(f\"  {class_label}: {accuracy:.4f}\")\n",
    "    print(\"Denoised data accuracies:\")\n",
    "    for class_label, accuracy in metrics['denoised'].items():\n",
    "        print(f\"  {class_label}: {accuracy:.4f}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79308fe6-d93e-4f8f-b32f-82b74fda4c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python3\\lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m756/756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 230ms/step\n",
      "\u001b[1m756/756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 235ms/step\n",
      "\u001b[1m756/756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 237ms/step\n",
      "Noise type: EM\n",
      "Noisy data accuracies:\n",
      "  N: 0.9966\n",
      "  V: 0.7190\n",
      "  A: 0.0709\n",
      "  L: 0.9242\n",
      "Denoised data accuracies:\n",
      "  N: 0.9895\n",
      "  V: 0.6810\n",
      "  A: 0.0781\n",
      "  L: 0.8215\n",
      "\n",
      "\n",
      "Noise type: BW\n",
      "Noisy data accuracies:\n",
      "  N: 0.9973\n",
      "  V: 0.7502\n",
      "  A: 0.0668\n",
      "  L: 0.9297\n",
      "Denoised data accuracies:\n",
      "  N: 0.9852\n",
      "  V: 0.4456\n",
      "  A: 0.0822\n",
      "  L: 0.7231\n",
      "\n",
      "\n",
      "Noise type: MA\n",
      "Noisy data accuracies:\n",
      "  N: 0.9968\n",
      "  V: 0.7400\n",
      "  A: 0.0688\n",
      "  L: 0.9451\n",
      "Denoised data accuracies:\n",
      "  N: 0.9858\n",
      "  V: 0.7538\n",
      "  A: 0.0725\n",
      "  L: 0.3842\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import wfdb\n",
    "import numpy as np\n",
    "import pywt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, Conv1DTranspose, LeakyReLU, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load ECG data with correct labels from the MIT-BIH Arrhythmia Database\n",
    "def load_ecg_data_with_labels(record_numbers, segment_length=512):\n",
    "    ecg_segments = []\n",
    "    labels = []\n",
    "    for rec_num in record_numbers:\n",
    "        record = wfdb.rdrecord(f'M:\\\\Dissertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0/{rec_num}')\n",
    "        annotation = wfdb.rdann(f'M:\\\\Dissertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0/{rec_num}', 'atr')\n",
    "        \n",
    "        for i in range(len(annotation.sample)):\n",
    "            start = max(0, annotation.sample[i] - segment_length // 2)\n",
    "            end = min(len(record.p_signal), start + segment_length)\n",
    "            if end - start == segment_length:\n",
    "                ecg_segments.append(record.p_signal[start:end, 0])  # Assuming MLII lead\n",
    "                labels.append(annotation.symbol[i])\n",
    "    \n",
    "    return np.array(ecg_segments), np.array(labels)\n",
    "\n",
    "# Load noise data from the correct directory and filenames\n",
    "def load_noise_data():\n",
    "    em = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\em', sampfrom=0).p_signal[:, 0]\n",
    "    bw = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\bw', sampfrom=0).p_signal[:, 0]\n",
    "    ma = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\ma', sampfrom=0).p_signal[:, 0]\n",
    "    return em, bw, ma\n",
    "\n",
    "# Instantiate the generator model\n",
    "def build_generator(input_shape=(512, 1)):\n",
    "    inp = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv1D(64, 15, padding='same')(inp)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Conv1D(128, 15, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = Conv1D(256, 15, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = Conv1D(512, 15, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Conv1DTranspose(256, 15, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Conv1DTranspose(128, 15, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Conv1DTranspose(64, 15, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    out = Conv1DTranspose(1, 15, padding='same', activation='tanh')(x)\n",
    "    \n",
    "    return Model(inp, out)\n",
    "\n",
    "generator = build_generator()\n",
    "\n",
    "# Load ECG and noise data\n",
    "ecg_records = [103, 105, 111, 116, 122, 205, 213, 219, 223, 230]  # Add more records as needed\n",
    "ecg_segments, labels = load_ecg_data_with_labels(ecg_records)\n",
    "em_noise, bw_noise, ma_noise = load_noise_data()\n",
    "\n",
    "# Filter and map labels to integer categories\n",
    "label_mapping = {'N': 0, 'V': 1, 'A': 2, 'L': 3}\n",
    "mapped_labels = np.array([label_mapping.get(label, -1) for label in labels])\n",
    "valid_indices = mapped_labels != -1\n",
    "\n",
    "ecg_segments = ecg_segments[valid_indices]\n",
    "mapped_labels = mapped_labels[valid_indices]\n",
    "\n",
    "# Compute class weights to handle imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(mapped_labels), y=mapped_labels)\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Extend and add noise to ECG segments\n",
    "def extend_noise_signal(noise_signal, target_length):\n",
    "    repeated_noise = np.tile(noise_signal, int(np.ceil(target_length / len(noise_signal))))\n",
    "    return repeated_noise[:target_length]\n",
    "\n",
    "def calculate_snr(signal, noise):\n",
    "    signal_power = np.sum(np.square(signal))\n",
    "    noise_power = np.sum(np.square(noise))\n",
    "    snr = 10 * np.log10(signal_power / noise_power)\n",
    "    return snr\n",
    "\n",
    "# Extend and add noise to ECG segments using the SNR formula\n",
    "def add_noise_to_segments(ecg_segments, noise_signal, target_snr_db):\n",
    "    noisy_segments = []\n",
    "    noise_signal = extend_noise_signal(noise_signal, ecg_segments.shape[1])\n",
    "    \n",
    "    for ecg_segment in ecg_segments:\n",
    "        current_snr = calculate_snr(ecg_segment, noise_signal[:len(ecg_segment)])\n",
    "        scaling_factor = np.sqrt(np.sum(np.square(ecg_segment)) / (np.sum(np.square(noise_signal)) * 10**(target_snr_db / 10)))\n",
    "        scaled_noise = noise_signal[:len(ecg_segment)] * scaling_factor\n",
    "        noisy_segment = ecg_segment + scaled_noise\n",
    "        noisy_segments.append(noisy_segment)\n",
    "    return np.array(noisy_segments)\n",
    "\n",
    "# Denoise function\n",
    "def denoise_signal(generator, noisy_signal):\n",
    "    noisy_signal = np.expand_dims(noisy_signal, axis=-1)\n",
    "    denoised_signal = generator.predict(noisy_signal)\n",
    "    return denoised_signal.squeeze()\n",
    "\n",
    "# Function to extract wavelet features\n",
    "def extract_wavelet_features(ecg_slice):\n",
    "    coeffs = pywt.wavedec(ecg_slice, 'db6', level=5)\n",
    "    return coeffs[0]  # You may want to use more features from different levels\n",
    "\n",
    "# Function to classify heartbeats using SVM\n",
    "def classify_heartbeats(features, labels):\n",
    "    clf = SVC(kernel='linear', class_weight=class_weight_dict)\n",
    "    clf.fit(features, labels)\n",
    "    return clf\n",
    "\n",
    "# Noise types\n",
    "noises = {'EM': em_noise, 'BW': bw_noise, 'MA': ma_noise}\n",
    "snr_db = 0  # Example SNR value\n",
    "\n",
    "results = {}\n",
    "\n",
    "for noise_name, noise_data in noises.items():\n",
    "    noisy_ecg_slices = add_noise_to_segments(ecg_segments, noise_data, snr_db)\n",
    "    denoised_ecg_slices = denoise_signal(generator, noisy_ecg_slices)\n",
    "    \n",
    "    features_noisy = np.array([extract_wavelet_features(slice) for slice in noisy_ecg_slices])\n",
    "    features_denoised = np.array([extract_wavelet_features(slice) for slice in denoised_ecg_slices])\n",
    "    \n",
    "    train_labels = mapped_labels[:len(features_noisy)]\n",
    "    \n",
    "    model_noisy = classify_heartbeats(features_noisy, train_labels)\n",
    "    model_denoised = classify_heartbeats(features_denoised, train_labels)\n",
    "    \n",
    "    predictions_noisy = model_noisy.predict(features_noisy)\n",
    "    predictions_denoised = model_denoised.predict(features_denoised)\n",
    "    \n",
    "    # Get the unique classes present in train_labels\n",
    "    unique_classes = np.unique(train_labels)\n",
    "    class_names = [name for i, name in enumerate(['N', 'V', 'A', 'L']) if i in unique_classes]\n",
    "    \n",
    "    # Evaluate accuracy for each class\n",
    "    report_noisy = classification_report(train_labels, predictions_noisy, target_names=class_names, output_dict=True, zero_division=0)\n",
    "    report_denoised = classification_report(train_labels, predictions_denoised, target_names=class_names, output_dict=True, zero_division=0)\n",
    "    \n",
    "    # Store accuracy for each class\n",
    "    results[noise_name] = {\n",
    "        'noisy': {class_name: report_noisy[class_name]['precision'] for class_name in class_names},\n",
    "        'denoised': {class_name: report_denoised[class_name]['precision'] for class_name in class_names}\n",
    "    }\n",
    "\n",
    "# Output the results for each class and noise condition\n",
    "for noise_name, metrics in results.items():\n",
    "    print(f\"Noise type: {noise_name}\")\n",
    "    print(\"Noisy data accuracies:\")\n",
    "    for class_label, accuracy in metrics['noisy'].items():\n",
    "        print(f\"  {class_label}: {accuracy:.4f}\")\n",
    "    print(\"Denoised data accuracies:\")\n",
    "    for class_label, accuracy in metrics['denoised'].items():\n",
    "        print(f\"  {class_label}: {accuracy:.4f}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "706761a0-eaff-4fbe-be18-be8b73884b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python3\\lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m756/756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 213ms/step\n",
      "\u001b[1m756/756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 229ms/step\n",
      "\u001b[1m756/756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 230ms/step\n",
      "\u001b[1m756/756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 230ms/step\n",
      "\u001b[1m756/756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 229ms/step\n",
      "\u001b[1m756/756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 229ms/step\n",
      "\u001b[1m756/756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 229ms/step\n",
      "Noise type: EM\n",
      "Noisy data accuracies:\n",
      "  N: 0.9966\n",
      "  V: 0.7190\n",
      "  A: 0.0709\n",
      "  L: 0.9242\n",
      "Denoised data accuracies:\n",
      "  N: 0.9876\n",
      "  V: 0.6036\n",
      "  A: 0.0226\n",
      "  L: 0.3090\n",
      "\n",
      "\n",
      "Noise type: BW\n",
      "Noisy data accuracies:\n",
      "  N: 0.9973\n",
      "  V: 0.7502\n",
      "  A: 0.0668\n",
      "  L: 0.9297\n",
      "Denoised data accuracies:\n",
      "  N: 0.9873\n",
      "  V: 0.6690\n",
      "  A: 0.0301\n",
      "  L: 0.3627\n",
      "\n",
      "\n",
      "Noise type: MA\n",
      "Noisy data accuracies:\n",
      "  N: 0.9968\n",
      "  V: 0.7400\n",
      "  A: 0.0688\n",
      "  L: 0.9451\n",
      "Denoised data accuracies:\n",
      "  N: 0.9829\n",
      "  V: 0.4419\n",
      "  A: 0.0123\n",
      "  L: 0.2920\n",
      "\n",
      "\n",
      "Noise type: EM+MA\n",
      "Noisy data accuracies:\n",
      "  N: 0.9966\n",
      "  V: 0.7254\n",
      "  A: 0.0697\n",
      "  L: 0.9238\n",
      "Denoised data accuracies:\n",
      "  N: 0.9869\n",
      "  V: 0.5878\n",
      "  A: 0.0226\n",
      "  L: 0.3044\n",
      "\n",
      "\n",
      "Noise type: EM+BW\n",
      "Noisy data accuracies:\n",
      "  N: 0.9967\n",
      "  V: 0.7432\n",
      "  A: 0.0670\n",
      "  L: 0.9143\n",
      "Denoised data accuracies:\n",
      "  N: 0.9849\n",
      "  V: 0.7048\n",
      "  A: 0.0221\n",
      "  L: 0.3508\n",
      "\n",
      "\n",
      "Noise type: MA+BW\n",
      "Noisy data accuracies:\n",
      "  N: 0.9974\n",
      "  V: 0.7514\n",
      "  A: 0.0678\n",
      "  L: 0.9284\n",
      "Denoised data accuracies:\n",
      "  N: 0.9867\n",
      "  V: 0.6541\n",
      "  A: 0.0291\n",
      "  L: 0.3590\n",
      "\n",
      "\n",
      "Noise type: EM+BW+MA\n",
      "Noisy data accuracies:\n",
      "  N: 0.9968\n",
      "  V: 0.7403\n",
      "  A: 0.0683\n",
      "  L: 0.9152\n",
      "Denoised data accuracies:\n",
      "  N: 0.9843\n",
      "  V: 0.6911\n",
      "  A: 0.0208\n",
      "  L: 0.3498\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import wfdb\n",
    "import numpy as np\n",
    "import pywt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, Conv1DTranspose, LeakyReLU, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load ECG data with correct labels from the MIT-BIH Arrhythmia Database\n",
    "def load_ecg_data_with_labels(record_numbers, segment_length=512):\n",
    "    ecg_segments = []\n",
    "    labels = []\n",
    "    for rec_num in record_numbers:\n",
    "        record = wfdb.rdrecord(f'M:\\\\Dissertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0/{rec_num}')\n",
    "        annotation = wfdb.rdann(f'M:\\\\Dissertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0/{rec_num}', 'atr')\n",
    "        \n",
    "        for i in range(len(annotation.sample)):\n",
    "            start = max(0, annotation.sample[i] - segment_length // 2)\n",
    "            end = min(len(record.p_signal), start + segment_length)\n",
    "            if end - start == segment_length:\n",
    "                ecg_segments.append(record.p_signal[start:end, 0])  # Assuming MLII lead\n",
    "                labels.append(annotation.symbol[i])\n",
    "    \n",
    "    return np.array(ecg_segments), np.array(labels)\n",
    "\n",
    "# Load noise data from the correct directory and filenames\n",
    "def load_noise_data():\n",
    "    em = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\em', sampfrom=0).p_signal[:, 0]\n",
    "    bw = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\bw', sampfrom=0).p_signal[:, 0]\n",
    "    ma = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\ma', sampfrom=0).p_signal[:, 0]\n",
    "    return em, bw, ma\n",
    "\n",
    "# Instantiate the generator model\n",
    "def build_generator(input_shape=(512, 1)):\n",
    "    inp = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv1D(64, 15, padding='same')(inp)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Conv1D(128, 15, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = Conv1D(256, 15, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = Conv1D(512, 15, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Conv1DTranspose(256, 15, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Conv1DTranspose(128, 15, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Conv1DTranspose(64, 15, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    out = Conv1DTranspose(1, 15, padding='same', activation='tanh')(x)\n",
    "    \n",
    "    return Model(inp, out)\n",
    "\n",
    "generator = build_generator()\n",
    "\n",
    "# Load ECG and noise data\n",
    "ecg_records = [103, 105, 111, 116, 122, 205, 213, 219, 223, 230]  # Add more records as needed\n",
    "ecg_segments, labels = load_ecg_data_with_labels(ecg_records)\n",
    "em_noise, bw_noise, ma_noise = load_noise_data()\n",
    "\n",
    "# Filter and map labels to integer categories\n",
    "label_mapping = {'N': 0, 'V': 1, 'A': 2, 'L': 3}\n",
    "mapped_labels = np.array([label_mapping.get(label, -1) for label in labels])\n",
    "valid_indices = mapped_labels != -1\n",
    "\n",
    "ecg_segments = ecg_segments[valid_indices]\n",
    "mapped_labels = mapped_labels[valid_indices]\n",
    "\n",
    "# Compute class weights to handle imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(mapped_labels), y=mapped_labels)\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Extend and add noise to ECG segments\n",
    "def extend_noise_signal(noise_signal, target_length):\n",
    "    repeated_noise = np.tile(noise_signal, int(np.ceil(target_length / len(noise_signal))))\n",
    "    return repeated_noise[:target_length]\n",
    "\n",
    "def calculate_snr(signal, noise):\n",
    "    signal_power = np.sum(np.square(signal))\n",
    "    noise_power = np.sum(np.square(noise))\n",
    "    snr = 10 * np.log10(signal_power / noise_power)\n",
    "    return snr\n",
    "\n",
    "# Extend and add noise to ECG segments using the SNR formula\n",
    "def add_noise_to_segments(ecg_segments, noise_signal, target_snr_db):\n",
    "    noisy_segments = []\n",
    "    noise_signal = extend_noise_signal(noise_signal, ecg_segments.shape[1])\n",
    "    \n",
    "    for ecg_segment in ecg_segments:\n",
    "        current_snr = calculate_snr(ecg_segment, noise_signal[:len(ecg_segment)])\n",
    "        scaling_factor = np.sqrt(np.sum(np.square(ecg_segment)) / (np.sum(np.square(noise_signal)) * 10**(target_snr_db / 10)))\n",
    "        scaled_noise = noise_signal[:len(ecg_segment)] * scaling_factor\n",
    "        noisy_segment = ecg_segment + scaled_noise\n",
    "        noisy_segments.append(noisy_segment)\n",
    "    return np.array(noisy_segments)\n",
    "\n",
    "# Denoise function\n",
    "def denoise_signal(generator, noisy_signal):\n",
    "    noisy_signal = np.expand_dims(noisy_signal, axis=-1)\n",
    "    denoised_signal = generator.predict(noisy_signal)\n",
    "    return denoised_signal.squeeze()\n",
    "\n",
    "# Function to extract wavelet features\n",
    "def extract_wavelet_features(ecg_slice):\n",
    "    coeffs = pywt.wavedec(ecg_slice, 'db6', level=5)\n",
    "    return coeffs[0]  # You may want to use more features from different levels\n",
    "\n",
    "# Function to classify heartbeats using SVM\n",
    "def classify_heartbeats(features, labels):\n",
    "    clf = SVC(kernel='linear', class_weight=class_weight_dict)\n",
    "    clf.fit(features, labels)\n",
    "    return clf\n",
    "\n",
    "# Create combined noise types\n",
    "combined_noises = {\n",
    "    'EM+MA': em_noise + ma_noise,\n",
    "    'EM+BW': em_noise + bw_noise,\n",
    "    'MA+BW': ma_noise + bw_noise,\n",
    "    'EM+BW+MA': em_noise + bw_noise + ma_noise\n",
    "}\n",
    "\n",
    "# Add single noises to combined noises\n",
    "noises = {\n",
    "    'EM': em_noise,\n",
    "    'BW': bw_noise,\n",
    "    'MA': ma_noise\n",
    "}\n",
    "noises.update(combined_noises)\n",
    "\n",
    "snr_db = 0  # Example SNR value\n",
    "\n",
    "results = {}\n",
    "\n",
    "for noise_name, noise_data in noises.items():\n",
    "    noisy_ecg_slices = add_noise_to_segments(ecg_segments, noise_data, snr_db)\n",
    "    denoised_ecg_slices = denoise_signal(generator, noisy_ecg_slices)\n",
    "    \n",
    "    features_noisy = np.array([extract_wavelet_features(slice) for slice in noisy_ecg_slices])\n",
    "    features_denoised = np.array([extract_wavelet_features(slice) for slice in denoised_ecg_slices])\n",
    "    \n",
    "    train_labels = mapped_labels[:len(features_noisy)]\n",
    "    \n",
    "    model_noisy = classify_heartbeats(features_noisy, train_labels)\n",
    "    model_denoised = classify_heartbeats(features_denoised, train_labels)\n",
    "    \n",
    "    predictions_noisy = model_noisy.predict(features_noisy)\n",
    "    predictions_denoised = model_denoised.predict(features_denoised)\n",
    "    \n",
    "    # Get the unique classes present in train_labels\n",
    "    unique_classes = np.unique(train_labels)\n",
    "    class_names = [name for i, name in enumerate(['N', 'V', 'A', 'L']) if i in unique_classes]\n",
    "    \n",
    "    # Evaluate accuracy for each class\n",
    "    report_noisy = classification_report(train_labels, predictions_noisy, target_names=class_names, output_dict=True, zero_division=0)\n",
    "    report_denoised = classification_report(train_labels, predictions_denoised, target_names=class_names, output_dict=True, zero_division=0)\n",
    "    \n",
    "    # Store accuracy for each class\n",
    "    results[noise_name] = {\n",
    "        'noisy': {class_name: report_noisy[class_name]['precision'] for class_name in class_names},\n",
    "        'denoised': {class_name: report_denoised[class_name]['precision'] for class_name in class_names}\n",
    "    }\n",
    "\n",
    "# Output the results for each class and noise condition\n",
    "for noise_name, metrics in results.items():\n",
    "    print(f\"Noise type: {noise_name}\")\n",
    "    print(\"Noisy data accuracies:\")\n",
    "    for class_label, accuracy in metrics['noisy'].items():\n",
    "        print(f\"  {class_label}: {accuracy:.4f}\")\n",
    "    print(\"Denoised data accuracies:\")\n",
    "    for class_label, accuracy in metrics['denoised'].items():\n",
    "        print(f\"  {class_label}: {accuracy:.4f}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf023421-8cdf-40dc-aac1-73f300353404",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python3\\lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m756/756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 212ms/step\n",
      "\u001b[1m756/756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 234ms/step\n",
      "\u001b[1m756/756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 235ms/step\n",
      "\u001b[1m756/756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 420ms/step\n",
      "\u001b[1m756/756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m363s\u001b[0m 481ms/step\n",
      "\u001b[1m756/756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m363s\u001b[0m 481ms/step\n",
      "\u001b[1m756/756\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m364s\u001b[0m 482ms/step\n",
      "Noise type: EM\n",
      "Average SNR after denoising: 8.7213 dB\n",
      "Average RMSE after denoising: 0.7568\n",
      "Noisy data accuracies:\n",
      "  N: 0.9966\n",
      "  V: 0.7190\n",
      "  A: 0.0709\n",
      "  L: 0.9242\n",
      "Denoised data accuracies:\n",
      "  N: 0.9916\n",
      "  V: 0.7162\n",
      "  A: 0.0558\n",
      "  L: 0.8044\n",
      "\n",
      "\n",
      "Noise type: BW\n",
      "Average SNR after denoising: 7.3092 dB\n",
      "Average RMSE after denoising: 0.8149\n",
      "Noisy data accuracies:\n",
      "  N: 0.9973\n",
      "  V: 0.7502\n",
      "  A: 0.0668\n",
      "  L: 0.9297\n",
      "Denoised data accuracies:\n",
      "  N: 0.9911\n",
      "  V: 0.4650\n",
      "  A: 0.0679\n",
      "  L: 0.7522\n",
      "\n",
      "\n",
      "Noise type: MA\n",
      "Average SNR after denoising: 11.0426 dB\n",
      "Average RMSE after denoising: 0.7273\n",
      "Noisy data accuracies:\n",
      "  N: 0.9968\n",
      "  V: 0.7400\n",
      "  A: 0.0688\n",
      "  L: 0.9451\n",
      "Denoised data accuracies:\n",
      "  N: 0.9930\n",
      "  V: 0.5583\n",
      "  A: 0.0593\n",
      "  L: 0.8382\n",
      "\n",
      "\n",
      "Noise type: EM+MA\n",
      "Average SNR after denoising: 8.6876 dB\n",
      "Average RMSE after denoising: 0.7589\n",
      "Noisy data accuracies:\n",
      "  N: 0.9966\n",
      "  V: 0.7254\n",
      "  A: 0.0697\n",
      "  L: 0.9238\n",
      "Denoised data accuracies:\n",
      "  N: 0.9916\n",
      "  V: 0.7275\n",
      "  A: 0.0547\n",
      "  L: 0.8169\n",
      "\n",
      "\n",
      "Noise type: EM+BW\n",
      "Average SNR after denoising: 7.4442 dB\n",
      "Average RMSE after denoising: 0.8091\n",
      "Noisy data accuracies:\n",
      "  N: 0.9967\n",
      "  V: 0.7432\n",
      "  A: 0.0670\n",
      "  L: 0.9143\n",
      "Denoised data accuracies:\n",
      "  N: 0.9892\n",
      "  V: 0.4992\n",
      "  A: 0.0652\n",
      "  L: 0.7055\n",
      "\n",
      "\n",
      "Noise type: MA+BW\n",
      "Average SNR after denoising: 7.3251 dB\n",
      "Average RMSE after denoising: 0.8149\n",
      "Noisy data accuracies:\n",
      "  N: 0.9974\n",
      "  V: 0.7514\n",
      "  A: 0.0678\n",
      "  L: 0.9284\n",
      "Denoised data accuracies:\n",
      "  N: 0.9911\n",
      "  V: 0.4583\n",
      "  A: 0.0669\n",
      "  L: 0.7643\n",
      "\n",
      "\n",
      "Noise type: EM+BW+MA\n",
      "Average SNR after denoising: 7.4424 dB\n",
      "Average RMSE after denoising: 0.8096\n",
      "Noisy data accuracies:\n",
      "  N: 0.9968\n",
      "  V: 0.7403\n",
      "  A: 0.0683\n",
      "  L: 0.9152\n",
      "Denoised data accuracies:\n",
      "  N: 0.9895\n",
      "  V: 0.5063\n",
      "  A: 0.0642\n",
      "  L: 0.7063\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import wfdb\n",
    "import numpy as np\n",
    "import pywt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, Conv1DTranspose, LeakyReLU, Input, Add\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load ECG data with correct labels from the MIT-BIH Arrhythmia Database\n",
    "def load_ecg_data_with_labels(record_numbers, segment_length=512):\n",
    "    ecg_segments = []\n",
    "    labels = []\n",
    "    for rec_num in record_numbers:\n",
    "        record = wfdb.rdrecord(f'M:\\\\Dissertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0/{rec_num}')\n",
    "        annotation = wfdb.rdann(f'M:\\\\Dissertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0/{rec_num}', 'atr')\n",
    "        \n",
    "        for i in range(len(annotation.sample)):\n",
    "            start = max(0, annotation.sample[i] - segment_length // 2)\n",
    "            end = min(len(record.p_signal), start + segment_length)\n",
    "            if end - start == segment_length:\n",
    "                ecg_segments.append(record.p_signal[start:end, 0])  # Assuming MLII lead\n",
    "                labels.append(annotation.symbol[i])\n",
    "    \n",
    "    return np.array(ecg_segments), np.array(labels)\n",
    "\n",
    "# Load noise data from the correct directory and filenames\n",
    "def load_noise_data():\n",
    "    em = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\em', sampfrom=0).p_signal[:, 0]\n",
    "    bw = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\bw', sampfrom=0).p_signal[:, 0]\n",
    "    ma = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\ma', sampfrom=0).p_signal[:, 0]\n",
    "    return em, bw, ma\n",
    "\n",
    "# Instantiate the generator model with skip connections\n",
    "def build_generator(input_shape=(512, 1)):\n",
    "    inp = Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    x1 = Conv1D(64, 15, padding='same')(inp)\n",
    "    x1 = LeakyReLU(alpha=0.2)(x1)\n",
    "\n",
    "    x2 = Conv1D(128, 15, padding='same')(x1)\n",
    "    x2 = LeakyReLU(alpha=0.2)(x2)\n",
    "\n",
    "    x3 = Conv1D(256, 15, padding='same')(x2)\n",
    "    x3 = LeakyReLU(alpha=0.2)(x3)\n",
    "\n",
    "    x4 = Conv1D(512, 15, padding='same')(x3)\n",
    "    x4 = LeakyReLU(alpha=0.2)(x4)\n",
    "\n",
    "    # Decoder with skip connections\n",
    "    x5 = Conv1DTranspose(256, 15, padding='same')(x4)\n",
    "    x5 = LeakyReLU(alpha=0.2)(x5)\n",
    "    x5 = Add()([x5, x3])\n",
    "\n",
    "    x6 = Conv1DTranspose(128, 15, padding='same')(x5)\n",
    "    x6 = LeakyReLU(alpha=0.2)(x6)\n",
    "    x6 = Add()([x6, x2])\n",
    "\n",
    "    x7 = Conv1DTranspose(64, 15, padding='same')(x6)\n",
    "    x7 = LeakyReLU(alpha=0.2)(x7)\n",
    "    x7 = Add()([x7, x1])\n",
    "\n",
    "    out = Conv1DTranspose(1, 15, padding='same', activation='tanh')(x7)\n",
    "    \n",
    "    return Model(inp, out)\n",
    "\n",
    "generator = build_generator()\n",
    "\n",
    "# Load ECG and noise data\n",
    "ecg_records = [103, 105, 111, 116, 122, 205, 213, 219, 223, 230]  # Add more records as needed\n",
    "ecg_segments, labels = load_ecg_data_with_labels(ecg_records)\n",
    "em_noise, bw_noise, ma_noise = load_noise_data()\n",
    "\n",
    "# Filter and map labels to integer categories\n",
    "label_mapping = {'N': 0, 'V': 1, 'A': 2, 'L': 3}\n",
    "mapped_labels = np.array([label_mapping.get(label, -1) for label in labels])\n",
    "valid_indices = mapped_labels != -1\n",
    "\n",
    "ecg_segments = ecg_segments[valid_indices]\n",
    "mapped_labels = mapped_labels[valid_indices]\n",
    "\n",
    "# Compute class weights to handle imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(mapped_labels), y=mapped_labels)\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Extend and add noise to ECG segments\n",
    "def extend_noise_signal(noise_signal, target_length):\n",
    "    repeated_noise = np.tile(noise_signal, int(np.ceil(target_length / len(noise_signal))))\n",
    "    return repeated_noise[:target_length]\n",
    "\n",
    "def calculate_snr(signal, noise):\n",
    "    signal_power = np.sum(np.square(signal))\n",
    "    noise_power = np.sum(np.square(noise))\n",
    "    snr = 10 * np.log10(signal_power / noise_power)\n",
    "    return snr\n",
    "\n",
    "def calculate_rmse(signal, denoised_signal):\n",
    "    return np.sqrt(np.mean((signal - denoised_signal) ** 2))\n",
    "\n",
    "# Extend and add noise to ECG segments using the SNR formula\n",
    "def add_noise_to_segments(ecg_segments, noise_signal, target_snr_db):\n",
    "    noisy_segments = []\n",
    "    noise_signal = extend_noise_signal(noise_signal, ecg_segments.shape[1])\n",
    "    \n",
    "    for ecg_segment in ecg_segments:\n",
    "        current_snr = calculate_snr(ecg_segment, noise_signal[:len(ecg_segment)])\n",
    "        scaling_factor = np.sqrt(np.sum(np.square(ecg_segment)) / (np.sum(np.square(noise_signal)) * 10**(target_snr_db / 10)))\n",
    "        scaled_noise = noise_signal[:len(ecg_segment)] * scaling_factor\n",
    "        noisy_segment = ecg_segment + scaled_noise\n",
    "        noisy_segments.append(noisy_segment)\n",
    "    return np.array(noisy_segments)\n",
    "\n",
    "# Denoise function\n",
    "def denoise_signal(generator, noisy_signal):\n",
    "    noisy_signal = np.expand_dims(noisy_signal, axis=-1)\n",
    "    denoised_signal = generator.predict(noisy_signal)\n",
    "    return denoised_signal.squeeze()\n",
    "\n",
    "# Function to extract wavelet features\n",
    "def extract_wavelet_features(ecg_slice):\n",
    "    coeffs = pywt.wavedec(ecg_slice, 'db6', level=5)\n",
    "    return coeffs[0]  # You may want to use more features from different levels\n",
    "\n",
    "# Function to classify heartbeats using SVM\n",
    "def classify_heartbeats(features, labels):\n",
    "    clf = SVC(kernel='linear', class_weight=class_weight_dict)\n",
    "    clf.fit(features, labels)\n",
    "    return clf\n",
    "\n",
    "# Create combined noise types\n",
    "combined_noises = {\n",
    "    'EM+MA': em_noise + ma_noise,\n",
    "    'EM+BW': em_noise + bw_noise,\n",
    "    'MA+BW': ma_noise + bw_noise,\n",
    "    'EM+BW+MA': em_noise + bw_noise + ma_noise\n",
    "}\n",
    "\n",
    "# Add single noises to combined noises\n",
    "noises = {\n",
    "    'EM': em_noise,\n",
    "    'BW': bw_noise,\n",
    "    'MA': ma_noise\n",
    "}\n",
    "noises.update(combined_noises)\n",
    "\n",
    "snr_db = 0  # Example SNR value\n",
    "\n",
    "results = {}\n",
    "\n",
    "for noise_name, noise_data in noises.items():\n",
    "    noisy_ecg_slices = add_noise_to_segments(ecg_segments, noise_data, snr_db)\n",
    "    denoised_ecg_slices = denoise_signal(generator, noisy_ecg_slices)\n",
    "    \n",
    "    # Calculate SNR and RMSE\n",
    "    snr_values = [calculate_snr(ecg, denoised) for ecg, denoised in zip(ecg_segments, denoised_ecg_slices)]\n",
    "    rmse_values = [calculate_rmse(ecg, denoised) for ecg, denoised in zip(ecg_segments, denoised_ecg_slices)]\n",
    "    \n",
    "    features_noisy = np.array([extract_wavelet_features(slice) for slice in noisy_ecg_slices])\n",
    "    features_denoised = np.array([extract_wavelet_features(slice) for slice in denoised_ecg_slices])\n",
    "    \n",
    "    train_labels = mapped_labels[:len(features_noisy)]\n",
    "    \n",
    "    model_noisy = classify_heartbeats(features_noisy, train_labels)\n",
    "    model_denoised = classify_heartbeats(features_denoised, train_labels)\n",
    "    \n",
    "    predictions_noisy = model_noisy.predict(features_noisy)\n",
    "    predictions_denoised = model_denoised.predict(features_denoised)\n",
    "    \n",
    "    # Get the unique classes present in train_labels\n",
    "    unique_classes = np.unique(train_labels)\n",
    "    class_names = [name for i, name in enumerate(['N', 'V', 'A', 'L']) if i in unique_classes]\n",
    "    \n",
    "    # Evaluate accuracy for each class\n",
    "    report_noisy = classification_report(train_labels, predictions_noisy, target_names=class_names, output_dict=True, zero_division=0)\n",
    "    report_denoised = classification_report(train_labels, predictions_denoised, target_names=class_names, output_dict=True, zero_division=0)\n",
    "    \n",
    "    # Store accuracy and SNR, RMSE for each class\n",
    "    results[noise_name] = {\n",
    "        'noisy': {class_name: report_noisy[class_name]['precision'] for class_name in class_names},\n",
    "        'denoised': {class_name: report_denoised[class_name]['precision'] for class_name in class_names},\n",
    "        'snr': np.mean(snr_values),\n",
    "        'rmse': np.mean(rmse_values)\n",
    "    }\n",
    "\n",
    "# Output the results for each class and noise condition\n",
    "for noise_name, metrics in results.items():\n",
    "    print(f\"Noise type: {noise_name}\")\n",
    "    print(f\"Average SNR after denoising: {metrics['snr']:.4f} dB\")\n",
    "    print(f\"Average RMSE after denoising: {metrics['rmse']:.4f}\")\n",
    "    print(\"Noisy data accuracies:\")\n",
    "    for class_label, accuracy in metrics['noisy'].items():\n",
    "        print(f\"  {class_label}: {accuracy:.4f}\")\n",
    "    print(\"Denoised data accuracies:\")\n",
    "    for class_label, accuracy in metrics['denoised'].items():\n",
    "        print(f\"  {class_label}: {accuracy:.4f}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca200586-3af2-49cb-873d-d5f7d7822d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python3\\lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m361/756\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m5:43:04\u001b[0m 52s/step"
     ]
    }
   ],
   "source": [
    "import wfdb\n",
    "import numpy as np\n",
    "import pywt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, Conv1DTranspose, LeakyReLU, Input, Add\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load ECG data with correct labels from the MIT-BIH Arrhythmia Database\n",
    "def load_ecg_data_with_labels(record_numbers, segment_length=512):\n",
    "    ecg_segments = []\n",
    "    labels = []\n",
    "    for rec_num in record_numbers:\n",
    "        record = wfdb.rdrecord(f'M:\\\\Dissertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0/{rec_num}')\n",
    "        annotation = wfdb.rdann(f'M:\\\\Dissertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0/{rec_num}', 'atr')\n",
    "        \n",
    "        for i in range(len(annotation.sample)):\n",
    "            start = max(0, annotation.sample[i] - segment_length // 2)\n",
    "            end = min(len(record.p_signal), start + segment_length)\n",
    "            if end - start == segment_length:\n",
    "                ecg_segments.append(record.p_signal[start:end, 0])  # Assuming MLII lead\n",
    "                labels.append(annotation.symbol[i])\n",
    "    \n",
    "    return np.array(ecg_segments), np.array(labels)\n",
    "\n",
    "# Load noise data from the correct directory and filenames\n",
    "def load_noise_data():\n",
    "    em = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\em', sampfrom=0).p_signal[:, 0]\n",
    "    bw = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\bw', sampfrom=0).p_signal[:, 0]\n",
    "    ma = wfdb.rdrecord(r'M:\\\\Dissertation\\\\New folder\\\\mit-bih-noise-stress-test-database-1.0.0\\\\ma', sampfrom=0).p_signal[:, 0]\n",
    "    return em, bw, ma\n",
    "\n",
    "# Instantiate the generator model with skip connections\n",
    "def build_generator(input_shape=(512, 1)):\n",
    "    inp = Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    x1 = Conv1D(64, 15, padding='same')(inp)\n",
    "    x1 = LeakyReLU(alpha=0.2)(x1)\n",
    "\n",
    "    x2 = Conv1D(128, 15, padding='same')(x1)\n",
    "    x2 = LeakyReLU(alpha=0.2)(x2)\n",
    "\n",
    "    x3 = Conv1D(256, 15, padding='same')(x2)\n",
    "    x3 = LeakyReLU(alpha=0.2)(x3)\n",
    "\n",
    "    x4 = Conv1D(512, 15, padding='same')(x3)\n",
    "    x4 = LeakyReLU(alpha=0.2)(x4)\n",
    "    \n",
    "    x5 = Conv1D(1024, 15, padding='same')(x4)\n",
    "    x5 = LeakyReLU(alpha=0.2)(x5)\n",
    "    \n",
    "    x6 = Conv1D(2048, 15, padding='same')(x5)\n",
    "    x6 = LeakyReLU(alpha=0.2)(x6)\n",
    "    \n",
    "    x7 = Conv1D(4096, 15, padding='same')(x6)\n",
    "    x7 = LeakyReLU(alpha=0.2)(x7)\n",
    "\n",
    "    # Decoder with skip connections\n",
    "    x8 = Conv1DTranspose(2048, 15, padding='same')(x7)\n",
    "    x8 = LeakyReLU(alpha=0.2)(x8)\n",
    "    x8 = Add()([x8, x6])\n",
    "\n",
    "    x9 = Conv1DTranspose(1024, 15, padding='same')(x8)\n",
    "    x9 = LeakyReLU(alpha=0.2)(x9)\n",
    "    x9 = Add()([x9, x5])\n",
    "\n",
    "    x10 = Conv1DTranspose(512, 15, padding='same')(x9)\n",
    "    x10 = LeakyReLU(alpha=0.2)(x10)\n",
    "    x10 = Add()([x10, x4])\n",
    "\n",
    "    x11 = Conv1DTranspose(256, 15, padding='same')(x10)\n",
    "    x11 = LeakyReLU(alpha=0.2)(x11)\n",
    "    x11 = Add()([x11, x3])\n",
    "\n",
    "    x12 = Conv1DTranspose(128, 15, padding='same')(x11)\n",
    "    x12 = LeakyReLU(alpha=0.2)(x12)\n",
    "    x12 = Add()([x12, x2])\n",
    "\n",
    "    x13 = Conv1DTranspose(64, 15, padding='same')(x12)\n",
    "    x13 = LeakyReLU(alpha=0.2)(x13)\n",
    "    x13 = Add()([x13, x1])\n",
    "\n",
    "    out = Conv1DTranspose(1, 15, padding='same', activation='tanh')(x13)\n",
    "    \n",
    "    return Model(inp, out)\n",
    "\n",
    "generator = build_generator()\n",
    "\n",
    "# Load ECG and noise data\n",
    "ecg_records = [103, 105, 111, 116, 122, 205, 213, 219, 223, 230]  # Add more records as needed\n",
    "ecg_segments, labels = load_ecg_data_with_labels(ecg_records)\n",
    "em_noise, bw_noise, ma_noise = load_noise_data()\n",
    "\n",
    "# Filter and map labels to integer categories\n",
    "label_mapping = {'N': 0, 'V': 1, 'A': 2, 'L': 3}\n",
    "mapped_labels = np.array([label_mapping.get(label, -1) for label in labels])\n",
    "valid_indices = mapped_labels != -1\n",
    "\n",
    "ecg_segments = ecg_segments[valid_indices]\n",
    "mapped_labels = mapped_labels[valid_indices]\n",
    "\n",
    "# Compute class weights to handle imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(mapped_labels), y=mapped_labels)\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Extend and add noise to ECG segments\n",
    "def extend_noise_signal(noise_signal, target_length):\n",
    "    repeated_noise = np.tile(noise_signal, int(np.ceil(target_length / len(noise_signal))))\n",
    "    return repeated_noise[:target_length]\n",
    "\n",
    "def calculate_snr(signal, noise):\n",
    "    signal_power = np.sum(np.square(signal))\n",
    "    noise_power = np.sum(np.square(noise))\n",
    "    snr = 10 * np.log10(signal_power / noise_power)\n",
    "    return snr\n",
    "\n",
    "def calculate_rmse(signal, denoised_signal):\n",
    "    return np.sqrt(np.mean((signal - denoised_signal) ** 2))\n",
    "\n",
    "# Extend and add noise to ECG segments using the SNR formula\n",
    "def add_noise_to_segments(ecg_segments, noise_signal, target_snr_db):\n",
    "    noisy_segments = []\n",
    "    noise_signal = extend_noise_signal(noise_signal, ecg_segments.shape[1])\n",
    "    \n",
    "    for ecg_segment in ecg_segments:\n",
    "        current_snr = calculate_snr(ecg_segment, noise_signal[:len(ecg_segment)])\n",
    "        scaling_factor = np.sqrt(np.sum(np.square(ecg_segment)) / (np.sum(np.square(noise_signal)) * 10**(target_snr_db / 10)))\n",
    "        scaled_noise = noise_signal[:len(ecg_segment)] * scaling_factor\n",
    "        noisy_segment = ecg_segment + scaled_noise\n",
    "        noisy_segments.append(noisy_segment)\n",
    "    return np.array(noisy_segments)\n",
    "\n",
    "# Denoise function\n",
    "def denoise_signal(generator, noisy_signal):\n",
    "    noisy_signal = np.expand_dims(noisy_signal, axis=-1)\n",
    "    denoised_signal = generator.predict(noisy_signal)\n",
    "    return denoised_signal.squeeze()\n",
    "\n",
    "# Function to extract wavelet features\n",
    "def extract_wavelet_features(ecg_slice):\n",
    "    coeffs = pywt.wavedec(ecg_slice, 'db6', level=5)\n",
    "    return coeffs[0]  # You may want to use more features from different levels\n",
    "\n",
    "# Function to classify heartbeats using SVM\n",
    "def classify_heartbeats(features, labels):\n",
    "    clf = SVC(kernel='linear', class_weight=class_weight_dict)\n",
    "    clf.fit(features, labels)\n",
    "    return clf\n",
    "\n",
    "# Create combined noise types\n",
    "combined_noises = {\n",
    "    'EM+MA': em_noise + ma_noise,\n",
    "    'EM+BW': em_noise + bw_noise,\n",
    "    'MA+BW': ma_noise + bw_noise,\n",
    "    'EM+BW+MA': em_noise + bw_noise + ma_noise\n",
    "}\n",
    "\n",
    "# Add single noises to combined noises\n",
    "noises = {\n",
    "    'EM': em_noise,\n",
    "    'BW': bw_noise,\n",
    "    'MA': ma_noise\n",
    "}\n",
    "noises.update(combined_noises)\n",
    "\n",
    "snr_db = 0  # Example SNR value\n",
    "\n",
    "results = {}\n",
    "\n",
    "for noise_name, noise_data in noises.items():\n",
    "    noisy_ecg_slices = add_noise_to_segments(ecg_segments, noise_data, snr_db)\n",
    "    denoised_ecg_slices = denoise_signal(generator, noisy_ecg_slices)\n",
    "    \n",
    "    # Calculate SNR and RMSE\n",
    "    snr_values = [calculate_snr(ecg, denoised) for ecg, denoised in zip(ecg_segments, denoised_ecg_slices)]\n",
    "    rmse_values = [calculate_rmse(ecg, denoised) for ecg, denoised in zip(ecg_segments, denoised_ecg_slices)]\n",
    "    \n",
    "    features_noisy = np.array([extract_wavelet_features(slice) for slice in noisy_ecg_slices])\n",
    "    features_denoised = np.array([extract_wavelet_features(slice) for slice in denoised_ecg_slices])\n",
    "    \n",
    "    train_labels = mapped_labels[:len(features_noisy)]\n",
    "    \n",
    "    model_noisy = classify_heartbeats(features_noisy, train_labels)\n",
    "    model_denoised = classify_heartbeats(features_denoised, train_labels)\n",
    "    \n",
    "    predictions_noisy = model_noisy.predict(features_noisy)\n",
    "    predictions_denoised = model_denoised.predict(features_denoised)\n",
    "    \n",
    "    # Get the unique classes present in train_labels\n",
    "    unique_classes = np.unique(train_labels)\n",
    "    class_names = [name for i, name in enumerate(['N', 'V', 'A', 'L']) if i in unique_classes]\n",
    "    \n",
    "    # Evaluate accuracy for each class\n",
    "    report_noisy = classification_report(train_labels, predictions_noisy, target_names=class_names, output_dict=True, zero_division=0)\n",
    "    report_denoised = classification_report(train_labels, predictions_denoised, target_names=class_names, output_dict=True, zero_division=0)\n",
    "    \n",
    "    # Store accuracy and SNR, RMSE for each class\n",
    "    results[noise_name] = {\n",
    "        'noisy': {class_name: report_noisy[class_name]['precision'] for class_name in class_names},\n",
    "        'denoised': {class_name: report_denoised[class_name]['precision'] for class_name in class_names},\n",
    "        'snr': np.mean(snr_values),\n",
    "        'rmse': np.mean(rmse_values)\n",
    "    }\n",
    "\n",
    "# Output the results for each class and noise condition\n",
    "for noise_name, metrics in results.items():\n",
    "    print(f\"Noise type: {noise_name}\")\n",
    "    print(f\"Average SNR after denoising: {metrics['snr']:.4f} dB\")\n",
    "    print(f\"Average RMSE after denoising: {metrics['rmse']:.4f}\")\n",
    "    print(\"Noisy data accuracies:\")\n",
    "    for class_label, accuracy in metrics['noisy'].items():\n",
    "        print(f\"  {class_label}: {accuracy:.4f}\")\n",
    "    print(\"Denoised data accuracies:\")\n",
    "    for class_label, accuracy in metrics['denoised'].items():\n",
    "        print(f\"  {class_label}: {accuracy:.4f}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72361079-62bb-4aa8-b549-3990d16fcb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting function for the noisy and denoised ECG signals\n",
    "def plot_ecg_signals(noise_name, ecg_segments, noisy_ecg_slices, denoised_ecg_slices, num_samples=3):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Plot original ECG\n",
    "        plt.subplot(num_samples, 3, i * 3 + 1)\n",
    "        plt.plot(ecg_segments[i], linewidth=0.8)\n",
    "        if i == 0:\n",
    "            plt.title('Original ECG')\n",
    "        plt.ylim([-2, 2])\n",
    "        plt.xlim([0, len(ecg_segments[i])])\n",
    "\n",
    "        # Plot noisy ECG\n",
    "        plt.subplot(num_samples, 3, i * 3 + 2)\n",
    "        plt.plot(noisy_ecg_slices[i], linewidth=0.8)\n",
    "        if i == 0:\n",
    "            plt.title(f'Noisy ECG ({noise_name})')\n",
    "        plt.ylim([-2, 2])\n",
    "        plt.xlim([0, len(noisy_ecg_slices[i])])\n",
    "\n",
    "        # Plot denoised ECG\n",
    "        plt.subplot(num_samples, 3, i * 3 + 3)\n",
    "        plt.plot(denoised_ecg_slices[i], linewidth=0.8)\n",
    "        if i == 0:\n",
    "            plt.title('Denoised ECG')\n",
    "        plt.ylim([-2, 2])\n",
    "        plt.xlim([0, len(denoised_ecg_slices[i])])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plotting for all noise types\n",
    "for noise_name, noise_data in noises.items():\n",
    "    noisy_ecg_slices = add_noise_to_segments(ecg_segments, noise_data, snr_db)\n",
    "    denoised_ecg_slices = denoise_signal(generator, noisy_ecg_slices)\n",
    "    \n",
    "    # Generate plots\n",
    "    plot_ecg_signals(noise_name, ecg_segments, noisy_ecg_slices, denoised_ecg_slices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02605f8-f28c-47ff-8f30-7280740b7cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
